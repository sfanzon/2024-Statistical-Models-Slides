{"title":"Statistical Models","markdown":{"yaml":{"title":"Statistical Models","subtitle":"Lecture 1","from":"markdown+emoji","author":[{"name":"Dr. Silvio Fanzon","id":"sf","email":"S.Fanzon@hull.ac.uk","url":"https://www.silviofanzon.com","affiliations":"University of Hull"},{"name":"Dr. John Fry","id":"jf","email":"J.M.Fry@hull.ac.uk","url":"https://www.hull.ac.uk/staff-directory/john-fry","affiliations":"University of Hull"}]},"headingText":"Lecture 1: <br>An introduction to Statistics","headingAttr":{"id":"","classes":[],"keyvalue":[["background-color","#cc0164"],["visibility","uncounted"]]},"containsRefs":false,"markdown":"\n\n\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Outline of Lecture 1\n\n1. Module info\n2. Motivation\n3. Probability background\n4. Statistics background\n\n\n\n\n\n# Part 1: <br>Module info {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Contact details\n\n- **Lecturer:** Dr. Silvio Fanzon\n- **Call me:**\n  * Silvio\n  * Dr. Fanzon\n- **Email:** S.Fanzon@hull.ac.uk\n- **Office:** Room 104a, Larkin Building\n- **Office hours:** Wednesday 12:00-13:00\n- **Meetings**: in my office or send me an Email\n\n\n\n\n\n\n## References\n### Main textbooks  \n\n\n::: {.column width=\"61%\"}\n\n<br>\n\nSlides are self-contained and based on the book\n\n- [@bingham-fry] Bingham, N. H. and Fry, J. M. \n<br> *Regression: Linear models in statistics.* <br> Springer, 2010\n\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/bingham_fry.png){width=82%}](https://link.springer.com/book/10.1007/978-1-84882-969-5)\n\n:::\n\n\n\n\n\n## References\n### Main textbooks \n\n\n::: {.column width=\"61%\"}\n<br>\n\n.. and also on the book\n\n- [@fry-burke] Fry, J. M. and Burke, M. \n<br>*Quantitative methods in finance using R.* \n<br>Open University Press, 2022\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/fry_burke.png){width=87%}](https://www.mheducation.co.uk/quantitative-methods-in-finance-using-r-9780335251261-emea-group)\n\n:::\n\n\n\n\n\n\n\n## References\n### Secondary References \n\n::: {.column width=\"69%\"}\n- [@casella-berger] Casella, G. and Berger R. L. <br>\n*Statistical inference.*\n<br> Second Edition, Brooks/Cole, 2002\n\n\n- [@degroot] DeGroot M. H. and Schervish M. J. <br> \n*Probability and Statistics.* \n<br> Fourth Edition, Addison-Wesley, 2012\n\n\n- [@dalgaard] Dalgaard, P. \n<br> *Introductory statistics with R.*\n<br> Second Edition, Springer, 2008   \n\n:::\n\n\n::: {.column width=\"30%\"}\n\n**Probability & Statistics manual**\n\n**Easier Probability & Statistics manual**\n\n**R manual**\n\n:::\n\n\n\n## Assessment \n\nThis course will be assessed as follows:\n\n<br>\n\n\n|**Type of Assessment**  | **Percentage of final grade** |\n|:-----                  |:-----                         |\n|  Coursework Portfolio  | 70%                           |\n|  Homework              | 30%                           |\n\n\n\n\n## Assessment \n### Rules for Coursework\n\n\n- Coursework available on Canvas from Week 2\n\n- We will discuss coursework exercises in class\n\n- Coursework must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Wednesday 1st May 2024**\n\n\n\n\n\n## Assessment\n### Rules for Homework\n\n\n- 10 Homework papers, posted weekly on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Each Homework paper is worth 14 points\n\n- Final Homework grade computation: \n    * Sum the top 7 scores, for a maximum of 98 points\n    * Bonus 2 points will be added to the final score\n\n- Homework papers must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Mondays** \n\n\n\n\n\n## Assessment\n### Key submission dates\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 1      | 5 Feb       |\n| Homework 2      | 12 Feb      |\n| Homework 3      | 19 Feb      |\n| Homework 4      | 26 Feb      |\n| Homework 5      | 4 Mar       |\n| Homework 6      | 11 Mar      |\n\n\n:::\n\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 7      | 18 Mar      |\n| Easter Break    | :sunglasses:|\n| Homework 8      | 8 Apr       |\n| Homework 9      | 15 Apr      |\n| Homework 10     | 22 Apr      |\n| Coursework      | 1 May       |\n\n:::\n\n\n\n## Assessment\n### How to submit assignments\n\n\n- Submit **PDFs only** on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- You have two options:\n\t* Write on tablet and submit PDF Output\n\t* Write on paper and **Scan in Black and White** using a **Scanner** or **Scanner App** (Tiny Scanner, Scanner Pro, ...)\n\n\n\n\n\n\n\n## The nature of Statistics\n### Statistics is a mathematical subject\n\n- Maths skills will give you a head start\n\n- There are other occasions where common sense and *detective skills* can be more important\n\n- Provides an early example of mathematics working in concert with the available computation\n\n\n\n\n## The nature of Statistics\n### We will use a combination of hand calculation and software\n\n- Recognises that you are maths students\n- Software (R) is really useful, particularly for dissertations\n- Please bring your laptop into class\n- Download R onto your laptop\n\n\n\n\n\n\n\n## Overview of the module\n\n\nModule has **11 Lectures**, divided into two parts:\n\n- **Part I** - Mathematical statistics\n\n- **Part II** - Applied statistics\n\n\n\n\n\n## Overview of the module\n### Part I - Mathematical statistics\n\n1. Introduction to statistics\n2. Normal distribution family and one-sample hypothesis tests\n3. Two-sample hypothesis tests\n4. The chi-squared test\n5. Non-parametric statistics\n6. The maths of regression\n\n\n\n\n\n## Overview of the module\n### Part II - Applied statistics\n\n\n7. An introduction to practical regression\n8. The extra sum of squares principle and regression modelling assumptions\n9. Violations of regression assumptions -- Autocorrelation\n10. Violation of regression assumptions -- Multicollinearity\n10. Dummy variable regression models \n\n\n\n\n\n# Part 2: <br>Motivation {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n## Simple but useful questions {.smaller}\n\n\n::: {.column width=\"48%\"}\n\n**Generic data:**\n\n- What is a *typical* observation\n  * What is the **mean**?\n\n- How spread out is the data?\n  * What is the **variance**?\n\n:::\n\n\n::: {.column width=\"48%\"}\n\n**Regression:**\n\n- What happens to $Y$ as $X$ increases?\n  * increases?\n  * decreases?\n  * nothing?\n\n:::\n\n\n\n**Statistics answers these questions systematically**\n\n- important for large datasets\n- The same mathematical machinery (normal family of distributions) can be applied to both questions\n\n\n\n\n## Analysing a general dataset\n\n**Two basic questions:**\n\n1. Location or mean\n2. Spread or variance\n\n\n**Statistics enables to answer systematically:**\n\n1. One sample and two-sample $t$-test\n2. Chi-squared test and $F$-test\n\n\n\n\n\n## Recall the following sketch\n\n![Curve represents data distribution](images/Fig1.png){width=90%}\n\n\n\n\n## Motivating regression\n\n**Basic question in regression:**\n\n- What happens to $Y$ as $X$ increases?\n\n  * increases?\n  * decreases?\n  * nothing?\n\n\n**In this way regression can be seen as a more advanced version of high-school maths**\n\n\n\n\n\n\n\n## Positive gradient\n\nAs $X$ increases $Y$ increases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = 1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n## Negative gradient\n\nAs $X$ increases $Y$ decreases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(-3, 3), \n  ylim = c(-3, 3), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = -1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Zero gradient\n\nChanges in $X$ do not affect $Y$\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 2.5, \n  b = 0, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Real data example\n\n\n- Real data is more **imperfect**\n- But the same basic idea applies\n- Example: \n    * $X =$ Stock price\n    * $Y =$ Gold price\n\n\n\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.75em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n\n::: {style=\"font-size: 0.55em\"}\n```{r}\n#| echo: false\n#| layout-ncol: 1\n\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\nknitr::kable(\n  list(data[1:11,], data[12:22,], data[23:33,]),\n  row.names = TRUE,\n  format = \"html\", \n  table.attr = 'class=\"table simple table-striped table-hover\"',\n) \n\n\n```\n\n\n:::\n\n\n\n\n## Real data example {.smaller}\n### Visualizing the data\n\n\n::::: {.columns style='display: flex !important; height: 80%;'}\n\n::: {.column width=\"38%\" style='display: flex; justify-content: center; align-items: center;'}\n\n- Plot Stock Price against Gold Price\n\n- Observation: \n\n  * As Stock price decreases, Gold price increases\n\n- Why? This might be because:\n    * Stock price decreases\n    * People invest in secure assets (Gold)\n    * Gold demand increases\n    * Gold price increases\n\n:::\n\n\n::: {.column width=\"61%\" style='display: flex; justify-content: center; align-items: center;'}\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\ndata1 <- read.table(\"datasets/L3eg1data.txt\")\nrealgoldprice<-data1[,1]\nrealstockprice<-data1[,2]\nplot(realstockprice, \n  realgoldprice, \n  xlab=\"\", \n  ylab=\"\")\n\nmtext(\"Stock Price\", side=1, line=3, cex=2)\nmtext(\"Gold Price\", side=2, line=2.5, cex=2)\n\n```\n\n:::\n\n:::::\n\n\n\n\n## Don't panic {.smaller}\n\n- Regression problems can look a lot harder than they really are\n  * Basic question remains the same: what happens to $Y$ as $X$ increases?\n\n- Beware of jargon. Various authors distinguish between\n  * Two variable regression model\n  * Multiple regression model\n  * Analysis of Variance\n  * Analysis of Covariance\n\n- Despite these apparent differences:\n  * Mathematical methodology stays (essentially) the same\n  * regression-fitting commands in R stay (essentially) the same\n\n\n\n\n\n# Part 3: <br>Probability background {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n\n## Sample space {.smaller}\n\n::: Definition\n### Sample space\n\nA set $\\Omega$ of all possible outcomes of some experiment\n\n:::\n\n\n**Examples**:\n\n- Coin toss: results in Heads $= H$ and Tails $= T$ \n$$\n\\Omega = \\{ H, T \\}\n$$\n\n- Student grade for *Statistical Models*: a number between $0$ and $100$\n$$\n\\Omega = \\{ x \\in \\mathbb{R} \\, \\colon \\, 0 \\leq x \\leq 100 \\} = [0,100]\n$$\n\n\n\n\n\n\n## Events {.smaller}\n\n\n::: Definition\n### Event\n\nA subset $E$ of the sample space $\\Omega$ (including $\\emptyset$ and $\\Omega$ itself)\n\n:::\n\n\n**Operations with events**:\n\n- **Union** of two events $A$ and $B$\n$$\nA \\cup B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ or } \\, x \\in B \\}\n$$\n\n- **Intersection** of two events $A$ and $B$\n$$\nA \\cap B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ and } \\, x \\in B \\}\n$$\n\n- **Complement** of an event $A$\n$$\nA^c  := \\{ x \\in \\Omega \\colon x \\notin A  \\}\n$$\n\n\n\n\n## Events {.smaller}\n\n\n**More Operations with events**:\n\n- **Infinite Union** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcup_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for some } \\, i \\in I \\}\n$$\n\n- **Infinite Intersection** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcap_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for all } \\, i \\in I \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n\n**Example**: Consider sample space and events\n$$\n\\Omega := (0,1] \\,, \\quad \nA_i = \\left[\\frac{1}{i} , 1 \\right] \\,, \\quad i \\in \\mathbb{N}\n$$\nThen\n$$\n\\bigcup_{i \\in I} A_i = (0,1] \\,, \\quad \n\\bigcap_{i \\in I} A_i = \\{ 1 \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Disjoint\n\nTwo events $A$ and $B$ are **disjoint** if\n$$\nA \\cap B = \\emptyset\n$$\nEvents $A_1, A_2, \\ldots$ are **pairwise disjoint** if\n$$\nA_i \\cap A_j = \\emptyset \\,, \\quad \\forall \\, i \\neq j\n$$\n:::\n\n\n::: Definition\n### Partition\n\nThe collection of events  $A_1, A_2, \\ldots$ is a **partition** of $\\Omega$ if\n\n1. $A_1, A_2, \\ldots$ are pairwise disjoint\n2. $\\Omega = \\cup_{i=1}^\\infty A_i$\n\n:::\n\n\n\n\n## What's a Probability? {.smaller}\n\n\n- To each event $E \\subset \\Omega$ we would like to associate a number\n$$\nP(E) \\in [0,1]\n$$\n\n- The number $P(E)$ is called the **probability** of $E$\n\n- The number $P(E)$ models the **frequency of occurrence** of $E$:\n\n    * $P(E)$ small means $E$ has low chance of occurring\n    * $P(E)$ large means $E$ has high chance of occurring\n\n- **Technical issue**:\n\n    * One cannot associate a number $P(E)$ for all events in $\\Omega$\n    * Probability function $P$ only defined for a **smaller** family of events\n    * Such family of events is called $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n\n\n::: Definition\n### sigma-algebra\n\nLet $\\mathcal{B}$ be a collection of events. We say that $\\mathcal{B}$ is a $\\sigma$-algebra if\n\n1. $\\emptyset \\in \\mathcal{B}$\n2. If $A \\in \\mathcal{B}$ then $A^c \\in \\mathcal{B}$\n3. If $A_1,A_2 , \\ldots \\in \\mathcal{B}$ then $\\cup_{i=1}^\\infty A_i \\in \\mathcal{B}$\n\n\n:::\n\n\n\n**Remarks**:\n\n- Since $\\emptyset \\in \\mathcal{B}$ and $\\emptyset^c = \\Omega$, we deduce that\n$$\n\\Omega \\in \\mathcal{B}\n$$\n\n- Thanks to DeMorgan's Law we have that\n$$\nA_1,A_2 , \\ldots \\in \\mathcal{B} \\quad \\implies \\quad \\cap_{i=1}^\\infty A_i \\in \\mathcal{B}\n$$\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\nSuppose $\\Omega$ is any set: \n\n- Then\n$$\n\\mathcal{B} = \\{ \\emptyset, \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n- The power set of $\\Omega$\n$$\n\\mathcal{B} = \\operatorname{Power} (\\Omega) := \\{ A \\colon A \\subset \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\n\n- If $\\Omega$ has $n$ elements then $\\mathcal{B} = \\operatorname{Power} (\\Omega)$ contains $2^n$ sets\n\n- If $\\Omega = \\{ 1,2,3\\}$ then\n\\begin{align*}\n\\mathcal{B} = \\operatorname{Power} (\\Omega) = \\big\\{ & \\{1\\} , \\, \\{2\\}, \\, \\{3\\}  \\\\\n                                       &  \\{1,2\\} , \\, \\{2,3\\}, \\, \\{1,3\\}      \\\\\n                                       & \\emptyset , \\{1,2,3\\}   \\big\\}\n\\end{align*}\n\n- If $\\Omega$ is **uncountable** then the power set of $\\Omega$ is not easy to describe\n\n\n\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Question\n\n$\\mathbb{R}$ is uncountable. Which $\\sigma$-algebra do we consider?\n\n:::\n\n\n::: Definition \n### Lebesgue sigma-algebra\n\n\nThe Lebesgue $\\sigma$-algebra on $\\mathbb{R}$ is the smallest $\\sigma$-algebra $\\mathcal{L}$ containing all sets of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n:::\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Important\n\nTherefore the events of $\\R$ are \n\n- Intervals\n- Unions and intersection of intervals\n- Countable Unions and intersection of intervals\n\n:::\n\n\n::: Warning \n\n- I only told you that the Lebsesgue $\\sigma$-algebra $\\mathcal{L}$ **exists**\n- Explicitly showing that $\\mathcal{L}$ exists is not easy, see [@rosenthal]\n\n:::\n\n\n\n\n## Probability measure {.smaller}\n\nSuppose given:\n\n- $\\Omega$ sample space \n- $\\mathcal{B}$ a $\\sigma$-algebra on $\\Omega$\n\n::: Definition\n### Probability measure\n\nA **probability measure** on $\\Omega$ is a map\n$$\nP \\colon \\mathcal{B} \\to [0,1]\n$$\nsuch that the **Axioms of Probability** hold\n\n1. $P(\\Omega) = 1$\n2. If $A_1, A_2,\\ldots$ are pairwise disjoint then\n$P\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  = \\sum_{i=1}^\\infty P(A_i)$\n\n:::\n\n\n\n\n## Properties of Probability {.smaller}\n\nLet $A, B \\in \\mathcal{B}$. As a consequence of the Axioms of Probability:\n\n\n1. $P(\\emptyset) = 0$\n2. If $A$ and $B$ are disjoint then\n$$\nP(A \\cup B) = P(A) + P(B)\n$$\n3. $P(A^c) = 1 - P(A)$\n4. $P(A) = P(A \\cap B) + P(A \\cap B^c)$\n5. $P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$\n6. If $A \\subset B$ then\n$$\nP(A) \\leq P(B)\n$$\n\n\n\n\n\n## Properties of Probability {.smaller} \n\n7. Suppose $A$ is an event and $B_1,B_2, \\ldots$ a partition of $\\Omega$. Then\n$$\nP(A) = \\sum_{i=1}^\\infty  P(A \\cap B_i)\n$$\n8. Suppose $A_1,A_2, \\ldots$ are events. Then\n$$\nP\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  \\leq  \\sum_{i=1}^\\infty P(A_i)\n$$\n\n\n\n\n## Example: Fair Coin Toss {.smaller}\n\n- The sample space for **coin toss** is $\\Omega = \\{ H, T \\}$\n\n- We take as $\\sigma$-algebra the power set of $\\Omega$\n$$\n\\mathcal{B} = \\{  \\emptyset , \\, \\{H\\}  , \\, \\{T\\} , \\, \\{H,T\\}   \\}\n$$\n\n- We suppose that the coin is fair\n\n    * This means $P \\colon \\mathcal{B} \\to [0,1]$ satisfies\n    $$\n    P(\\{H\\}) = P(\\{T\\})\n    $$\n    * Assuming the above we get\n    $$\n    1 = P(\\Omega) = P(\\{H\\} \\cup \\{T\\}) = \n    P(\\{H\\}) + P(\\{T\\}) = 2 P(\\{H\\})\n    $$\n    * Therefore \n    $$\n    P(\\{H\\}) = P(\\{T\\}) = \\frac12\n    $$\n\n\n\n\n\n## Conditional Probability {.smaller}\n\n::: Definition\n### Conditional Probability\n\nLet $A,B$ be events in $\\Omega$ with \n$$\nP(B)>0\n$$\nThe **conditional probability** of $A$ given $B$ is\n$$\nP(A|B) := \\frac{P(A \\cap B)}{P(B)} \n$$\n\n:::\n\n\n- $P(A|B)$ represents the probability of $A$, knowing that $B$ happened\n- The function $A \\mapsto P(A|B)$ is a probability measure on $\\Omega$\n\n\n\n\n## Bayes' Rule {.smaller}\n\n\n- For two events $A$ and $B$ is holds\n\n$$\nP(A | B ) = P(B|A) \\frac{P(A)}{P(B)}\n$$\n\n\n- Given a partition $A_1, A_2, \\ldots$ of the sample space we have\n\n$$\nP(A_i | B ) = \\frac{ P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B | A_j) P(A_j)}\n$$\n\n\n\n\n\n## Independence {.smaller}\n\n\n::: Definition\n\nTwo events $A$ and $B$ are **independent** if \n$$\nP(A \\cap B) = P(A)P(B)\n$$\nA collection of events $A_1 , \\ldots ,A_n$ are **mutually independent** if\nfor any subcollection $A_{i_1}, \\ldots, A_{i_k}$ it holds\n$$\nP \\left(  \\bigcap_{j=1}^k A_j  \\right) = \\prod_{j=1}^k  P(A_{i_j})\n$$\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Motivation\n\n- Consider the experiment of flipping a coin $50$ times\n- The sample space consists of $2^{50}$ elements\n- Elements are vectors of $50$ entries recording the outcome $H$ or $T$ of each flip\n- This is a very large sample space!\n\n\nSuppose we are only interested in\n$$\nX = \\text{ number of } \\, H \\, \\text{ in } \\, 50 \\, \\text{flips}\n$$\n\n- Then the new sample space is the set of integers\n$$\n\\{ 0,1,2,\\ldots,50\\}\n$$\n- This is much smaller!\n- $X$ is called a Random Variable\n\n\n\n\n## Random Variables {.smaller}\n\n\nAssume given\n\n- $\\Omega$ sample space\n- $\\mathcal{B}$ a $\\sigma$-algebra of events on $\\Omega$\n- $P \\colon \\mathcal{B} \\to [0,1]$ a probability measure\n\n\n::: Definition\n### Random variable\n\nA function $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\nWe will abbreviate Random Variable with r.v.\n\n\n\n\n\n## Random Variables {.smaller}\n### Technical remark \n\n\n::: Definition\n### Random variable\n\nA [**measurable**]{style=\"color:#cc0164;\"}\nfunction $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\n**Technicality**: \n$X$ is a [**measurable**]{style=\"color:#cc0164;\"}\nfunction if\n$$\n\\{ X \\in I \\} := \\{ \\omega \\in \\Omega \\colon X(\\omega) \\in I \\} \\in \\mathcal{B} \\,, \\quad \\forall \\, I \\in \\mathcal{L}\n$$\nwhere \n\n- $\\mathcal{L}$ is the Lebsgue $\\sigma$-algebra on $\\mathbb{R}$\n- $\\mathcal{B}$ is the given $\\sigma$-algebra on $\\Omega$\n\n\n\n\n## Random Variables {.smaller}\n### Notation\n\n- In particular $I \\in \\mathcal{L}$ can be of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n- In this case the set \n$$\n\\{X \\in I\\} \\in \\mathcal{B}\n$$ \nis denoted by, respectively:\n$$\n\\{ a < X < b \\} \\,, \\quad \\{ a < X \\leq b \\} \\,, \\quad \\{ a \\leq X < b \\} \\,, \\quad \\{ a \\leq X \\leq b \\}\n$$\n\n- If $a=b=x$ then $I=[x,x]=\\{x\\}$. Then we denote\n$$\n\\{X \\in I\\} = \\{X = x\\}\n$$\n\n\n## Random Variables {.smaller}\n### Why do we require measurability?\n\n**Answer**: Because it allows to define a new probability measure on $\\mathbb{R}$\n\n\n::: Definition\n### Distribution\n\nThe **distribution** of a random variable $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( \\{X \\in I\\} \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n:::\n\n\n**Note**: \n\n- One can show that $P_X$ satisfies the *Probability Axioms*\n- Thus $P_X$ is a probability measure on $\\mathbb{R}$\n- In the future we will denote \n$$\nP \\left( X \\in I \\right) := P \\left( \\{X \\in I\\} \\right)\n$$\n\n\n\n\n\n\n\n## Random Variables {.smaller}\n### Why is the distribution useful?\n\n**Answer**: Because it allows to define a random variable $X$\n\n- by specifying the distribution values\n$$\nP \\left( X \\in I \\right) \n$$\n- rather than defining an explicit function $X \\colon \\Omega \\to \\mathbb{R}$\n\n\n**Important**: More often than not\n\n- We care about the distribution of $X$\n- We do not care about how $X$ is defined\n\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Sample space $\\Omega$ given by the below values of $\\omega$\n\n+:---------:+:----:+:---:+:---:+:---:+:---:+:---:+:---:+:---:+\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n+-----------+------+-----+-----+-----+-----+-----+-----+-----+\n\n\n- The probability of each outcome is the same\n$$\nP(\\omega) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8} \\,, \\quad \\forall \\, \\omega \\in \\Omega\n$$\n\n\n- Define the random variable $X \\colon \\Omega \\to \\R$\n$$\nX(\\omega) :=  \\text{ Number of H in }  \\omega\n$$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The range of $X$ is $\\{0,1,2,3\\}$\n\n- Hence the only interesting values of $P_X$ are\n$$\nP(X=0) \\,, \\quad \nP(X=1) \\,, \\quad \nP(X=2) \\,, \\quad \nP(X=3)\n$$\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- We compute\n\\begin{align*}\nP(X=0) & = P(TTT) = \\frac{1}{8} \\\\\nP(X=1) & = P(TTH) + P(THT) + P(HTT) = \\frac{3}{8} \\\\\nP(X=2) & = P(HHT) + P(HTH) + P(THH) = \\frac{3}{8} \\\\\nP(X=3) & = P(HHH) = \\frac{1}{8}\n\\end{align*}\n\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The distribution of $X$ is summarized in the table below\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n\n\n\n## Random Variables {.smaller}\n### Cumulative distribution function\n\n\n**Recall**: The **distribution** of a r.v. $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( X \\in I \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n::: Definition\n### cdf\n\nThe **cumulative distribution function** or **cdf** of a r.v. $X \\colon \\Omega \\to \\mathbb{R}$ is\n$$\nF_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nF_X(x) := P_X (X \\leq x) \n$$\n\n:::\n\n\n**Intuition**: $F_X$ is the *primitive* of $P_X$\n\n\n\n\n## Random Variables {.smaller}\n### Example of cdf\n\n\n- Consider again 3 coin tosses and the r.v.\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- We computed that the distribution $P_X$ of $X$ is\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n::: {.column width=\"38%\"}\n\n- One can compute\n$$\nF_X(x) = \n\\begin{cases}\n0  & \\text{if } x < 0 \\\\\n\\frac{1}{8}  & \\text{if } 0 \\leq x < 1 \\\\\n\\frac{1}{2}  & \\text{if } 1 \\leq x < 2 \\\\\n\\frac{7}{8}  & \\text{if } 2 \\leq x < 3 \\\\\n1            & \\text{if } 3 \\leq x\n\\end{cases}\n$$\n\n:::\n\n\n::: {.column width=\"56%\"}\n\n- For example\n\\begin{align*}\nF_X(2.1) & = P(X \\leq 2.1) \\\\\n         & = P(X=0,1 \\text{ or } 2) \\\\\n         & = P(X=0) + P(X=1) + P(X=2) \\\\\n         & = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}\n\\end{align*}\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Example of cdf\n\n\n```{r}\n\n# Install and load the necessary package\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Define the step function\nstep_function <- function(x) {\n  if (x < 0) {\n    return(0)\n  } else if (x < 1) {\n    return(1/8)\n  } else if (x < 2) {\n    return(1/2)\n  } else if (x < 3) {\n    return(7/8)\n  } else {\n    return(1)\n  }\n}\n\n# Generate x values\nx_values <- seq(-1, 4, length.out = 1000)\n\n# Calculate corresponding y values using the step function\ny_values <- sapply(x_values, step_function)\n\n# Create a data frame for ggplot\ndf <- data.frame(x = x_values, y = y_values)\n\n# Plot the step function\nggplot(df, aes(x, y)) +\n  geom_step() +\n  labs(title = \"Step Function F_X(x)\",\n       x = \"x\",\n       y = \"F_X(x)\")\n\n```\n\n::: {.column width=\"52%\"}\n\n- Plot of $F_X$: it is **step function**\n- $F_X'=0$ except at jumps $x=0,1,2,3$\n\n:::\n\n::: {.column width=\"47%\"}\n\n- Globally $F_X'=P_X$ in the sense of **distributions**\n- This is an advanced analysis concept (not covered here)\n\n:::\n\n\n\n## Examples of Continuous RV {.smaller}\n\n\n- The **Normal distribution** with mean $\\mu$ and variance $\\sigma^2$ is denoted by \n$$\nN(\\mu,\\sigma^2)\n$$\n\n- The **pdf** of $N(\\mu,\\sigma^2)$ is\n$$\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n$$\n\n- $N(0,1)$ is the **standard normal** distribution\n\n\n\n\n\n\n## Examples of Continuous RV {.smaller}\n\n\n::: Theorem\n\nIf $X \\sim N(\\mu,\\sigma^2)$ then $Z \\sim N(0,1)$ where\n$$\nZ := \\frac{X-\\mu}{\\sigma}\n$$\n:::\n\nCheck\n\n::: Proof\n\nCheck\n\n:::\n\n\n\n\n# Part 4: <br>Statistics background {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n# Thank you! {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n\n::: footer\n\n<div color=\"#cc0164\"> </div>\n\n:::\n\n\n\n## References\n\n\n\n\n\n\n::: {.content-hidden}\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.8em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n::: {style=\"font-size: 0.6em\"}\n\n```{r}\n#| echo: false\n#| layout-ncol: 3\n#| tbl-cap: \"Dataset with 33 entries for Stock and Gold price pairs\"\n\n# Read dataset\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\n\nknitr::kable(data[1:11,], row.names = TRUE)\n\nknitr::kable(data[12:22,], row.names = TRUE)\n\nknitr::kable(data[23:33,], row.names = TRUE)\n\n```\n\n:::\n\n:::\n\n","srcMarkdownNoYaml":"\n\n\n# Lecture 1: <br>An introduction to Statistics {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Outline of Lecture 1\n\n1. Module info\n2. Motivation\n3. Probability background\n4. Statistics background\n\n\n\n\n\n# Part 1: <br>Module info {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Contact details\n\n- **Lecturer:** Dr. Silvio Fanzon\n- **Call me:**\n  * Silvio\n  * Dr. Fanzon\n- **Email:** S.Fanzon@hull.ac.uk\n- **Office:** Room 104a, Larkin Building\n- **Office hours:** Wednesday 12:00-13:00\n- **Meetings**: in my office or send me an Email\n\n\n\n\n\n\n## References\n### Main textbooks  \n\n\n::: {.column width=\"61%\"}\n\n<br>\n\nSlides are self-contained and based on the book\n\n- [@bingham-fry] Bingham, N. H. and Fry, J. M. \n<br> *Regression: Linear models in statistics.* <br> Springer, 2010\n\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/bingham_fry.png){width=82%}](https://link.springer.com/book/10.1007/978-1-84882-969-5)\n\n:::\n\n\n\n\n\n## References\n### Main textbooks \n\n\n::: {.column width=\"61%\"}\n<br>\n\n.. and also on the book\n\n- [@fry-burke] Fry, J. M. and Burke, M. \n<br>*Quantitative methods in finance using R.* \n<br>Open University Press, 2022\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/fry_burke.png){width=87%}](https://www.mheducation.co.uk/quantitative-methods-in-finance-using-r-9780335251261-emea-group)\n\n:::\n\n\n\n\n\n\n\n## References\n### Secondary References \n\n::: {.column width=\"69%\"}\n- [@casella-berger] Casella, G. and Berger R. L. <br>\n*Statistical inference.*\n<br> Second Edition, Brooks/Cole, 2002\n\n\n- [@degroot] DeGroot M. H. and Schervish M. J. <br> \n*Probability and Statistics.* \n<br> Fourth Edition, Addison-Wesley, 2012\n\n\n- [@dalgaard] Dalgaard, P. \n<br> *Introductory statistics with R.*\n<br> Second Edition, Springer, 2008   \n\n:::\n\n\n::: {.column width=\"30%\"}\n\n**Probability & Statistics manual**\n\n**Easier Probability & Statistics manual**\n\n**R manual**\n\n:::\n\n\n\n## Assessment \n\nThis course will be assessed as follows:\n\n<br>\n\n\n|**Type of Assessment**  | **Percentage of final grade** |\n|:-----                  |:-----                         |\n|  Coursework Portfolio  | 70%                           |\n|  Homework              | 30%                           |\n\n\n\n\n## Assessment \n### Rules for Coursework\n\n\n- Coursework available on Canvas from Week 2\n\n- We will discuss coursework exercises in class\n\n- Coursework must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Wednesday 1st May 2024**\n\n\n\n\n\n## Assessment\n### Rules for Homework\n\n\n- 10 Homework papers, posted weekly on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Each Homework paper is worth 14 points\n\n- Final Homework grade computation: \n    * Sum the top 7 scores, for a maximum of 98 points\n    * Bonus 2 points will be added to the final score\n\n- Homework papers must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Mondays** \n\n\n\n\n\n## Assessment\n### Key submission dates\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 1      | 5 Feb       |\n| Homework 2      | 12 Feb      |\n| Homework 3      | 19 Feb      |\n| Homework 4      | 26 Feb      |\n| Homework 5      | 4 Mar       |\n| Homework 6      | 11 Mar      |\n\n\n:::\n\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 7      | 18 Mar      |\n| Easter Break    | :sunglasses:|\n| Homework 8      | 8 Apr       |\n| Homework 9      | 15 Apr      |\n| Homework 10     | 22 Apr      |\n| Coursework      | 1 May       |\n\n:::\n\n\n\n## Assessment\n### How to submit assignments\n\n\n- Submit **PDFs only** on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- You have two options:\n\t* Write on tablet and submit PDF Output\n\t* Write on paper and **Scan in Black and White** using a **Scanner** or **Scanner App** (Tiny Scanner, Scanner Pro, ...)\n\n\n\n\n\n\n\n## The nature of Statistics\n### Statistics is a mathematical subject\n\n- Maths skills will give you a head start\n\n- There are other occasions where common sense and *detective skills* can be more important\n\n- Provides an early example of mathematics working in concert with the available computation\n\n\n\n\n## The nature of Statistics\n### We will use a combination of hand calculation and software\n\n- Recognises that you are maths students\n- Software (R) is really useful, particularly for dissertations\n- Please bring your laptop into class\n- Download R onto your laptop\n\n\n\n\n\n\n\n## Overview of the module\n\n\nModule has **11 Lectures**, divided into two parts:\n\n- **Part I** - Mathematical statistics\n\n- **Part II** - Applied statistics\n\n\n\n\n\n## Overview of the module\n### Part I - Mathematical statistics\n\n1. Introduction to statistics\n2. Normal distribution family and one-sample hypothesis tests\n3. Two-sample hypothesis tests\n4. The chi-squared test\n5. Non-parametric statistics\n6. The maths of regression\n\n\n\n\n\n## Overview of the module\n### Part II - Applied statistics\n\n\n7. An introduction to practical regression\n8. The extra sum of squares principle and regression modelling assumptions\n9. Violations of regression assumptions -- Autocorrelation\n10. Violation of regression assumptions -- Multicollinearity\n10. Dummy variable regression models \n\n\n\n\n\n# Part 2: <br>Motivation {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n## Simple but useful questions {.smaller}\n\n\n::: {.column width=\"48%\"}\n\n**Generic data:**\n\n- What is a *typical* observation\n  * What is the **mean**?\n\n- How spread out is the data?\n  * What is the **variance**?\n\n:::\n\n\n::: {.column width=\"48%\"}\n\n**Regression:**\n\n- What happens to $Y$ as $X$ increases?\n  * increases?\n  * decreases?\n  * nothing?\n\n:::\n\n\n\n**Statistics answers these questions systematically**\n\n- important for large datasets\n- The same mathematical machinery (normal family of distributions) can be applied to both questions\n\n\n\n\n## Analysing a general dataset\n\n**Two basic questions:**\n\n1. Location or mean\n2. Spread or variance\n\n\n**Statistics enables to answer systematically:**\n\n1. One sample and two-sample $t$-test\n2. Chi-squared test and $F$-test\n\n\n\n\n\n## Recall the following sketch\n\n![Curve represents data distribution](images/Fig1.png){width=90%}\n\n\n\n\n## Motivating regression\n\n**Basic question in regression:**\n\n- What happens to $Y$ as $X$ increases?\n\n  * increases?\n  * decreases?\n  * nothing?\n\n\n**In this way regression can be seen as a more advanced version of high-school maths**\n\n\n\n\n\n\n\n## Positive gradient\n\nAs $X$ increases $Y$ increases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = 1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n## Negative gradient\n\nAs $X$ increases $Y$ decreases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(-3, 3), \n  ylim = c(-3, 3), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = -1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Zero gradient\n\nChanges in $X$ do not affect $Y$\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 2.5, \n  b = 0, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Real data example\n\n\n- Real data is more **imperfect**\n- But the same basic idea applies\n- Example: \n    * $X =$ Stock price\n    * $Y =$ Gold price\n\n\n\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.75em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n\n::: {style=\"font-size: 0.55em\"}\n```{r}\n#| echo: false\n#| layout-ncol: 1\n\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\nknitr::kable(\n  list(data[1:11,], data[12:22,], data[23:33,]),\n  row.names = TRUE,\n  format = \"html\", \n  table.attr = 'class=\"table simple table-striped table-hover\"',\n) \n\n\n```\n\n\n:::\n\n\n\n\n## Real data example {.smaller}\n### Visualizing the data\n\n\n::::: {.columns style='display: flex !important; height: 80%;'}\n\n::: {.column width=\"38%\" style='display: flex; justify-content: center; align-items: center;'}\n\n- Plot Stock Price against Gold Price\n\n- Observation: \n\n  * As Stock price decreases, Gold price increases\n\n- Why? This might be because:\n    * Stock price decreases\n    * People invest in secure assets (Gold)\n    * Gold demand increases\n    * Gold price increases\n\n:::\n\n\n::: {.column width=\"61%\" style='display: flex; justify-content: center; align-items: center;'}\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\ndata1 <- read.table(\"datasets/L3eg1data.txt\")\nrealgoldprice<-data1[,1]\nrealstockprice<-data1[,2]\nplot(realstockprice, \n  realgoldprice, \n  xlab=\"\", \n  ylab=\"\")\n\nmtext(\"Stock Price\", side=1, line=3, cex=2)\nmtext(\"Gold Price\", side=2, line=2.5, cex=2)\n\n```\n\n:::\n\n:::::\n\n\n\n\n## Don't panic {.smaller}\n\n- Regression problems can look a lot harder than they really are\n  * Basic question remains the same: what happens to $Y$ as $X$ increases?\n\n- Beware of jargon. Various authors distinguish between\n  * Two variable regression model\n  * Multiple regression model\n  * Analysis of Variance\n  * Analysis of Covariance\n\n- Despite these apparent differences:\n  * Mathematical methodology stays (essentially) the same\n  * regression-fitting commands in R stay (essentially) the same\n\n\n\n\n\n# Part 3: <br>Probability background {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n\n## Sample space {.smaller}\n\n::: Definition\n### Sample space\n\nA set $\\Omega$ of all possible outcomes of some experiment\n\n:::\n\n\n**Examples**:\n\n- Coin toss: results in Heads $= H$ and Tails $= T$ \n$$\n\\Omega = \\{ H, T \\}\n$$\n\n- Student grade for *Statistical Models*: a number between $0$ and $100$\n$$\n\\Omega = \\{ x \\in \\mathbb{R} \\, \\colon \\, 0 \\leq x \\leq 100 \\} = [0,100]\n$$\n\n\n\n\n\n\n## Events {.smaller}\n\n\n::: Definition\n### Event\n\nA subset $E$ of the sample space $\\Omega$ (including $\\emptyset$ and $\\Omega$ itself)\n\n:::\n\n\n**Operations with events**:\n\n- **Union** of two events $A$ and $B$\n$$\nA \\cup B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ or } \\, x \\in B \\}\n$$\n\n- **Intersection** of two events $A$ and $B$\n$$\nA \\cap B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ and } \\, x \\in B \\}\n$$\n\n- **Complement** of an event $A$\n$$\nA^c  := \\{ x \\in \\Omega \\colon x \\notin A  \\}\n$$\n\n\n\n\n## Events {.smaller}\n\n\n**More Operations with events**:\n\n- **Infinite Union** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcup_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for some } \\, i \\in I \\}\n$$\n\n- **Infinite Intersection** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcap_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for all } \\, i \\in I \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n\n**Example**: Consider sample space and events\n$$\n\\Omega := (0,1] \\,, \\quad \nA_i = \\left[\\frac{1}{i} , 1 \\right] \\,, \\quad i \\in \\mathbb{N}\n$$\nThen\n$$\n\\bigcup_{i \\in I} A_i = (0,1] \\,, \\quad \n\\bigcap_{i \\in I} A_i = \\{ 1 \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Disjoint\n\nTwo events $A$ and $B$ are **disjoint** if\n$$\nA \\cap B = \\emptyset\n$$\nEvents $A_1, A_2, \\ldots$ are **pairwise disjoint** if\n$$\nA_i \\cap A_j = \\emptyset \\,, \\quad \\forall \\, i \\neq j\n$$\n:::\n\n\n::: Definition\n### Partition\n\nThe collection of events  $A_1, A_2, \\ldots$ is a **partition** of $\\Omega$ if\n\n1. $A_1, A_2, \\ldots$ are pairwise disjoint\n2. $\\Omega = \\cup_{i=1}^\\infty A_i$\n\n:::\n\n\n\n\n## What's a Probability? {.smaller}\n\n\n- To each event $E \\subset \\Omega$ we would like to associate a number\n$$\nP(E) \\in [0,1]\n$$\n\n- The number $P(E)$ is called the **probability** of $E$\n\n- The number $P(E)$ models the **frequency of occurrence** of $E$:\n\n    * $P(E)$ small means $E$ has low chance of occurring\n    * $P(E)$ large means $E$ has high chance of occurring\n\n- **Technical issue**:\n\n    * One cannot associate a number $P(E)$ for all events in $\\Omega$\n    * Probability function $P$ only defined for a **smaller** family of events\n    * Such family of events is called $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n\n\n::: Definition\n### sigma-algebra\n\nLet $\\mathcal{B}$ be a collection of events. We say that $\\mathcal{B}$ is a $\\sigma$-algebra if\n\n1. $\\emptyset \\in \\mathcal{B}$\n2. If $A \\in \\mathcal{B}$ then $A^c \\in \\mathcal{B}$\n3. If $A_1,A_2 , \\ldots \\in \\mathcal{B}$ then $\\cup_{i=1}^\\infty A_i \\in \\mathcal{B}$\n\n\n:::\n\n\n\n**Remarks**:\n\n- Since $\\emptyset \\in \\mathcal{B}$ and $\\emptyset^c = \\Omega$, we deduce that\n$$\n\\Omega \\in \\mathcal{B}\n$$\n\n- Thanks to DeMorgan's Law we have that\n$$\nA_1,A_2 , \\ldots \\in \\mathcal{B} \\quad \\implies \\quad \\cap_{i=1}^\\infty A_i \\in \\mathcal{B}\n$$\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\nSuppose $\\Omega$ is any set: \n\n- Then\n$$\n\\mathcal{B} = \\{ \\emptyset, \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n- The power set of $\\Omega$\n$$\n\\mathcal{B} = \\operatorname{Power} (\\Omega) := \\{ A \\colon A \\subset \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\n\n- If $\\Omega$ has $n$ elements then $\\mathcal{B} = \\operatorname{Power} (\\Omega)$ contains $2^n$ sets\n\n- If $\\Omega = \\{ 1,2,3\\}$ then\n\\begin{align*}\n\\mathcal{B} = \\operatorname{Power} (\\Omega) = \\big\\{ & \\{1\\} , \\, \\{2\\}, \\, \\{3\\}  \\\\\n                                       &  \\{1,2\\} , \\, \\{2,3\\}, \\, \\{1,3\\}      \\\\\n                                       & \\emptyset , \\{1,2,3\\}   \\big\\}\n\\end{align*}\n\n- If $\\Omega$ is **uncountable** then the power set of $\\Omega$ is not easy to describe\n\n\n\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Question\n\n$\\mathbb{R}$ is uncountable. Which $\\sigma$-algebra do we consider?\n\n:::\n\n\n::: Definition \n### Lebesgue sigma-algebra\n\n\nThe Lebesgue $\\sigma$-algebra on $\\mathbb{R}$ is the smallest $\\sigma$-algebra $\\mathcal{L}$ containing all sets of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n:::\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Important\n\nTherefore the events of $\\R$ are \n\n- Intervals\n- Unions and intersection of intervals\n- Countable Unions and intersection of intervals\n\n:::\n\n\n::: Warning \n\n- I only told you that the Lebsesgue $\\sigma$-algebra $\\mathcal{L}$ **exists**\n- Explicitly showing that $\\mathcal{L}$ exists is not easy, see [@rosenthal]\n\n:::\n\n\n\n\n## Probability measure {.smaller}\n\nSuppose given:\n\n- $\\Omega$ sample space \n- $\\mathcal{B}$ a $\\sigma$-algebra on $\\Omega$\n\n::: Definition\n### Probability measure\n\nA **probability measure** on $\\Omega$ is a map\n$$\nP \\colon \\mathcal{B} \\to [0,1]\n$$\nsuch that the **Axioms of Probability** hold\n\n1. $P(\\Omega) = 1$\n2. If $A_1, A_2,\\ldots$ are pairwise disjoint then\n$P\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  = \\sum_{i=1}^\\infty P(A_i)$\n\n:::\n\n\n\n\n## Properties of Probability {.smaller}\n\nLet $A, B \\in \\mathcal{B}$. As a consequence of the Axioms of Probability:\n\n\n1. $P(\\emptyset) = 0$\n2. If $A$ and $B$ are disjoint then\n$$\nP(A \\cup B) = P(A) + P(B)\n$$\n3. $P(A^c) = 1 - P(A)$\n4. $P(A) = P(A \\cap B) + P(A \\cap B^c)$\n5. $P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$\n6. If $A \\subset B$ then\n$$\nP(A) \\leq P(B)\n$$\n\n\n\n\n\n## Properties of Probability {.smaller} \n\n7. Suppose $A$ is an event and $B_1,B_2, \\ldots$ a partition of $\\Omega$. Then\n$$\nP(A) = \\sum_{i=1}^\\infty  P(A \\cap B_i)\n$$\n8. Suppose $A_1,A_2, \\ldots$ are events. Then\n$$\nP\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  \\leq  \\sum_{i=1}^\\infty P(A_i)\n$$\n\n\n\n\n## Example: Fair Coin Toss {.smaller}\n\n- The sample space for **coin toss** is $\\Omega = \\{ H, T \\}$\n\n- We take as $\\sigma$-algebra the power set of $\\Omega$\n$$\n\\mathcal{B} = \\{  \\emptyset , \\, \\{H\\}  , \\, \\{T\\} , \\, \\{H,T\\}   \\}\n$$\n\n- We suppose that the coin is fair\n\n    * This means $P \\colon \\mathcal{B} \\to [0,1]$ satisfies\n    $$\n    P(\\{H\\}) = P(\\{T\\})\n    $$\n    * Assuming the above we get\n    $$\n    1 = P(\\Omega) = P(\\{H\\} \\cup \\{T\\}) = \n    P(\\{H\\}) + P(\\{T\\}) = 2 P(\\{H\\})\n    $$\n    * Therefore \n    $$\n    P(\\{H\\}) = P(\\{T\\}) = \\frac12\n    $$\n\n\n\n\n\n## Conditional Probability {.smaller}\n\n::: Definition\n### Conditional Probability\n\nLet $A,B$ be events in $\\Omega$ with \n$$\nP(B)>0\n$$\nThe **conditional probability** of $A$ given $B$ is\n$$\nP(A|B) := \\frac{P(A \\cap B)}{P(B)} \n$$\n\n:::\n\n\n- $P(A|B)$ represents the probability of $A$, knowing that $B$ happened\n- The function $A \\mapsto P(A|B)$ is a probability measure on $\\Omega$\n\n\n\n\n## Bayes' Rule {.smaller}\n\n\n- For two events $A$ and $B$ is holds\n\n$$\nP(A | B ) = P(B|A) \\frac{P(A)}{P(B)}\n$$\n\n\n- Given a partition $A_1, A_2, \\ldots$ of the sample space we have\n\n$$\nP(A_i | B ) = \\frac{ P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B | A_j) P(A_j)}\n$$\n\n\n\n\n\n## Independence {.smaller}\n\n\n::: Definition\n\nTwo events $A$ and $B$ are **independent** if \n$$\nP(A \\cap B) = P(A)P(B)\n$$\nA collection of events $A_1 , \\ldots ,A_n$ are **mutually independent** if\nfor any subcollection $A_{i_1}, \\ldots, A_{i_k}$ it holds\n$$\nP \\left(  \\bigcap_{j=1}^k A_j  \\right) = \\prod_{j=1}^k  P(A_{i_j})\n$$\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Motivation\n\n- Consider the experiment of flipping a coin $50$ times\n- The sample space consists of $2^{50}$ elements\n- Elements are vectors of $50$ entries recording the outcome $H$ or $T$ of each flip\n- This is a very large sample space!\n\n\nSuppose we are only interested in\n$$\nX = \\text{ number of } \\, H \\, \\text{ in } \\, 50 \\, \\text{flips}\n$$\n\n- Then the new sample space is the set of integers\n$$\n\\{ 0,1,2,\\ldots,50\\}\n$$\n- This is much smaller!\n- $X$ is called a Random Variable\n\n\n\n\n## Random Variables {.smaller}\n\n\nAssume given\n\n- $\\Omega$ sample space\n- $\\mathcal{B}$ a $\\sigma$-algebra of events on $\\Omega$\n- $P \\colon \\mathcal{B} \\to [0,1]$ a probability measure\n\n\n::: Definition\n### Random variable\n\nA function $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\nWe will abbreviate Random Variable with r.v.\n\n\n\n\n\n## Random Variables {.smaller}\n### Technical remark \n\n\n::: Definition\n### Random variable\n\nA [**measurable**]{style=\"color:#cc0164;\"}\nfunction $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\n**Technicality**: \n$X$ is a [**measurable**]{style=\"color:#cc0164;\"}\nfunction if\n$$\n\\{ X \\in I \\} := \\{ \\omega \\in \\Omega \\colon X(\\omega) \\in I \\} \\in \\mathcal{B} \\,, \\quad \\forall \\, I \\in \\mathcal{L}\n$$\nwhere \n\n- $\\mathcal{L}$ is the Lebsgue $\\sigma$-algebra on $\\mathbb{R}$\n- $\\mathcal{B}$ is the given $\\sigma$-algebra on $\\Omega$\n\n\n\n\n## Random Variables {.smaller}\n### Notation\n\n- In particular $I \\in \\mathcal{L}$ can be of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n- In this case the set \n$$\n\\{X \\in I\\} \\in \\mathcal{B}\n$$ \nis denoted by, respectively:\n$$\n\\{ a < X < b \\} \\,, \\quad \\{ a < X \\leq b \\} \\,, \\quad \\{ a \\leq X < b \\} \\,, \\quad \\{ a \\leq X \\leq b \\}\n$$\n\n- If $a=b=x$ then $I=[x,x]=\\{x\\}$. Then we denote\n$$\n\\{X \\in I\\} = \\{X = x\\}\n$$\n\n\n## Random Variables {.smaller}\n### Why do we require measurability?\n\n**Answer**: Because it allows to define a new probability measure on $\\mathbb{R}$\n\n\n::: Definition\n### Distribution\n\nThe **distribution** of a random variable $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( \\{X \\in I\\} \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n:::\n\n\n**Note**: \n\n- One can show that $P_X$ satisfies the *Probability Axioms*\n- Thus $P_X$ is a probability measure on $\\mathbb{R}$\n- In the future we will denote \n$$\nP \\left( X \\in I \\right) := P \\left( \\{X \\in I\\} \\right)\n$$\n\n\n\n\n\n\n\n## Random Variables {.smaller}\n### Why is the distribution useful?\n\n**Answer**: Because it allows to define a random variable $X$\n\n- by specifying the distribution values\n$$\nP \\left( X \\in I \\right) \n$$\n- rather than defining an explicit function $X \\colon \\Omega \\to \\mathbb{R}$\n\n\n**Important**: More often than not\n\n- We care about the distribution of $X$\n- We do not care about how $X$ is defined\n\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Sample space $\\Omega$ given by the below values of $\\omega$\n\n+:---------:+:----:+:---:+:---:+:---:+:---:+:---:+:---:+:---:+\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n+-----------+------+-----+-----+-----+-----+-----+-----+-----+\n\n\n- The probability of each outcome is the same\n$$\nP(\\omega) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8} \\,, \\quad \\forall \\, \\omega \\in \\Omega\n$$\n\n\n- Define the random variable $X \\colon \\Omega \\to \\R$\n$$\nX(\\omega) :=  \\text{ Number of H in }  \\omega\n$$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The range of $X$ is $\\{0,1,2,3\\}$\n\n- Hence the only interesting values of $P_X$ are\n$$\nP(X=0) \\,, \\quad \nP(X=1) \\,, \\quad \nP(X=2) \\,, \\quad \nP(X=3)\n$$\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- We compute\n\\begin{align*}\nP(X=0) & = P(TTT) = \\frac{1}{8} \\\\\nP(X=1) & = P(TTH) + P(THT) + P(HTT) = \\frac{3}{8} \\\\\nP(X=2) & = P(HHT) + P(HTH) + P(THH) = \\frac{3}{8} \\\\\nP(X=3) & = P(HHH) = \\frac{1}{8}\n\\end{align*}\n\n\n\n\n\n## Random Variables {.smaller}\n### Example - Three coin tosses\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The distribution of $X$ is summarized in the table below\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n\n\n\n## Random Variables {.smaller}\n### Cumulative distribution function\n\n\n**Recall**: The **distribution** of a r.v. $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( X \\in I \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n::: Definition\n### cdf\n\nThe **cumulative distribution function** or **cdf** of a r.v. $X \\colon \\Omega \\to \\mathbb{R}$ is\n$$\nF_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nF_X(x) := P_X (X \\leq x) \n$$\n\n:::\n\n\n**Intuition**: $F_X$ is the *primitive* of $P_X$\n\n\n\n\n## Random Variables {.smaller}\n### Example of cdf\n\n\n- Consider again 3 coin tosses and the r.v.\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- We computed that the distribution $P_X$ of $X$ is\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n::: {.column width=\"38%\"}\n\n- One can compute\n$$\nF_X(x) = \n\\begin{cases}\n0  & \\text{if } x < 0 \\\\\n\\frac{1}{8}  & \\text{if } 0 \\leq x < 1 \\\\\n\\frac{1}{2}  & \\text{if } 1 \\leq x < 2 \\\\\n\\frac{7}{8}  & \\text{if } 2 \\leq x < 3 \\\\\n1            & \\text{if } 3 \\leq x\n\\end{cases}\n$$\n\n:::\n\n\n::: {.column width=\"56%\"}\n\n- For example\n\\begin{align*}\nF_X(2.1) & = P(X \\leq 2.1) \\\\\n         & = P(X=0,1 \\text{ or } 2) \\\\\n         & = P(X=0) + P(X=1) + P(X=2) \\\\\n         & = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}\n\\end{align*}\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Example of cdf\n\n\n```{r}\n\n# Install and load the necessary package\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Define the step function\nstep_function <- function(x) {\n  if (x < 0) {\n    return(0)\n  } else if (x < 1) {\n    return(1/8)\n  } else if (x < 2) {\n    return(1/2)\n  } else if (x < 3) {\n    return(7/8)\n  } else {\n    return(1)\n  }\n}\n\n# Generate x values\nx_values <- seq(-1, 4, length.out = 1000)\n\n# Calculate corresponding y values using the step function\ny_values <- sapply(x_values, step_function)\n\n# Create a data frame for ggplot\ndf <- data.frame(x = x_values, y = y_values)\n\n# Plot the step function\nggplot(df, aes(x, y)) +\n  geom_step() +\n  labs(title = \"Step Function F_X(x)\",\n       x = \"x\",\n       y = \"F_X(x)\")\n\n```\n\n::: {.column width=\"52%\"}\n\n- Plot of $F_X$: it is **step function**\n- $F_X'=0$ except at jumps $x=0,1,2,3$\n\n:::\n\n::: {.column width=\"47%\"}\n\n- Globally $F_X'=P_X$ in the sense of **distributions**\n- This is an advanced analysis concept (not covered here)\n\n:::\n\n\n\n## Examples of Continuous RV {.smaller}\n\n\n- The **Normal distribution** with mean $\\mu$ and variance $\\sigma^2$ is denoted by \n$$\nN(\\mu,\\sigma^2)\n$$\n\n- The **pdf** of $N(\\mu,\\sigma^2)$ is\n$$\nf(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n$$\n\n- $N(0,1)$ is the **standard normal** distribution\n\n\n\n\n\n\n## Examples of Continuous RV {.smaller}\n\n\n::: Theorem\n\nIf $X \\sim N(\\mu,\\sigma^2)$ then $Z \\sim N(0,1)$ where\n$$\nZ := \\frac{X-\\mu}{\\sigma}\n$$\n:::\n\nCheck\n\n::: Proof\n\nCheck\n\n:::\n\n\n\n\n# Part 4: <br>Statistics background {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n# Thank you! {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n\n::: footer\n\n<div color=\"#cc0164\"> </div>\n\n:::\n\n\n\n## References\n\n\n\n\n\n\n::: {.content-hidden}\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.8em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n::: {style=\"font-size: 0.6em\"}\n\n```{r}\n#| echo: false\n#| layout-ncol: 3\n#| tbl-cap: \"Dataset with 33 entries for Stock and Gold price pairs\"\n\n# Read dataset\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\n\nknitr::kable(data[1:11,], row.names = TRUE)\n\nknitr::kable(data[12:22,], row.names = TRUE)\n\nknitr::kable(data[23:33,], row.names = TRUE)\n\n```\n\n:::\n\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"katex","slide-level":2,"to":"revealjs","filters":["custom-numbered-blocks"],"toc":false,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\">\n"}],"from":"markdown+emoji","output-file":"lecture_1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.542","auto-stretch":true,"bibliography":["../teaching.bib","teaching.bib"],"csl":"../elsevier-with-titles.csl","search":false,"favicon":"images/favicon.png","custom-numbered-blocks":{"groups":{"thmlike":{"colors":["f4cce0","db4d92"],"boxstyle":"foldbox.simple","collapse":false,"listin":["mathstuff"],"numbered":false},"todos":"default"},"classes":{"Theorem":{"group":"thmlike"},"Corollary":{"group":"thmlike"},"Conjecture":{"group":"thmlike"},"Proposition":{"group":"thmlike"},"Lemma":{"group":"thmlike"},"Question":{"group":"thmlike"},"Axiom":{"group":"thmlike"},"Important":{"group":"thmlike","numbered":false},"Warning":{"group":"thmlike","numbered":false},"Problem":{"group":"thmlike","numbered":false},"Definition":{"group":"thmlike","colors":["c6e6ed","1995ad"]},"Notation":{"group":"thmlike","colors":["c6e6ed","1995ad"]},"Remark":{"group":"thmlike","colors":["bce5de","21aa93"]},"Example":{"group":"thmlike","colors":["bce5de","21aa93"]},"Proof":{"group":"thmlike","colors":["f1f1f2","c0c0c1"],"numbered":false},"TODO":{"label":"To do","colors":["e7b1b4","8c3236"],"group":"todos","listin":["stilltodo"]},"DONE":{"label":"Done","colors":["cce7b1","86b754"],"group":"todos"}}},"footer":"[Homepage](/index.html) &nbsp;&nbsp;&nbsp;&nbsp; [License](/license.html) &nbsp;&nbsp;&nbsp;&nbsp; [Contact](https://www.silviofanzon.com/contact)","theme":["default","custom-theme-slides.scss"],"transition":"none","backgroundTransition":"fade","previewLinks":"auto","slideNumber":true,"touch":true,"callout-icon":false,"keyboard":true,"title":"Statistical Models","subtitle":"Lecture 1","author":[{"name":"Dr. Silvio Fanzon","id":"sf","email":"S.Fanzon@hull.ac.uk","url":"https://www.silviofanzon.com","affiliations":"University of Hull"},{"name":"Dr. John Fry","id":"jf","email":"J.M.Fry@hull.ac.uk","url":"https://www.hull.ac.uk/staff-directory/john-fry","affiliations":"University of Hull"}]}}},"projectFormats":["html"]}