{"title":"Statistical Models","markdown":{"yaml":{"title":"Statistical Models","subtitle":"Lecture 1","from":"markdown+emoji","author":[{"name":"Dr. Silvio Fanzon","id":"sf","email":"S.Fanzon@hull.ac.uk","url":"https://www.silviofanzon.com","affiliations":"University of Hull"},{"name":"Dr. John Fry","id":"jf","email":"J.M.Fry@hull.ac.uk","url":"https://www.hull.ac.uk/staff-directory/john-fry","affiliations":"University of Hull"}]},"headingText":"Lecture 1: <br>An introduction to Statistics","headingAttr":{"id":"","classes":[],"keyvalue":[["background-color","#cc0164"],["visibility","uncounted"]]},"containsRefs":false,"markdown":"\n\n\n\n::: {.content-hidden}\n$\n{{< include macros.tex >}}\n$\n:::\n\n\n\n\n\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Outline of Lecture 1\n\n1. Module info\n2. Introduction\n3. Probability revision\n4. Moment generating functions\n\n\n\n\n\n# Part 1: <br>Module info {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Contact details\n\n- **Lecturer:** Dr. Silvio Fanzon\n- **Call me:**\n  * Silvio\n  * Dr. Fanzon\n- **Email:** S.Fanzon@hull.ac.uk\n- **Office:** Room 104a, Larkin Building\n- **Office hours:** Wednesday 12:00-13:00\n- **Meetings**: in my office or send me an Email\n\n\n\n\n\n## Questions\n\n\n- If you have any questions please feel free to ``email me``\n\n- We will address ``Homework`` and ``Coursework`` in class\n\n- In addition, please do not hesitate to attend ``office hours``\n\n\n\n## Lectures\n\nEach week we have\n\n- 2 Lectures of 2h each\n- 1 Tutorial of 1h\n\n|    Session    |      Date        |      Place        |\n|---------------|------------------|-------------------|\n|  Lecture 1    |  Wed 10:00-12:00 | Wilberforce LR 22 |\n|  Lecture 2    |  Thu 15:00-17:00 | Wilberforce LR 10 |\n|  Tutorial     |  Thu 11:00-12:00 | Wilberforce LR 7  |\n\n\n\n\n\n## Assessment \n\nThis course will be assessed as follows:\n\n<br>\n\n\n|**Type of Assessment**  | **Percentage of final grade** |\n|:-----                  |:-----                         |\n|  Coursework Portfolio  | 70%                           |\n|  Homework              | 30%                           |\n\n\n\n\n\n## Rules for Coursework\n\n\n- Coursework available on Canvas from Week 3\n\n- We will discuss coursework exercises in class\n\n- Coursework must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- Deadline: **14:00 on Thursday 2nd May**\n\n\n\n\n\n\n## Rules for Homework\n\n\n- 10 Homework papers, posted weekly on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Each Homework paper is worth 14 points\n\n- Final Homework grade computation: \n    * Sum the top 7 scores (max score 98 points)\n    * Bonus 2 points will be added to the final score\n\n- Homework papers must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Deadlines: **14:00 on Mondays** \n\n\n\n\n\n\n## Key submission dates\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 1      | 5 Feb       |\n| Homework 2      | 12 Feb      |\n| Homework 3      | 19 Feb      |\n| Homework 4      | 26 Feb      |\n| Homework 5      | 4 Mar       |\n| Homework 6      | 11 Mar      |\n\n\n:::\n\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 7      | 18 Mar      |\n| Easter Break    | :sunglasses:|\n| Homework 8      | 8 Apr       |\n| Homework 9      | 15 Apr      |\n| Homework 10     | 22 Apr      |\n| Coursework      | 2 May       |\n\n:::\n\n\n\n\n## How to submit assignments\n\n\n- Submit **PDFs only** on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- You have two options:\n\t* Write on tablet and submit PDF Output\n\t* Write on paper and **Scan in Black and White** using a **Scanner** or **Scanner App** (Tiny Scanner, Scanner Pro, ...)\n\n\n\n\n**Important**: I will not mark\n\n\n- Assignments submitted **outside of Canvas**\n- Assignments submitted **After the Deadline**\n\n\n\n\n\n\n## References\n### Main textbooks  \n\n\n::: {.column width=\"61%\"}\n\n<br>\n\nSlides are self-contained and based on the book\n\n- [@bingham-fry] Bingham, N. H. and Fry, J. M. \n<br> *Regression: Linear models in statistics.* <br> Springer, 2010\n\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/bingham_fry.png){width=82%}](https://link.springer.com/book/10.1007/978-1-84882-969-5)\n\n:::\n\n\n\n\n\n## References\n### Main textbooks \n\n\n::: {.column width=\"61%\"}\n<br>\n\n.. and also on the book\n\n- [@fry-burke] Fry, J. M. and Burke, M. \n<br>*Quantitative methods in finance using R.* \n<br>Open University Press, 2022\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/fry_burke.png){width=87%}](https://www.mheducation.co.uk/quantitative-methods-in-finance-using-r-9780335251261-emea-group)\n\n:::\n\n\n\n\n\n\n\n## References\n### Secondary References \n\n::: {.column width=\"69%\"}\n- [@casella-berger] Casella, G. and Berger R. L. <br>\n*Statistical inference.*\n<br> Second Edition, Brooks/Cole, 2002\n\n\n- [@degroot] DeGroot M. H. and Schervish M. J. <br> \n*Probability and Statistics.* \n<br> Fourth Edition, Addison-Wesley, 2012\n\n\n- [@dalgaard] Dalgaard, P. \n<br> *Introductory statistics with R.*\n<br> Second Edition, Springer, 2008   \n\n:::\n\n\n::: {.column width=\"30%\"}\n\n**Probability & Statistics manual**\n\n**Easier Probability & Statistics manual**\n\n**R manual**\n\n:::\n\n\n\n\n\n\n# Part 2: <br>Introduction {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## The nature of Statistics\n### Statistics is a mathematical subject\n\n- Maths skills will give you a head start\n\n- There are other occasions where common sense and *detective skills* can be more important\n\n- Provides an early example of mathematics working in concert with the available computation\n\n\n\n\n## The nature of Statistics\n### We will use a combination of hand calculation and software\n\n- Recognises that you are maths students\n- Software (R) is really useful, particularly for dissertations\n- Please bring your laptop into class\n- Download R onto your laptop\n\n\n\n\n\n\n\n## Overview of the module\n\n\nModule has **11 Lectures**, divided into two parts:\n\n- **Part I** - Mathematical statistics\n\n- **Part II** - Applied statistics\n\n\n\n\n\n## Overview of the module\n### Part I - Mathematical statistics\n\n1. Introduction to statistics\n2. Normal distribution family and one-sample hypothesis tests\n3. Two-sample hypothesis tests\n4. The chi-squared test\n5. Non-parametric statistics\n6. The maths of regression\n\n\n\n\n\n## Overview of the module\n### Part II - Applied statistics\n\n\n7. An introduction to practical regression\n8. The extra sum of squares principle and regression modelling assumptions\n9. Violations of regression assumptions -- Autocorrelation\n10. Violation of regression assumptions -- Multicollinearity\n10. Dummy variable regression models \n\n\n\n\n\n\n\n## Simple but useful questions {.smaller}\n\n\n::: {.column width=\"48%\"}\n\n**Generic data:**\n\n- What is a *typical* observation\n  * What is the **mean**?\n\n- How spread out is the data?\n  * What is the **variance**?\n\n:::\n\n\n::: {.column width=\"48%\"}\n\n**Regression:**\n\n- What happens to $Y$ as $X$ increases?\n  * increases?\n  * decreases?\n  * nothing?\n\n:::\n\n\n\n**Statistics answers these questions systematically**\n\n- important for large datasets\n- The same mathematical machinery (normal family of distributions) can be applied to both questions\n\n\n\n\n## Analysing a general dataset\n\n**Two basic questions:**\n\n1. Location or mean\n2. Spread or variance\n\n\n**Statistics enables to answer systematically:**\n\n1. One sample and two-sample $t$-test\n2. Chi-squared test and $F$-test\n\n\n\n\n\n## Recall the following sketch\n\n![Curve represents data distribution](images/Fig1.png){width=90%}\n\n\n\n\n## Motivating regression\n\n**Basic question in regression:**\n\n- What happens to $Y$ as $X$ increases?\n\n  * increases?\n  * decreases?\n  * nothing?\n\n\n**In this way regression can be seen as a more advanced version of high-school maths**\n\n\n\n\n\n\n\n## Positive gradient\n\nAs $X$ increases $Y$ increases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = 1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n## Negative gradient\n\nAs $X$ increases $Y$ decreases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(-3, 3), \n  ylim = c(-3, 3), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = -1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Zero gradient\n\nChanges in $X$ do not affect $Y$\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 2.5, \n  b = 0, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Real data example\n\n\n- Real data is more **imperfect**\n- But the same basic idea applies\n- Example: \n    * $X =$ Stock price\n    * $Y =$ Gold price\n\n\n\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.75em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n\n::: {style=\"font-size: 0.55em\"}\n```{r}\n#| echo: false\n#| layout-ncol: 1\n\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\nknitr::kable(\n  list(data[1:11,], data[12:22,], data[23:33,]),\n  row.names = TRUE,\n  format = \"html\", \n  table.attr = 'class=\"table simple table-striped table-hover\"',\n) \n\n\n```\n\n\n:::\n\n\n\n\n## Real data example {.smaller}\n### Visualizing the data\n\n\n::::: {.columns style='display: flex !important; height: 80%;'}\n\n::: {.column width=\"38%\" style='display: flex; justify-content: center; align-items: center;'}\n\n- Plot Stock Price against Gold Price\n\n- Observation: \n\n  * As Stock price decreases, Gold price increases\n\n- Why? This might be because:\n    * Stock price decreases\n    * People invest in secure assets (Gold)\n    * Gold demand increases\n    * Gold price increases\n\n:::\n\n\n::: {.column width=\"61%\" style='display: flex; justify-content: center; align-items: center;'}\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\ndata1 <- read.table(\"datasets/L3eg1data.txt\")\nrealgoldprice<-data1[,1]\nrealstockprice<-data1[,2]\nplot(realgoldprice,\n  realstockprice, \n  xlab=\"\", \n  ylab=\"\")\n\nmtext(\"Stock Price\", side=1, line=3, cex=2)\nmtext(\"Gold Price\", side=2, line=2.5, cex=2)\n\n```\n\n:::\n\n:::::\n\n\n\n\n## Don't panic {.smaller}\n\n- Regression problems can look a lot harder than they really are\n  * Basic question remains the same: what happens to $Y$ as $X$ increases?\n\n- Beware of jargon. Various authors distinguish between\n  * Two variable regression model\n  * Multiple regression model\n  * Analysis of Variance\n  * Analysis of Covariance\n\n- Despite these apparent differences:\n  * Mathematical methodology stays (essentially) the same\n  * regression-fitting commands in R stay (essentially) the same\n\n\n\n\n\n# Part 3: <br>Probability revision {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Probability revision {.smaller}\n\n\n- We start with reviewing some fundamental Probability notions\n- You saw these in the Y1 module **Introduction to Probability & Statistics**\n- We will adopt a slightly more mature mathematical approach\n- Remember: The mathematical description might look (a bit) different, but the concepts are the same\n\n**Topics reviewed**:\n    \n::: {.column width=\"38%\"}\n    \n- Sample space\n- Events\n- Probability measure\n- Conditional probability\n- Events independence\n\n:::\n\n::: {.column width=\"38%\"}\n\n- Random Variable\n- Distribution\n- cdf\n- pmf\n- pdf\n\n:::\n\n\n\n\n\n## Sample space {.smaller}\n\n\n::: Definition\n### Sample space\n\nA set $\\Omega$ of all possible outcomes of some experiment\n\n:::\n\n\n**Examples**:\n\n- Coin toss: results in Heads $= H$ and Tails $= T$ \n$$\n\\Omega = \\{ H, T \\}\n$$\n\n- Student grade for *Statistical Models*: a number between $0$ and $100$\n$$\n\\Omega = \\{ x \\in \\mathbb{R} \\, \\colon \\, 0 \\leq x \\leq 100 \\} = [0,100]\n$$\n\n\n\n\n\n\n## Events {.smaller}\n\n\n::: Definition\n### Event\n\nA subset $E$ of the sample space $\\Omega$ (including $\\emptyset$ and $\\Omega$ itself)\n\n:::\n\n\n**Operations with events**:\n\n- **Union** of two events $A$ and $B$\n$$\nA \\cup B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ or } \\, x \\in B \\}\n$$\n\n- **Intersection** of two events $A$ and $B$\n$$\nA \\cap B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ and } \\, x \\in B \\}\n$$\n\n\n\n\n## Events {.smaller}\n\n\n**More Operations with events**:\n\n- **Complement** of an event $A$\n$$\nA^c  := \\{ x \\in \\Omega \\colon x \\notin A  \\}\n$$\n\n\n- **Infinite Union** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcup_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for some } \\, i \\in I \\}\n$$\n\n- **Infinite Intersection** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcap_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for all } \\, i \\in I \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n\n**Example**: Consider sample space and events\n$$\n\\Omega := (0,1] \\,, \\quad \nA_i = \\left[\\frac{1}{i} , 1 \\right] \\,, \\quad i \\in \\mathbb{N}\n$$\nThen\n$$\n\\bigcup_{i \\in I} A_i = (0,1] \\,, \\quad \n\\bigcap_{i \\in I} A_i = \\{ 1 \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Disjoint\n\nTwo events $A$ and $B$ are **disjoint** if\n$$\nA \\cap B = \\emptyset\n$$\nEvents $A_1, A_2, \\ldots$ are **pairwise disjoint** if\n$$\nA_i \\cap A_j = \\emptyset \\,, \\quad \\forall \\, i \\neq j\n$$\n:::\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Partition\n\nThe collection of events  $A_1, A_2, \\ldots$ is a **partition** of $\\Omega$ if\n\n1. $A_1, A_2, \\ldots$ are pairwise disjoint\n2. $\\Omega = \\cup_{i=1}^\\infty A_i$\n\n:::\n\n\n\n\n\n\n## What's a Probability? {.smaller}\n\n\n- To each event $E \\subset \\Omega$ we would like to associate a number\n$$\nP(E) \\in [0,1]\n$$\n\n- The number $P(E)$ is called the **probability** of $E$\n\n- The number $P(E)$ models the **frequency of occurrence** of $E$:\n\n    * $P(E)$ small means $E$ has low chance of occurring\n    * $P(E)$ large means $E$ has high chance of occurring\n\n- **Technical issue**:\n\n    * One cannot associate a number $P(E)$ for all events in $\\Omega$\n    * Probability function $P$ only defined for a **smaller** family of events\n    * Such family of events is called $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n\n\n::: Definition\n### sigma-algebra\n\nLet $\\mathcal{B}$ be a collection of events. We say that $\\mathcal{B}$ is a $\\sigma$-algebra if\n\n1. $\\emptyset \\in \\mathcal{B}$\n2. If $A \\in \\mathcal{B}$ then $A^c \\in \\mathcal{B}$\n3. If $A_1,A_2 , \\ldots \\in \\mathcal{B}$ then $\\cup_{i=1}^\\infty A_i \\in \\mathcal{B}$\n\n\n:::\n\n\n\n**Remarks**:\n\n- Since $\\emptyset \\in \\mathcal{B}$ and $\\emptyset^c = \\Omega$, we deduce that\n$\\Omega \\in \\mathcal{B}$\n\n- Thanks to DeMorgan's Law we have that\n$$\nA_1,A_2 , \\ldots \\in \\mathcal{B} \\quad \\implies \\quad \\cap_{i=1}^\\infty A_i \\in \\mathcal{B}\n$$\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\nSuppose $\\Omega$ is any set: \n\n- Then\n$$\n\\mathcal{B} = \\{ \\emptyset, \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n- The power set of $\\Omega$\n$$\n\\mathcal{B} = \\operatorname{Power} (\\Omega) := \\{ A \\colon A \\subset \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\n\n- If $\\Omega$ has $n$ elements then $\\mathcal{B} = \\operatorname{Power} (\\Omega)$ contains $2^n$ sets\n\n- If $\\Omega = \\{ 1,2,3\\}$ then\n\\begin{align*}\n\\mathcal{B} = \\operatorname{Power} (\\Omega) = \\big\\{ & \\{1\\} , \\, \\{2\\}, \\, \\{3\\}  \\\\\n                                       &  \\{1,2\\} , \\, \\{2,3\\}, \\, \\{1,3\\}      \\\\\n                                       & \\emptyset , \\{1,2,3\\}   \\big\\}\n\\end{align*}\n\n- If $\\Omega$ is **uncountable** then the power set of $\\Omega$ is not easy to describe\n\n\n\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Question\n\n$\\mathbb{R}$ is uncountable. Which $\\sigma$-algebra do we consider?\n\n:::\n\n\n::: Definition \n### Lebesgue sigma-algebra\n\n\nThe Lebesgue $\\sigma$-algebra on $\\mathbb{R}$ is the smallest $\\sigma$-algebra $\\mathcal{L}$ containing all sets of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n:::\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Important\n\nTherefore the events of $\\R$ are \n\n- Intervals\n- Unions and intersection of intervals\n- Countable Unions and intersection of intervals\n\n:::\n\n\n::: Warning \n\n- I only told you that the Lebsesgue $\\sigma$-algebra $\\mathcal{L}$ **exists**\n- Explicitly showing that $\\mathcal{L}$ exists is not easy, see [@rosenthal]\n\n:::\n\n\n\n\n## Probability measure {.smaller}\n\nSuppose given:\n\n- $\\Omega$ sample space \n- $\\mathcal{B}$ a $\\sigma$-algebra on $\\Omega$\n\n::: Definition\n### Probability measure\n\nA **probability measure** on $\\Omega$ is a map\n$$\nP \\colon \\mathcal{B} \\to [0,1]\n$$\nsuch that the **Axioms of Probability** hold\n\n1. $P(\\Omega) = 1$\n2. If $A_1, A_2,\\ldots$ are pairwise disjoint then\n$P\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  = \\sum_{i=1}^\\infty P(A_i)$\n\n:::\n\n\n\n\n## Properties of Probability {.smaller}\n\nLet $A, B \\in \\mathcal{B}$. As a consequence of the Axioms of Probability:\n\n\n1. $P(\\emptyset) = 0$\n2. If $A$ and $B$ are disjoint then\n$$\nP(A \\cup B) = P(A) + P(B)\n$$\n3. $P(A^c) = 1 - P(A)$\n4. $P(A) = P(A \\cap B) + P(A \\cap B^c)$\n5. $P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$\n6. If $A \\subset B$ then\n$$\nP(A) \\leq P(B)\n$$\n\n\n\n\n\n## Properties of Probability {.smaller} \n\n7. Suppose $A$ is an event and $B_1,B_2, \\ldots$ a partition of $\\Omega$. Then\n$$\nP(A) = \\sum_{i=1}^\\infty  P(A \\cap B_i)\n$$\n8. Suppose $A_1,A_2, \\ldots$ are events. Then\n$$\nP\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  \\leq  \\sum_{i=1}^\\infty P(A_i)\n$$\n\n\n\n\n## Example: Fair Coin Toss {.smaller}\n\n- The sample space for **coin toss** is $\\Omega = \\{ H, T \\}$\n\n- We take as $\\sigma$-algebra the power set of $\\Omega$\n$$\n\\mathcal{B} = \\{  \\emptyset , \\, \\{H\\}  , \\, \\{T\\} , \\, \\{H,T\\}   \\}\n$$\n\n- We suppose that the coin is fair\n\n    * This means $P \\colon \\mathcal{B} \\to [0,1]$ satisfies\n    $$\n    P(\\{H\\}) = P(\\{T\\})\n    $$\n    * Assuming the above we get\n    $$\n    1 = P(\\Omega) = P(\\{H\\} \\cup \\{T\\}) = \n    P(\\{H\\}) + P(\\{T\\}) = 2 P(\\{H\\})\n    $$\n    * Therefore \n    $P(\\{H\\}) = P(\\{T\\}) = \\frac12$\n\n\n\n\n\n## Conditional Probability {.smaller}\n\n::: Definition\n### Conditional Probability\n\nLet $A,B$ be events in $\\Omega$ with \n$$\nP(B)>0\n$$\nThe **conditional probability** of $A$ given $B$ is\n$$\nP(A|B) := \\frac{P(A \\cap B)}{P(B)} \n$$\n\n:::\n\n\n- $P(A|B)$ represents the probability of $A$, knowing that $B$ happened\n- The function $A \\mapsto P(A|B)$ is a probability measure on $\\Omega$\n\n\n\n\n## Bayes' Rule {.smaller}\n\n\n- For two events $A$ and $B$ is holds\n\n$$\nP(A | B ) = P(B|A) \\frac{P(A)}{P(B)}\n$$\n\n\n- Given a partition $A_1, A_2, \\ldots$ of the sample space we have\n\n$$\nP(A_i | B ) = \\frac{ P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B | A_j) P(A_j)}\n$$\n\n\n\n\n\n## Independence {.smaller}\n\n\n::: Definition\n\nTwo events $A$ and $B$ are **independent** if \n$$\nP(A \\cap B) = P(A)P(B)\n$$\nA collection of events $A_1 , \\ldots ,A_n$ are **mutually independent** if\nfor any subcollection $A_{i_1}, \\ldots, A_{i_k}$ it holds\n$$\nP \\left(  \\bigcap_{j=1}^k A_j  \\right) = \\prod_{j=1}^k  P(A_{i_j})\n$$\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Motivation\n\n- Consider the experiment of flipping a coin $50$ times\n- The sample space consists of $2^{50}$ elements\n- Elements are vectors of $50$ entries recording the outcome $H$ or $T$ of each flip\n- This is a very large sample space!\n\n\nSuppose we are only interested in\n$$\nX = \\text{ number of } \\, H \\, \\text{ in } \\, 50 \\, \\text{flips}\n$$\n\n- Then the new sample space is the set of integers\n$\\{ 0,1,2,\\ldots,50\\}$\n- This is much smaller!\n- $X$ is called a Random Variable\n\n\n\n\n## Random Variables {.smaller}\n\n\nAssume given\n\n- $\\Omega$ sample space\n- $\\mathcal{B}$ a $\\sigma$-algebra of events on $\\Omega$\n- $P \\colon \\mathcal{B} \\to [0,1]$ a probability measure\n\n\n::: Definition\n### Random variable\n\nA function $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\nWe will abbreviate Random Variable with rv\n\n\n\n\n\n## Random Variables {.smaller}\n### Technical remark \n\n\n::: Definition\n### Random variable\n\nA [**measurable**]{style=\"color:#cc0164;\"}\nfunction $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\n**Technicality**: \n$X$ is a [**measurable**]{style=\"color:#cc0164;\"}\nfunction if\n$$\n\\{ X \\in I \\} := \\{ \\omega \\in \\Omega \\colon X(\\omega) \\in I \\} \\in \\mathcal{B} \\,, \\quad \\forall \\, I \\in \\mathcal{L}\n$$\nwhere \n\n- $\\mathcal{L}$ is the Lebsgue $\\sigma$-algebra on $\\mathbb{R}$\n- $\\mathcal{B}$ is the given $\\sigma$-algebra on $\\Omega$\n\n\n\n\n## Random Variables {.smaller}\n### Notation\n\n- In particular $I \\in \\mathcal{L}$ can be of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \\,, \\quad \n\\forall \\, a, b \\in \\mathbb{R}\n$$\n\n- In this case the set \n$$\n\\{X \\in I\\} \\in \\mathcal{B}\n$$ \nis denoted by, respectively:\n$$\n\\{ a < X < b \\} \\,, \\quad \\{ a < X \\leq b \\} \\,, \\quad \\{ a \\leq X < b \\} \\,, \\quad \\{ a \\leq X \\leq b \\}\n$$\n\n- If $a=b=x$ then $I=[x,x]=\\{x\\}$. Then we denote\n$$\n\\{X \\in I\\} = \\{X = x\\}\n$$\n\n\n\n## Distribution {.smaller}\n### Why do we require measurability?\n\n**Answer**: Because it allows to define a new probability measure on $\\mathbb{R}$\n\n\n::: Definition\n### Distribution\n\nThe **distribution** of a random variable $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( \\{X \\in I\\} \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n:::\n\n\n**Note**: \n\n- One can show that $P_X$ satisfies the *Probability Axioms*\n- Thus $P_X$ is a probability measure on $\\mathbb{R}$\n- In the future we will denote \n$P \\left( X \\in I \\right) := P \\left( \\{X \\in I\\} \\right)$\n\n\n\n\n\n\n\n## Distribution {.smaller}\n### Why is the distribution useful?\n\n**Answer**: Because it allows to define a random variable $X$\n\n- by specifying the distribution values\n$$\nP \\left( X \\in I \\right) \n$$\n- rather than defining an explicit function $X \\colon \\Omega \\to \\mathbb{R}$\n\n\n**Important**: More often than not\n\n- We care about the distribution of $X$\n- We do not care about how $X$ is defined\n\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Sample space $\\Omega$ given by the below values of $\\omega$\n\n+:---------:+:----:+:---:+:---:+:---:+:---:+:---:+:---:+:---:+\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n+-----------+------+-----+-----+-----+-----+-----+-----+-----+\n\n\n- The probability of each outcome is the same\n$$\nP(\\omega) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8} \\,, \\quad \\forall \\, \\omega \\in \\Omega\n$$\n\n\n- Define the random variable $X \\colon \\Omega \\to \\R$ by\n$$\nX(\\omega) :=  \\text{ Number of H in }  \\omega\n$$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The range of $X$ is $\\{0,1,2,3\\}$\n\n- Hence the only interesting values of $P_X$ are\n$$\nP(X=0) \\,, \\quad \nP(X=1) \\,, \\quad \nP(X=2) \\,, \\quad \nP(X=3)\n$$\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- We compute\n\\begin{align*}\nP(X=0) & = P(TTT) = \\frac{1}{8} \\\\\nP(X=1) & = P(TTH) + P(THT) + P(HTT) = \\frac{3}{8} \\\\\nP(X=2) & = P(HHT) + P(HTH) + P(THH) = \\frac{3}{8} \\\\\nP(X=3) & = P(HHH) = \\frac{1}{8}\n\\end{align*}\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The distribution of $X$ is summarized in the table below\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n\n\n\n## Cumulative Distribution Function {.smaller}\n\n\n**Recall**: The **distribution** of a rv $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( X \\in I \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n::: Definition\n### cdf\n\nThe **cumulative distribution function** or **cdf** of a rv $X \\colon \\Omega \\to \\mathbb{R}$ is\n$$\nF_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nF_X(x) := P_X (X \\leq x) \n$$\n\n:::\n\n\n**Intuition**: \n\n- $F_X$ is the *primitive* of $P_X$\n- $P_X$ will be the *derivative* of $F_X$^[In a suitable generalized sense]\n\n\n\n\n## Distribution Function {.smaller}\n### Example\n\n\n- Consider again 3 coin tosses and the rv\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- We computed that the distribution $P_X$ of $X$ is\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n::: {.column width=\"38%\"}\n\n- One can compute\n$$\nF_X(x) = \n\\begin{cases}\n0  & \\text{if } x < 0 \\\\\n\\frac{1}{8}  & \\text{if } 0 \\leq x < 1 \\\\\n\\frac{1}{2}  & \\text{if } 1 \\leq x < 2 \\\\\n\\frac{7}{8}  & \\text{if } 2 \\leq x < 3 \\\\\n1            & \\text{if } 3 \\leq x\n\\end{cases}\n$$\n\n:::\n\n\n::: {.column width=\"56%\"}\n\n- For example\n\\begin{align*}\nF_X(2.1) & = P(X \\leq 2.1) \\\\\n         & = P(X=0,1 \\text{ or } 2) \\\\\n         & = P(X=0) + P(X=1) + P(X=2) \\\\\n         & = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}\n\\end{align*}\n\n:::\n\n\n\n\n\n## Cumulative Distribution Function {.smaller}\n### Example\n\n\n```{r}\n# Define the step function\nstep_function <- function(x) {\n  if (x < 0) {\n    return(0)\n  } else if (x < 1) {\n    return(1/8)\n  } else if (x < 2) {\n    return(1/2)\n  } else if (x < 3) {\n    return(7/8)\n  } else {\n    return(1)\n  }\n}\n\n# Generate x values\nx_values <- seq(-1, 4, length.out = 1000)\n\n# Calculate corresponding y values using the step function\ny_values <- sapply(x_values, step_function)\n\n# Create a data frame for ggplot\nplot(x_values, \n    y_values, \n    type = 's',\n    xlab = \"\",\n    ylab = \"\")\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"F_X(x)\", side=2, line=2.5, cex=2)\n\n```\n\n\n\n::: {.column width=\"43%\"}\n\n- Plot of $F_X$: it is a **step function**\n- $F_X'=0$ except at $x=0,1,2,3$\n- $F_X$ jumps at $x=0,1,2,3$ \n\n:::\n\n::: {.column width=\"56%\"}\n\n- Size of jump at $x$ is $P(X=x)$ \n- $F_X'=P_X$ in the sense of **distributions**\n<br> (Advanced analysis concept - not covered)\n\n:::\n\n\n\n\n## Discrete Random Variables {.smaller}\n\nIn the previous example:\n\n- The cdf $F_X$ had **jumps**\n- Hence $F_X$ was **discountinuous**\n- We take this as definition of **discrete** rv\n\n\n\n::: Definition\n\n$X \\colon \\Omega \\to \\mathbb{R}$ is **discrete** if $F_X$ has jumps\n\n:::\n\n\n\n\n\n## Probability Mass Function {.smaller}\n\n- In this slide $X$ is a **discrete** rv\n- Therefore $F_X$ has **jumps**\n\n::: Definition\n\nThe **Probability Mass Function** or **pmf** of a discrete rv $X$ is\n$$\nf_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nf_X(x) := P(X = x) \n$$\n\n:::\n\n\n\n## Probability Mass Function {.smaller}\n### Properties\n\n::: Proposition\n\nThe **pmf** $f_X(x) = P(X=x)$ can be used to\n\n- compute probabilities\n    $$\n    P(a \\leq X \\leq b) = \\sum_{k = a}^b f_X (k) \\,, \\quad \\forall \\, a,b \\in \n    \\mathbb{Z} \\,, \\,\\, a \\leq b \n    $$\n- compute the **cdf**\n    $$\n    F_X(x) = P(X \\leq x) = \\sum_{k=-\\infty}^x f_X(k)\n    $$\n\n:::\n\n\n\n\n\n## Example 1 - Discrete RV {.smaller}\n\n- Consider again 3 coin tosses and the RV\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- The pmf of $X$ is $f_X(x):=P(X=x)$, which we have already computed\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$f_X(x)= P(X=x)$ | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n```{r}\n# Example data\nvalues <- c(0, 1, 2, 3)\nprobabilities <- c(1/8, 3/8, 3/8, 1/8)\n\n# Plot PMF\nplot(values, \n    probabilities, \n    xlab = \"\",\n    ylab = \"\",\n    xaxt='n',\n    type=\"h\", \n    lwd=4, \n    col=\"blue\")\n\naxis(1, at = values,labels = T)\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"f_X(x)\", side=2, line=2.5, cex=2)\n\npoints(values, probabilities, col = \"red\", pch = 16)\n\ngrid()\n```\n\n\n\n\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n- Suppose $p \\in (0,1)$ is a given probability of **success**\n- Hence $1-p$ is probability of **failure**\n- Consider the random variable \n$$\nX = \\text{ Number of attempts to obtain first success}\n$$\n- Since each trial is independent, the pmf of $X$ is\n$$\nf_X (x) = P(X=x) = (1-p)^{x-1} p \\,, \\quad \\forall \\, x \\in \\mathbb{N}\n$$\n- This is called **geometric distribution**\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n- We want to compute the cdf of $X$: For $x \\in \\mathbb{N}$ with $x > 0$\n\\begin{align*}\nF_X(x) & = P(X \\leq x) = \\sum_{k=1}^x P(X=k) = \\sum_{k=1}^x f_X(k) \\\\\n       & = \\sum_{k=1}^x (1-p)^{k-1} p \n         = \\frac{1-(1-p)^x}{1-(1-p)} p = 1 - (1-p)^x \n\\end{align*}\nwhere we used the formula for the sum of geometric series:\n$$\n\\sum_{k=1}^x t^{k-1} = \\frac{1-t^x}{1-t} \\,, \\quad t \\neq 1 \n$$\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n\n- $F_X$ is flat between two consecutive natural numbers:\n\\begin{align*}\nF_X(x+k) & = P(X \\leq x+k) \\\\\n         & = P(X \\leq x) \\\\\n         & = F_X(x) \n\\end{align*} \nfor all $x \\in \\mathbb{N}, k \\in [0,1)$\n\n- Therefore $F_X$ has **jumps** and $X$ is **discrete**\n\n\n\n\n\n## Continuous Random Variables {.smaller} \n\n**Recall**: $X$ is **discrete** if $F_X$ has **jumps**\n\n\n::: Definition\n### Continuous Random Variable\n\n$X \\colon \\Omega \\to \\mathbb{R}$ is **continuous** if \n$F_X$ is **continuous**\n\n:::\n\n\n\n\n\n\n\n## Probability Mass Function? {.smaller}\n\n- Suppose $X$ is a **continuous** rv\n- Therefore $F_X$ is **continuous**\n\n::: Question \n\nCan we define the Probability Mass Function for $X$?\n\n:::\n\n\n**Answer**: \n\n- Yes we can, but it would be useless - pmf carries no information\n- This is because \n$$\nf_X(x) = P(X=x) = 0 \\,, \\quad \\forall \\, x \\in \\mathbb{R}\n$$\n\n\n\n\n## Probability Mass Function? {.smaller}\n\n\n- Indeed, for all $\\varepsilon>0$ we have $\\{ X = x \\} \\subset \\{  x - \\varepsilon < X \\leq x  \\}$\n- Therefore by the properties of probabilities we have\n\\begin{align*}\nP (X = x ) & \\leq P(  x - \\varepsilon < X \\leq x ) \\\\\n           & = P(X \\leq x) - P(X \\leq x - \\varepsilon) \\\\\n           & = F_X(x) - F_X(x-\\varepsilon)\n\\end{align*}\nwhere we also used the definition of $F_X$\n- Since $F_X$ is continuous we get\n$$\n0 \\leq P(X = x) \\leq \\lim_{\\varepsilon \\to 0} F_X(x) - F_X(x-\\varepsilon) = 0\n$$\n- Then $f_X(x) = P(X=x) = 0$ for all $x \\in \\mathbb{R}$\n\n\n\n\n\n\n## Probability Density Function {.smaller}\n\n\n- **pmf** carries no information for continuous RV\n- We instead define the **pdf**\n\n\n::: Definition\n\nThe **Probability Density Function** or **pdf** of a continuous rv $X$ is\na function $f_X \\colon \\mathbb{R} \\to \\mathbb{R}$ s.t.\n$$\nF_X(x) =  \\int_{-\\infty}^x f_X(t) \\, dt  \\,, \\quad \\forall \\, x \\in\n\\mathbb{R}\n$$\n\n:::\n\n\n**Technical issue**: \n\n- If $X$ is continuous then pdf **does not exist** in general\n- Counterexamples are rare, therefore we will **assume** existence of pdf\n\n\n\n\n## Probability Density Function {.smaller}\n### Properties\n\n::: Proposition\n\nSuppose $X$ is continuous rv. They hold\n\n- The **cdf** $F_X$ is continuous and differentiable (a.e.) with\n$$\nF_X' = f_X\n$$\n\n- Probability can be computed via\n    $$\n    P(a \\leq X \\leq b) = \\int_{a}^b f_X (t) \\, dt \\,, \\quad \\forall \\, a,b \\in \n    \\mathbb{R} \\,, \\,\\, a \\leq b \n    $$\n\n:::\n\n\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n- The random variable $X$ has **logistic distribution** if its **pdf** is\n$$\nf_X(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n$$\n\n\n\n```{r}\n\n# Define the probability density function (PDF)\nf_X <- function(x) {\n  exp(-x) / (1 + exp(-x))^2\n}\n\n# Generate x values for plotting\nx_values <- seq(-10, 10, length.out = 1000)\n\n# Calculate corresponding y values\ny_values <- f_X(x_values)\n\n# Plot the distribution using base R plot function\nplot(x_values, \n    y_values, \n    type = \"l\", \n    col = \"blue\", \n    lwd = 4,\n    xlab = \"\", \n    ylab = \"\")\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"f_X\", side=2, line=2.5, cex=2)\n\n\n# Add a grid for better visualization\ngrid()\n\n```\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n- The random variable $X$ has **logistic distribution** if its **pdf** is\n$$\nf_X(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n$$\n\n\n\n- The **cdf** can be computed to be\n$$\nF_X(x) = \\int_{-\\infty}^\\infty f_X(t) \\, dt = \\frac{1}{1+e^{-x}}\n$$\n\n\n- The RHS is known as **logistic function**\n\n\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n**Application**: Logistic function models expected score in chess (see [Wikipedia](https://en.wikipedia.org/wiki/Elo_rating_system))\n\n\n\n- $R_A$ is ELO rating of player $A$, $R_B$ is ELO rating of player $B$\n- $E_A$ is expected score of player $A$: \n$$\nE_A := P(A \\text{ wins}) + \\frac12 P(A \\text{ draws})\n$$\n- $E_A$ modelled by logistic function\n$$\nE_A := \\frac{1}{1+ 10^{(R_B-R_A)/400} }\n$$\n- Example: Beginner is rated $1000$, International Master is rated $2400$\n$$\nR_{\\rm Begin} = 1000, \\quad  R_{\\rm IM}=2400 , \\quad E_{\\rm Begin} = \\frac{1}{1 + 10^{1400/400}} = 0.00031612779\n$$\n\n\n\n\n## Characterization of pmf and pdf {.smaller}\n\n\n::: Theorem\n\nLet $f \\colon \\mathbb{R} \\to \\mathbb{R}$. Then $f$ is pmf or pdf of a RV $X$ iff\n\n1. $f(x) \\geq 0$ for all $x \\in \\mathbb{R}$\n2. $\\sum_{x=-\\infty}^\\infty f(x) = 1 \\,\\,\\,$ (pmf) $\\quad$ or $\\quad$ $\\int_{-\\infty}^\\infty f(x) \\, dx = 1\\,\\,\\,$ (pdf)\n\n:::\n\nIn the above setting: \n\n- The RV $X$ has **distribution**\n$$\nP(X = x) = f(x)  \\,\\,\\, \\text{ (pmf) } \\quad  \\text{ or }  \\quad\nP(a \\leq X \\leq b) = \\int_a^b f(t) \\, dt   \\,\\,\\, \\text{ (pdf)}\n$$\n\n- The symbol $X \\sim f$ denotes that $X$ has distribution $f$\n\n\n\n\n\n## Summary - Random Variables {.smaller}\n\n- Suppose $X \\colon \\Omega \\to \\mathbb{R}$ is RV\n\n- **Cumulative Density Function (cdf)**:\n$F_X(x) := P(X \\leq x)$\n\n\n| Discrete RV                       |        Continuous RV         |\n|--------------                     |----------------              |\n| $F_X$ has **jumps**               |  $F_X$ is **continuous**     |\n|**Probability Mass Function (pmf)**|**Probability Density Function (pdf)**|\n| $f_X(x) := P(X=x)$                | $f_X(x) := F_X'(x)$          |  \n| $f_X \\geq 0$                      |  $f_X \\geq 0$                |\n|$\\sum_{x=-\\infty}^\\infty f_X(x) = 1$| $\\int_{-\\infty}^\\infty f_X(x) \\, dx = 1$ |\n| $F_X (x) = \\sum_{k=-\\infty}^x f_X(k)$| $F_X (x) = \\int_{-\\infty}^x f_X(t) \\, dt$\n| $P(a \\leq X \\leq b) = \\sum_{k = a}^{b} f_X(k)$ | $P(a \\leq X \\leq b) = \\int_a^b f_X(t) \\, dt$ |\n\n\n\n\n\n\n\n# Part 4: <br>Moment generating functions {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\n- $X \\colon \\Omega \\to \\mathbb{R}$ random variable and $g \\colon \\mathbb{R} \\to \\mathbb{R}$ function\n- Then $Y:=g(X) \\colon \\Omega \\to \\mathbb{R}$ is random variable\n- For $A \\subset \\mathbb{R}$ we define the pre-image \n$$\ng^{-1}(A) := \\{  x \\in \\mathbb{R} \\colon g(x) \\in A \\}\n$$\n- For $A=\\{y\\}$ single element set we denote\n$$\ng^{-1}(\\{y\\}) = g^{-1}(y) =  \\{  x \\in \\mathbb{R} \\colon g(x) = y \\}\n$$\n- The distribution of $Y$ is\n$$\nP(Y \\in A) = P(g(X) \\in A ) = P(X \\in g^{-1}(A))\n$$\n\n\n\n## Functions of Random Variables {.smaller}\n\n**Question**: What is the relationship between $f_X$ and $f_Y$?\n\n- **$X$ discrete**: Then $Y$ is discrete and\n$$\nf_Y (y) = P(Y = y) = \\sum_{x \\in g^{-1}(y)} P(X=x) = \\sum_{x \\in g^{-1}(y)} f_X(x) \n$$\n\n- **$X$ and $Y$ continuous**: Then\n\\begin{align*}\nF_Y(y) & = P(Y \\leq y) = P(g(X) \\leq y) \\\\\n       & = P(\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}   ) = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt\n\\end{align*}\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\n**Issue**: The below set may be tricky to compute\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}\n$$\n\nHowever it can be easily computed if $g$ is **strictly monotone**:\n\n- **g strictly increasing**: Meaning that \n$$\nx_1 < x_2 \\quad \\implies \\quad g(x_1) < g(x_2)\n$$\n\n- **g strictly decreasing**: Meaning that \n$$\nx_1 < x_2 \\quad \\implies \\quad g(x_1) > g(x_2)\n$$\n\n- In both cases $g$ is **invertible**\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\nLet $g$ be **strictly increasing**:\n\n- Then\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\} = \\{ x \\in \\mathbb{R} \\colon x \\leq g^{-1}(y) \\}\n$$\n\n- Therefore\n\\begin{align*}\nF_Y(y) & = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt \n         = \\int_{\\{ x \\in \\mathbb{R} \\colon x \\leq g^{-1}(y) \\}} f_X(t) \\, dt  \\\\\n       & = \\int_{-\\infty}^{g^{-1}(y)} f_X(t) \\, dt \n         = F_X(g^{-1}(y)) \n\\end{align*}\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\nLet $g$ be **strictly decreasing**:\n\n- Then\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\} = \\{ x \\in \\mathbb{R} \\colon x \\geq g^{-1}(y) \\}\n$$\n\n- Therefore\n\\begin{align*}\nF_Y(y) & = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt \n         = \\int_{\\{ x \\in \\mathbb{R} \\colon x \\geq g^{-1}(y) \\}} f_X(t) \\, dt  \\\\\n       & = \\int_{g^{-1}(y)}^{\\infty} f_X(t) \\, dt \n         = 1 - \\int_{-\\infty}^{g^{-1}(y)}f_X(t) \\, dt  \\\\\n       & = 1 - F_X(g^{-1}(y)) \n\\end{align*}\n\n\n\n\n\n## Summary - Functions of Random Variables {.smaller}\n\n\n- **$X$ discrete**: Then $Y$ is discrete and\n$$\nf_Y (y) = \\sum_{x \\in g^{-1}(y)} f_X(x) \n$$\n\n- **$X$ and $Y$ continuous**: Then\n$$\nF_Y(y) = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt\n$$\n\n- **$X$ and $Y$ continuous** and\n    * **$g$ strictly increasing**: $F_Y(y) = F_X(g^{-1}(y))$\n    * **$g$ strictly decreasing**: $F_Y(y) = 1 - F_X(g^{-1}(y))$\n\n\n\n\n\n## Expected Value {.smaller}\n\nExpected value is the **average** value of a random variable\n\n\n::: Definition\n\n$X$ rv and $g \\colon \\mathbb{R} \\to \\mathbb{R}$ function. The **expected value** or **mean**\nof $g(X)$ is $\\Expect [g(X)]$\n\n- If $X$ discrete\n$$\n\\Expect [g(X)]:= \\sum_{x \\in \\mathbb{R}} g(x) f_X(x) = \\sum_{x \\in \\mathbb{R}} g(x) P(X = x)\n$$\n\n- If $X$ continuous\n$$\n\\Expect [g(X)]:= \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx\n$$\n\n:::\n\n\n\n\n\n## Expected Value  {.smaller}\n### Properties\n\n\nIn particular we have^[These follow by taking $g(x)=x$ in previous definitions, so that $g(X) = X$]\n\n\n- If $X$ discrete\n$$\n\\Expect [X] = \\sum_{x \\in \\mathbb{R}} x f_X(x) = \\sum_{x \\in \\mathbb{R}} x P(X = x)\n$$\n\n- If $X$ continuous\n$$\n\\Expect [X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx\n$$\n\n\n\n\n\n\n\n\n## Expected Value  {.smaller}\n### Properties\n\n::: Theorem\n\n$X$ rv and $a,b \\in \\mathbb{R}$. The expected value is **linear**\n\\begin{equation} \\tag{1}\n\\Expect [aX + b] = a\\Expect [X] + b\n\\end{equation}\nIn particular the expected value of a constant is the constant itself:\n\\begin{equation} \\tag{2}\n\\Expect [b] = b\n\\end{equation}\n:::\n\n\n\n\n\n## Expected Value  {.smaller}\n### Proof of Theorem\n\n\n- Note that (2) follows from (1) by setting $a=0$\n\n- We now show (1). Suppose $X$ is continuous and set $g(x):=ax+b$. Then \n\\begin{align*}\n\\Expect [aX + b] & = \\Expect [g(X)] = \\int_{\\R} g(x) f_X(x) \\, dx \\\\\n& = \\int_{\\R} (ax + b) f_X(x) \\, dx  \\\\\n& = a\\int_{\\R} x f_X(x) \\, dx  + b\\int_{\\R} f_X(x) \\, dx  \\\\\n& = a \\Expect[X] + b\n\\end{align*}\n\n- If $X$ is discrete just replace series with integrals in the above argument\n\n\n\n\n\n\n\n## Expected Value  {.smaller}\n### Further Properties\n\nBelow are further properties of $\\Expect$, which we do not prove\n\n::: Theorem\n\nSuppose $X$ and $Y$ are rv. The expected value is:\n\n- **Monotone**: $X \\leq Y  \\quad \\implies \\quad \\Expect [X] \\leq  \\Expect [Y]$\n\n- **Non-degenerate**: $\\Expect[|X|] = 0 \\quad \\implies \\quad X = 0$\n\n- $X=Y \\quad \\implies \\quad \\Expect[X]=\\Expect[Y]$ \n\n:::\n\n\n\n\n\n\n## Variance {.smaller}\n\nVariance measures how much a rv $X$ deviates from $\\Expect[X]$\n\n::: Definition\n### Variance\n\nThe **variance** of a random variable $X$ is\n$$\n\\Var[X]:= \\Expect[(X - \\Expect[X])^2]\n$$\n\n:::\n\n\n**Note**:\n\n- $\\Var [X] = 0 \\quad \\implies \\quad (X - \\Expect[X])^2 = 0 \\quad  \\implies \\quad X = \\Expect [X]$\n- If $\\Var[X]$ is small then $X$ is close to $\\Expect[X]$\n- If $\\Var[X]$ is large then $X$ is very variable\n\n\n\n\n## Variance {.smaller}\n### Equivalent formula\n\n::: Proposition\n$$\n\\Var[X] =  \\Expect[X^2] - \\Expect[X]^2\n$$\n:::\n\n**Proof:**\n\\begin{align*}\n\\Var[X] & = \\Expect[(X - \\Expect[X])^2] \\\\\n        & = \\Expect[X^2 - 2 X \\Expect[X] +  \\Expect[X]^2] \\\\  \n        & = \\Expect[X^2] - \\Expect[2 X \\Expect[X]]  + \\Expect[ \\Expect[X]^2] \\\\\n        & = \\Expect[X^2] - 2 \\Expect[X]^2 + \\Expect[X]^2 \\\\\n        & = \\Expect[X^2] - \\Expect[X]^2\n\\end{align*}\n\n\n\n\n\n## Variance {.smaller}\n### Variance is quadratic\n\n::: Proposition\n\n$X$ rv and $a,b \\in \\R$. Then\n$$\n\\Var[a X + b] = a^2 \\Var[X] \n$$\n\n:::\n\n\n**Proof:** Using linearity of $\\Expect$ and the fact that $\\Expect[c]=c$ for constants:\n\\begin{align*}\n\\Var[a X + b] & = \\Expect[ (aX + b)^2 ] - \\Expect[ aX + b ]^2 \\\\\n              & = \\Expect[ a^2X^2 + b^2 + 2abX ] - ( a\\Expect[X]  + b)^2 \\\\\n              & = a^2 \\Expect[ X^2 ] + b^2 + 2ab \\Expect[X] - \n                  a^2 \\Expect[X]^2   - b^2 - 2ab \\Expect[X] \\\\\n              & = a^2 ( \\Expect[ X^2 ] - \\Expect[ X ]^2 ) = a^2 \\Var[X]\n\\end{align*}\n\n\n\n\n\n## Variance {.smaller}\n### How to compute the Variance\n\nWe have\n$$\n\\Var[X] =  \\Expect[X^2] - \\Expect[X]^2\n$$\n\n- $X$ discrete:\n$$\nE[X] = \\sum_{x \\in \\mathbb{R}} x f_X(x) \\,, \\qquad \nE[X^2] = \\sum_{x \\in \\mathbb{R}} x^2 f_X(x)\n$$\n\n\n- $X$ continuous:\n$$\nE[X] = \\int_{-\\infty}^\\infty x f_X(x) \\, dx \\,, \\qquad \nE[X^2] = \\int_{-\\infty}^\\infty x^2 f_X(x) \\, dx\n$$\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n\nThe **Gamma distribution** with parameters $\\alpha,\\beta>0$ is\n$$\nf(x) := \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\,, \\quad x > 0\n$$\nwhere $\\Gamma$ is the **Gamma function**\n$$\n\\Gamma(a) :=\\int_0^{\\infty} x^{a-1} e^{-x} \\, dx\n$$\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n**Properties of $\\Gamma$**:\n\n- The Gamma function coincides with the factorial on natural numbers\n$$\n\\Gamma(n)=(n-1)! \\,, \\quad \\forall \\, n \\in \\N\n$$\n\n- More in general\n$$\n\\Gamma(a)=(a-1)\\Gamma(a-1) \\,, \\quad \\forall \\, a > 0\n$$\n\n\n-  Definition of $\\Gamma$ implies normalization of the Gamma distribution:\n$$\n\\int_0^{\\infty} f(x) \\,dx = \n\\int_0^{\\infty} \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  \\, dx\n=  1 \n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n$X$ has Gamma distribution with parameters $\\alpha,\\beta$ if \n\n- the pdf of $X$ is\n$$\nf_X(x) = \n\\begin{cases} \n\\dfrac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  & \\text{ if } x > 0 \\\\\n0                                                                 & \\text{ if } x \\leq 0\n\\end{cases}\n$$\n\n- In this case we write $X \\sim \\Gamma(\\alpha,\\beta)$\n\n- $\\alpha$ is **shape** parameter\n\n- $\\beta$ is **rate** parameter\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Plot\n\nPlotting $\\Gamma(\\alpha,\\beta)$ for parameters $(2,1)$ and $(3,2)$\n\n```{r}\n# Set the shape and rate parameters for the first gamma distribution\nalpha1 <- 2\nbeta1 <- 1\n\n# Set the shape and rate parameters for the second gamma distribution\nalpha2 <- 3\nbeta2 <- 2\n\n# Generate values for the x-axis\nx_values <- seq(0, 15, by = 0.1)\n\n# Calculate the probability density function (PDF) values for each x for the first distribution\npdf_values1 <- dgamma(x_values, shape = alpha1, rate = beta1)\n\n# Calculate the probability density function (PDF) values for each x for the second distribution\npdf_values2 <- dgamma(x_values, shape = alpha2, rate = beta2)\n\n# Plot the gamma PDFs\nplot(x_values, \n    pdf_values1, \n    type = \"l\", \n    col = \"blue\", lwd = 2,\n    xlab = \"\", \n    ylab = \"\",\n    ylim = c(0, max(pdf_values1, pdf_values2) + 0.1))\n\nlines(x_values, \n      pdf_values2, \n      col = \"red\", \n      lwd = 2)\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"pdf\", side=2, line=2.5, cex=2)\n\n# Add a legend\nlegend(\"topright\", legend = c(paste(\"Gamma(\", alpha1, \",\", beta1, \")\", sep = \"\"),\n                              paste(\"Gamma(\", alpha2, \",\", beta2, \")\", sep = \"\")),\n       col = c(\"blue\", \"red\"), lty = 1, lwd = 2, cex = 1.5)\n\n```  \n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nLet $X \\sim \\Gamma(\\alpha,\\beta)$. We have:\n\\begin{align*}\n\\Expect [X] & = \\int_{-\\infty}^\\infty x f_X(x) \\, dx  \\\\\n            & = \\int_0^\\infty  x \\, \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, dx \\\\\n            & = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nRecall previous calculation:\n$$\n\\Expect [X] = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx\n$$\nChange variable $y=\\beta x$ and recall definition of $\\Gamma$:\n\\begin{align*}\n \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx & = \n  \\int_0^\\infty  \\frac{1}{\\beta^{\\alpha}} (\\beta x)^{\\alpha} e^{-\\beta{x}} \\frac{1}{\\beta} \\, \\beta \\, dx \\\\\n  & = \\frac{1}{\\beta^{\\alpha+1}} \\int_0^\\infty  y^{\\alpha} e^{-y} \\, dy \\\\\n  & = \\frac{1}{\\beta^{\\alpha+1}} \\Gamma(\\alpha+1)\n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nTherefore\n\\begin{align*}\n\\Expect [X] & = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx \\\\\n            & =  \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) } \\, \\frac{1}{\\beta^{\\alpha+1}} \\Gamma(\\alpha+1) \\\\\n            & = \\frac{\\Gamma(\\alpha+1)}{\\beta \\Gamma(\\alpha)}\n\\end{align*}\n\nRecalling that $\\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha)$:\n$$\n\\Expect [X] = \\frac{\\Gamma(\\alpha+1)}{\\beta \\Gamma(\\alpha)} = \\frac{\\alpha}{\\beta}\n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nWe want to compute\n$$\n\\Var[X] = \\Expect[X^2] - \\Expect[X]^2\n$$\n\n- We already have $\\Expect[X]$\n- Need to compute $\\Expect[X^2]$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nProceeding similarly we have:\n\n\\begin{align*}\n\\Expect[X^2] & = \\int_{-\\infty}^{\\infty} x^2 f_X(x) \\, dx \\\\\n             & = \\int_{0}^{\\infty}  x^2 \\, \\frac{ x^{\\alpha-1} \\beta^{\\alpha} e^{- \\beta x} }{ \\Gamma(\\alpha) } \\, dx \\\\\n             & =  \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty}  x^{\\alpha+1} e^{- \\beta x} \\, dx \n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nRecall previous calculation:\n$$\n\\Expect [X^2] =  \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty}  x^{\\alpha+1} e^{- \\beta x} \\, dx \n$$\nChange variable $y=\\beta x$ and recall definition of $\\Gamma$:\n\\begin{align*}\n \\int_0^\\infty  x^{\\alpha+1} e^{-\\beta{x}}  \\, dx & = \n  \\int_0^\\infty  \\frac{1}{\\beta^{\\alpha+1}} (\\beta x)^{\\alpha + 1} e^{-\\beta{x}} \\frac{1}{\\beta} \\, \\beta \\, dx \\\\\n  & = \\frac{1}{\\beta^{\\alpha+2}} \\int_0^\\infty  y^{\\alpha + 1 } e^{-y} \\, dy \\\\\n  & = \\frac{1}{\\beta^{\\alpha+2}} \\Gamma(\\alpha+2)\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nTherefore\n$$\n\\Expect [X^2]  = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha+1} e^{-\\beta{x}}  \\, dx \n             =  \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) } \\, \\frac{1}{\\beta^{\\alpha+2}} \\Gamma(\\alpha+2) \n              = \\frac{\\Gamma(\\alpha+2)}{\\beta^2 \\Gamma(\\alpha)}\n$$\nNow use following formula twice $\\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha)$:\n$$\n\\Gamma(\\alpha+2)= (\\alpha + 1) \\Gamma(\\alpha + 1) = (\\alpha + 1) \\alpha \\Gamma(\\alpha)\n$$\nSubstituting we get\n$$\n\\Expect [X^2] = \\frac{\\Gamma(\\alpha+2)}{\\beta^2 \\Gamma(\\alpha)} = \\frac{(\\alpha+1) \\alpha}{\\beta^2}\n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nTherefore\n$$\n\\Expect [X] = \\frac{\\alpha}{\\beta}  \\quad \\qquad\n\\Expect [X^2] = \\frac{(\\alpha+1) \\alpha}{\\beta^2}\n$$\nand the variance is\n\\begin{align*}\n\\Var[X] & = \\Expect [X^2] - \\Expect [X]^2 \\\\\n        & = \\frac{(\\alpha+1) \\alpha}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\\n        & = \\frac{\\alpha}{\\beta^2}\n\\end{align*}\n\n\n\n\n\n\n\n\n## Moment generating function {.smaller}\n\n\n- We abbreviate **Moment generating function** with **MGF**\n\n- MGF is almost the Laplace transform of the probability density function\n\n- MGF provides a short-cut to calculating mean and variance\n\n- MGF gives a way of proving distributional results for sums of independent random variables\n\n\n\n\n## Moment generating function {.smaller}\n\n\n::: Definition\n\nThe **moment generating function** or **MGF** of a rv $X$ is\n$$\nM_X(t) := \\Expect [e^{tX}] \\,, \\quad \\forall \\, t \\in \\R\n$$\n\n:::\n\nIn particular we have:\n\n::: {.column width=\"48%\"}\n\n- $X$ discrete:\n$$\nM_X(t) = \\sum_{x \\in \\R} e^{tx} f_X(x)\n$$\n\n:::\n\n::: {.column width=\"48%\"}\n\n- $X$ continuous:\n$$\nM_X(t) = \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx\n$$\n\n:::\n\n\n\n\n## Moment generating function {.smaller}\n### Computing moments\n\n\n::: Theorem\n\nIf $X$ has MGF $M_X$ then\n$$\n\\Expect[X^n] = M_X^{(n)} (0)\n$$\nwhere we denote\n$$\nM_X^{(n)} (0) :=  \\frac{d^n}{dt^n} M_X^{(n)}(t) \\bigg|_{t=0}\n$$\n\n:::\n\nThe quantity $\\Expect[X^n]$ is called **$n$-th moment** of $X$\n \n\n\n\n## Moment generating function {.smaller}\n### Proof of Theorem\n\nSuppose $X$ continuous and that we can exchange derivative and integral:\n\\begin{align*}\n\\frac{d}{dt} M_X(t) & = \\frac{d}{dt} \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx \n                      = \\int_{-\\infty}^\\infty \\left( \\frac{d}{dt} e^{tx} \\right) f_X(x) \\, dx \\\\\n                    & = \\int_{-\\infty}^\\infty xe^{tx} f_X(x) \\, dx \n                      = \\Expect(Xe^{tX})\n\\end{align*}\nEvaluating at $t = 0$:\n$$\n\\frac{d}{dt} M_X(t) \\bigg|_{t = 0} = \\Expect(Xe^{0}) = \\Expect[X]\n$$\n\n\n\n\n## Moment generating function {.smaller}\n### Proof of Theorem\n\nProceeding by induction we obtain:\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\Expect(X^n e^{tX})\n$$\nEvaluating at $t = 0$ yields the thesis:\n$$\n\\frac{d^n}{dt^n} M_X(t) \\bigg|_{t = 0} = \\Expect(X^n e^{0}) = \\Expect[X^n]\n$$\n\n\n\n\n\n\n## Moment generating function {.smaller}\n### Notation\n\nFor the first 3 derivatives we use special notations:\n\n$$\nM_X'(0) := M^{(1)}_X(0) = \\Expect[X] \n$$\n$$\nM_X''(0) := M^{(2)}_X(0) = \\Expect[X^2] \n$$\n$$\nM_X'''(0) := M^{(3)}_X(0) = \\Expect[X^3] \n$$\n\n\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Definition\n\n\n- The **normal distribution** with mean $\\mu$ and variance $\\sigma^2$ is\n$$\nf(x) := \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\,, \\quad x \\in \\R\n$$\n\n\n- $X$ has **normal distribution** with mean $\\mu$ and variance $\\sigma^2$ if $f_X = f$\n\n    * In this case we write $X \\sim N(\\mu,\\sigma^2)$\n\n- The **standard normal distribution** is denoted $N(0,1)$\n\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Plot\n\nPlotting $N(\\mu,\\sigma^2)$ for parameters $(0,1)$ and $(3,2)$\n\n```{r}\n# Set the shape and rate parameters for the first gamma distribution\nmu1 <- 0\nsigma1 <- 1\n\n# Set the shape and rate parameters for the second gamma distribution\nmu2 <- 3\nsigma2 <- 2\n\n# Generate values for the x-axis\nx_values <- seq(-7, 7, by = 0.1)\n\n# Calculate the probability density function (PDF) values for each x for the first distribution\npdf_values1 <- dnorm(x_values, mean = mu1, sd = sqrt(sigma1))\n\n# Calculate the probability density function (PDF) values for each x for the second distribution\npdf_values2 <- dnorm(x_values, mean = mu2, sd = sqrt(sigma2))\n\n# Plot the gamma PDFs\nplot(x_values, \n    pdf_values1, \n    type = \"l\", \n    col = \"blue\", lwd = 2,\n    xlab = \"\", \n    ylab = \"\",\n    ylim = c(0, max(pdf_values1, pdf_values2) + 0.1))\n\nlines(x_values, \n      pdf_values2, \n      col = \"red\", \n      lwd = 2)\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"pdf\", side=2, line=2.5, cex=2)\n\n# Add a legend\nlegend(\"topright\", legend = c(paste(\"N(\", mu1, \",\", sigma1, \")\", sep = \"\"),\n                              paste(\"N(\", mu2, \",\", sigma2, \")\", sep = \"\")),\n       col = c(\"blue\", \"red\"), lty = 1, lwd = 2, cex = 1.5)\n\n``` \n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nThe equation for the normal pdf is\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\nBeing pdf, we must have $\\int f_X(x) \\, dx = 1$. This yields:\n\\begin{equation} \\tag{1}\n\\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{x^2}{2\\sigma^2} + \\frac{\\mu{x}}{\\sigma^2} \\right) \\, dx = \\exp \\left(\\frac{\\mu^2}{2\\sigma^2} \\right) \\sqrt{2\\pi} \\sigma\n\\end{equation}\n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nWe have\n\\begin{align*}\nM_X(t) & := \\Expect (e^{tX}) \n         = \\int_{-\\infty}^{\\infty} e^{tx} f_X(x) \\, dx  \\\\\n       & = \\int_{-\\infty}^{\\infty} e^{tx} \\frac{1}{\\sqrt{2\\pi}\\sigma} \n       \\exp \\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\, dx \\\\\n       & = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} e^{tx} \n       \\exp \\left( -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} + \\frac{x\\mu}{\\sigma^2} \\right) \\, dx \\\\     \n       & = \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2} \\right) \n       \\frac{1}{\\sqrt{2\\pi}\\sigma}\n       \\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx\n\\end{align*}\n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nWe have shown\n\\begin{equation} \\tag{2}\nM_X(t) = \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2} \\right) \n       \\frac{1}{\\sqrt{2\\pi}\\sigma}\n       \\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx\n\\end{equation}\nReplacing $\\mu$ by $(t\\sigma^2 + \\mu)$ in (1) we obtain\n\\begin{equation} \\tag{3}\n\\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx \n= \\exp \\left( \\frac{(t\\sigma^2+\\mu)^2}{2\\sigma^2}  \\right) \\,  \n\\frac{1}{\\sqrt{2\\pi}\\sigma}\n\\end{equation}\nSubstituting (3) in (2) and simplifying we get\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Mean\n\nRecall the mgf\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nThe first derivative is\n$$\nM_X'(t) = (\\mu + \\sigma^2 t ) \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nTherefore the mean:\n$$\n\\Expect [X] = M_X'(0) = \\mu \n$$\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Variance\n\nThe first derivative of mgf is\n$$\nM_X'(t) = (\\mu + \\sigma^2 t ) \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nThe second derivative is then\n$$\nM_X''(t) = \\sigma^2  \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right) +\n(\\mu + \\sigma^2 t )^2 \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nTherefore the second moment is:\n$$\n\\Expect [X^2] = M_X''(0) = \\sigma^2 + \\mu^2 \n$$\n\n\n\n## Example - Normal distribution {.smaller}\n### Variance\n\nWe have seen that:\n$$\n\\Expect[X] = \\mu   \\quad  \\qquad \\Expect [X^2] = \\sigma^2 + \\mu^2 \n$$\nTherefore the variance is:\n\\begin{align*}\n\\Var[X] & = \\Expect[X^2] - \\Expect[X]^2 \\\\\n        & =  \\sigma^2 + \\mu^2 - \\mu^2 \\\\\n        & = \\sigma^2 \n\\end{align*}\n\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nSuppose $X \\sim \\Gamma(\\alpha,\\beta)$. This means\n$$\nf_X(x) = \n\\begin{cases} \n\\dfrac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  & \\text{ if } x > 0 \\\\\n0                                                                 & \\text{ if } x \\leq 0\n\\end{cases}\n$$\n\n- We have seen already that \n$$\n\\Expect[X] = \\frac{\\alpha}{\\beta} \\quad  \\qquad\n\\Var[X] =  \\frac{\\alpha}{\\beta^2}\n$$\n\n\n- We want to compute mgf $M_X$ to derive again $\\Expect[X]$ and $\\Var[X]$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nWe compute\n\\begin{align*}\nM_X(t) & = \\Expect [e^{tX}] = \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx \\\\\n       & = \\int_0^{\\infty} e^{tx} \\, \\frac{x^{\\alpha-1}e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, dx \\\\\n       & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx\n\\end{align*}\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nFrom the previous slide we have\n$$\nM_X(t) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx\n$$\nChange variable $y=(\\beta-t)x$ and recall the definition of $\\Gamma$:\n\\begin{align*}\n\\int_0^{\\infty} x^{\\alpha-1} e^{-(\\beta-t)x} \\, dx & =\n\\int_0^{\\infty} \\frac{1}{(\\beta-t)^{\\alpha-1}} [(\\beta-t)x]^{\\alpha-1} e^{-(\\beta-t)x}  \\frac{1}{(\\beta-t)} (\\beta - t) \\, dx   \\\\\n& = \\frac{1}{(\\beta-t)^{\\alpha}} \\int_0^{\\infty} y^{\\alpha-1} e^{-y}  \\, dy \\\\\n& = \\frac{1}{(\\beta-t)^{\\alpha}} \\Gamma(\\alpha)\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nTherefore\n\\begin{align*}\nM_X(t) & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx \\\\\n       & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\cdot \\frac{1}{(\\beta-t)^{\\alpha}} \\Gamma(\\alpha) \\\\\n       & = \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expectation\n\nFrom the mgf\n$$\nM_X(t) = \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n$$\nwe compute the first derivative:\n\\begin{align*}\nM_X'(t) & = \\frac{d}{dt} [\\beta^{\\alpha}(\\beta-t)^{-\\alpha}] \\\\\n        & = \\beta^{\\alpha}(-\\alpha)(\\beta-t)^{-\\alpha-1}(-1) \\\\\n        & = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expectation\n\nFrom the first derivative\n$$\nM_X'(t) = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n$$\nwe compute the expectation\n\\begin{align*}\n\\Expect[X] & = M_X'(0) \\\\\n           & = \\alpha\\beta^{\\alpha}(\\beta)^{-\\alpha-1} \\\\\n           & =\\frac{\\alpha}{\\beta}  \n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the first derivative\n$$\nM_X'(t) = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n$$\nwe compute the second derivative\n\\begin{align*}\nM_X''(t) & = \\frac{d}{dt}[\\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}] \\\\\n         & = \\alpha\\beta^{\\alpha}(-\\alpha-1)(\\beta-t)^{-\\alpha-2}(-1)\\\\\n         & = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta-t)^{-\\alpha-2}\n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the second derivative\n$$\nM_X''(t) = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta-t)^{-\\alpha-2}\n$$\nwe compute the second moment:\n\\begin{align*}\n\\Expect[X^2] & = M_X''(0) \\\\\n             & = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta)^{-\\alpha-2} \\\\\n             & = \\frac{\\alpha(\\alpha + 1)}{\\beta^2}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the first and second moments:\n$$\n\\Expect[X] = \\frac{\\alpha}{\\beta} \\qquad \\qquad\n\\Expect[X^2] = \\frac{\\alpha(\\alpha + 1)}{\\beta^2}\n$$\nwe can compute the variance\n\\begin{align*}\n\\Var[X] & = \\Expect[X^2] - \\Expect[X]^2 \\\\\n        & = \\frac{\\alpha(\\alpha + 1)}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\\n        & = \\frac{\\alpha}{\\beta^2}\n\\end{align*}\n\n\n\n\n## Moment generating function {.smaller}\n### The mgf characterizes a distribution\n\n\n::: Theorem\n\nLet $X$ and $Y$ be random variables with mgfs $M_X$ and $M_Y$ respectively. Assume there exists $\\e>0$ such that\n$$\nM_X(t) = M_Y(t) \\,, \\quad \\forall \\, t \\in (-\\e , \\e)\n$$\nThen $X$ and $Y$ have the same cdf\n$$\nF_X(u) = F_Y(u) \\,, \\quad \\forall \\, x \\in \\R\n$$\n\n:::\n\nIn other words: $\\qquad$ **same mgf** $\\quad \\implies \\quad$ **same distribution**\n\n\n\n\n## Example  {.smaller}\n\n- Suppose $X$ is a random variable such that\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nAs the above is the mgf of a normal distribution, by the previous Theorem we infer $X \\sim N(\\mu,\\sigma^2)$\n\n- Suppose $Y$ is a random variable such that\n$$\nM_Y(t) =  \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n$$\nAs the above is the mgf of a Gamma distribution, by the previous Theorem we infer $Y \\sim \\Gamma(\\alpha,\\beta)$\n\n\n\n\n\n\n\n\n\n\n\n## References\n\n\n\n\n\n\n::: {.content-hidden}\n\n\n\n\n# Thank you! {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n\n::: footer\n\n<div color=\"#cc0164\"> </div>\n\n:::\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.8em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n::: {style=\"font-size: 0.6em\"}\n\n```{r}\n#| echo: false\n#| layout-ncol: 3\n#| tbl-cap: \"Dataset with 33 entries for Stock and Gold price pairs\"\n\n# Read dataset\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\n\nknitr::kable(data[1:11,], row.names = TRUE)\n\nknitr::kable(data[12:22,], row.names = TRUE)\n\nknitr::kable(data[23:33,], row.names = TRUE)\n\n```\n\n:::\n\n:::\n\n","srcMarkdownNoYaml":"\n\n\n\n::: {.content-hidden}\n$\n{{< include macros.tex >}}\n$\n:::\n\n\n\n\n\n# Lecture 1: <br>An introduction to Statistics {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Outline of Lecture 1\n\n1. Module info\n2. Introduction\n3. Probability revision\n4. Moment generating functions\n\n\n\n\n\n# Part 1: <br>Module info {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Contact details\n\n- **Lecturer:** Dr. Silvio Fanzon\n- **Call me:**\n  * Silvio\n  * Dr. Fanzon\n- **Email:** S.Fanzon@hull.ac.uk\n- **Office:** Room 104a, Larkin Building\n- **Office hours:** Wednesday 12:00-13:00\n- **Meetings**: in my office or send me an Email\n\n\n\n\n\n## Questions\n\n\n- If you have any questions please feel free to ``email me``\n\n- We will address ``Homework`` and ``Coursework`` in class\n\n- In addition, please do not hesitate to attend ``office hours``\n\n\n\n## Lectures\n\nEach week we have\n\n- 2 Lectures of 2h each\n- 1 Tutorial of 1h\n\n|    Session    |      Date        |      Place        |\n|---------------|------------------|-------------------|\n|  Lecture 1    |  Wed 10:00-12:00 | Wilberforce LR 22 |\n|  Lecture 2    |  Thu 15:00-17:00 | Wilberforce LR 10 |\n|  Tutorial     |  Thu 11:00-12:00 | Wilberforce LR 7  |\n\n\n\n\n\n## Assessment \n\nThis course will be assessed as follows:\n\n<br>\n\n\n|**Type of Assessment**  | **Percentage of final grade** |\n|:-----                  |:-----                         |\n|  Coursework Portfolio  | 70%                           |\n|  Homework              | 30%                           |\n\n\n\n\n\n## Rules for Coursework\n\n\n- Coursework available on Canvas from Week 3\n\n- We will discuss coursework exercises in class\n\n- Coursework must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- Deadline: **14:00 on Thursday 2nd May**\n\n\n\n\n\n\n## Rules for Homework\n\n\n- 10 Homework papers, posted weekly on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Each Homework paper is worth 14 points\n\n- Final Homework grade computation: \n    * Sum the top 7 scores (max score 98 points)\n    * Bonus 2 points will be added to the final score\n\n- Homework papers must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** \n\n- Deadlines: **14:00 on Mondays** \n\n\n\n\n\n\n## Key submission dates\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 1      | 5 Feb       |\n| Homework 2      | 12 Feb      |\n| Homework 3      | 19 Feb      |\n| Homework 4      | 26 Feb      |\n| Homework 5      | 4 Mar       |\n| Homework 6      | 11 Mar      |\n\n\n:::\n\n\n\n::: {.column width=\"48%\"}\n\n|  **Assignment** |**Due date** |\n|:--------        |:----------- |\n| Homework 7      | 18 Mar      |\n| Easter Break    | :sunglasses:|\n| Homework 8      | 8 Apr       |\n| Homework 9      | 15 Apr      |\n| Homework 10     | 22 Apr      |\n| Coursework      | 2 May       |\n\n:::\n\n\n\n\n## How to submit assignments\n\n\n- Submit **PDFs only** on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**\n\n- You have two options:\n\t* Write on tablet and submit PDF Output\n\t* Write on paper and **Scan in Black and White** using a **Scanner** or **Scanner App** (Tiny Scanner, Scanner Pro, ...)\n\n\n\n\n**Important**: I will not mark\n\n\n- Assignments submitted **outside of Canvas**\n- Assignments submitted **After the Deadline**\n\n\n\n\n\n\n## References\n### Main textbooks  \n\n\n::: {.column width=\"61%\"}\n\n<br>\n\nSlides are self-contained and based on the book\n\n- [@bingham-fry] Bingham, N. H. and Fry, J. M. \n<br> *Regression: Linear models in statistics.* <br> Springer, 2010\n\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/bingham_fry.png){width=82%}](https://link.springer.com/book/10.1007/978-1-84882-969-5)\n\n:::\n\n\n\n\n\n## References\n### Main textbooks \n\n\n::: {.column width=\"61%\"}\n<br>\n\n.. and also on the book\n\n- [@fry-burke] Fry, J. M. and Burke, M. \n<br>*Quantitative methods in finance using R.* \n<br>Open University Press, 2022\n\n:::\n\n\n::: {.column width=\"38%\"}\n\n[![](images/fry_burke.png){width=87%}](https://www.mheducation.co.uk/quantitative-methods-in-finance-using-r-9780335251261-emea-group)\n\n:::\n\n\n\n\n\n\n\n## References\n### Secondary References \n\n::: {.column width=\"69%\"}\n- [@casella-berger] Casella, G. and Berger R. L. <br>\n*Statistical inference.*\n<br> Second Edition, Brooks/Cole, 2002\n\n\n- [@degroot] DeGroot M. H. and Schervish M. J. <br> \n*Probability and Statistics.* \n<br> Fourth Edition, Addison-Wesley, 2012\n\n\n- [@dalgaard] Dalgaard, P. \n<br> *Introductory statistics with R.*\n<br> Second Edition, Springer, 2008   \n\n:::\n\n\n::: {.column width=\"30%\"}\n\n**Probability & Statistics manual**\n\n**Easier Probability & Statistics manual**\n\n**R manual**\n\n:::\n\n\n\n\n\n\n# Part 2: <br>Introduction {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## The nature of Statistics\n### Statistics is a mathematical subject\n\n- Maths skills will give you a head start\n\n- There are other occasions where common sense and *detective skills* can be more important\n\n- Provides an early example of mathematics working in concert with the available computation\n\n\n\n\n## The nature of Statistics\n### We will use a combination of hand calculation and software\n\n- Recognises that you are maths students\n- Software (R) is really useful, particularly for dissertations\n- Please bring your laptop into class\n- Download R onto your laptop\n\n\n\n\n\n\n\n## Overview of the module\n\n\nModule has **11 Lectures**, divided into two parts:\n\n- **Part I** - Mathematical statistics\n\n- **Part II** - Applied statistics\n\n\n\n\n\n## Overview of the module\n### Part I - Mathematical statistics\n\n1. Introduction to statistics\n2. Normal distribution family and one-sample hypothesis tests\n3. Two-sample hypothesis tests\n4. The chi-squared test\n5. Non-parametric statistics\n6. The maths of regression\n\n\n\n\n\n## Overview of the module\n### Part II - Applied statistics\n\n\n7. An introduction to practical regression\n8. The extra sum of squares principle and regression modelling assumptions\n9. Violations of regression assumptions -- Autocorrelation\n10. Violation of regression assumptions -- Multicollinearity\n10. Dummy variable regression models \n\n\n\n\n\n\n\n## Simple but useful questions {.smaller}\n\n\n::: {.column width=\"48%\"}\n\n**Generic data:**\n\n- What is a *typical* observation\n  * What is the **mean**?\n\n- How spread out is the data?\n  * What is the **variance**?\n\n:::\n\n\n::: {.column width=\"48%\"}\n\n**Regression:**\n\n- What happens to $Y$ as $X$ increases?\n  * increases?\n  * decreases?\n  * nothing?\n\n:::\n\n\n\n**Statistics answers these questions systematically**\n\n- important for large datasets\n- The same mathematical machinery (normal family of distributions) can be applied to both questions\n\n\n\n\n## Analysing a general dataset\n\n**Two basic questions:**\n\n1. Location or mean\n2. Spread or variance\n\n\n**Statistics enables to answer systematically:**\n\n1. One sample and two-sample $t$-test\n2. Chi-squared test and $F$-test\n\n\n\n\n\n## Recall the following sketch\n\n![Curve represents data distribution](images/Fig1.png){width=90%}\n\n\n\n\n## Motivating regression\n\n**Basic question in regression:**\n\n- What happens to $Y$ as $X$ increases?\n\n  * increases?\n  * decreases?\n  * nothing?\n\n\n**In this way regression can be seen as a more advanced version of high-school maths**\n\n\n\n\n\n\n\n## Positive gradient\n\nAs $X$ increases $Y$ increases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = 1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n## Negative gradient\n\nAs $X$ increases $Y$ decreases\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(-3, 3), \n  ylim = c(-3, 3), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 0, \n  b = -1, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Zero gradient\n\nChanges in $X$ do not affect $Y$\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\n\n# Create a scatter plot with no data points (type = \"n\")\nplot(\n  1, 1, \n  type = \"n\", \n  xlab = \"\", \n  ylab = \"\", \n  xlim = c(0, 5), \n  ylim = c(0, 5), \n  frame.plot = TRUE, \n  axes = FALSE, \n  asp = 1)\n\n\nmtext(\"X\", side=1, line=3, cex=3)\nmtext(\"Y\", side=2, line=2, cex=3)\n\n# Add the line y = x\nabline(\n  a = 2.5, \n  b = 0, \n  col = \"black\", \n  lwd = 2)\n\n```\n\n\n\n\n\n## Real data example\n\n\n- Real data is more **imperfect**\n- But the same basic idea applies\n- Example: \n    * $X =$ Stock price\n    * $Y =$ Gold price\n\n\n\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.75em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n\n::: {style=\"font-size: 0.55em\"}\n```{r}\n#| echo: false\n#| layout-ncol: 1\n\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\nknitr::kable(\n  list(data[1:11,], data[12:22,], data[23:33,]),\n  row.names = TRUE,\n  format = \"html\", \n  table.attr = 'class=\"table simple table-striped table-hover\"',\n) \n\n\n```\n\n\n:::\n\n\n\n\n## Real data example {.smaller}\n### Visualizing the data\n\n\n::::: {.columns style='display: flex !important; height: 80%;'}\n\n::: {.column width=\"38%\" style='display: flex; justify-content: center; align-items: center;'}\n\n- Plot Stock Price against Gold Price\n\n- Observation: \n\n  * As Stock price decreases, Gold price increases\n\n- Why? This might be because:\n    * Stock price decreases\n    * People invest in secure assets (Gold)\n    * Gold demand increases\n    * Gold price increases\n\n:::\n\n\n::: {.column width=\"61%\" style='display: flex; justify-content: center; align-items: center;'}\n\n```{r}\n#| echo: false\n#| fig-asp: 1\n\ndata1 <- read.table(\"datasets/L3eg1data.txt\")\nrealgoldprice<-data1[,1]\nrealstockprice<-data1[,2]\nplot(realgoldprice,\n  realstockprice, \n  xlab=\"\", \n  ylab=\"\")\n\nmtext(\"Stock Price\", side=1, line=3, cex=2)\nmtext(\"Gold Price\", side=2, line=2.5, cex=2)\n\n```\n\n:::\n\n:::::\n\n\n\n\n## Don't panic {.smaller}\n\n- Regression problems can look a lot harder than they really are\n  * Basic question remains the same: what happens to $Y$ as $X$ increases?\n\n- Beware of jargon. Various authors distinguish between\n  * Two variable regression model\n  * Multiple regression model\n  * Analysis of Variance\n  * Analysis of Covariance\n\n- Despite these apparent differences:\n  * Mathematical methodology stays (essentially) the same\n  * regression-fitting commands in R stay (essentially) the same\n\n\n\n\n\n# Part 3: <br>Probability revision {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n## Probability revision {.smaller}\n\n\n- We start with reviewing some fundamental Probability notions\n- You saw these in the Y1 module **Introduction to Probability & Statistics**\n- We will adopt a slightly more mature mathematical approach\n- Remember: The mathematical description might look (a bit) different, but the concepts are the same\n\n**Topics reviewed**:\n    \n::: {.column width=\"38%\"}\n    \n- Sample space\n- Events\n- Probability measure\n- Conditional probability\n- Events independence\n\n:::\n\n::: {.column width=\"38%\"}\n\n- Random Variable\n- Distribution\n- cdf\n- pmf\n- pdf\n\n:::\n\n\n\n\n\n## Sample space {.smaller}\n\n\n::: Definition\n### Sample space\n\nA set $\\Omega$ of all possible outcomes of some experiment\n\n:::\n\n\n**Examples**:\n\n- Coin toss: results in Heads $= H$ and Tails $= T$ \n$$\n\\Omega = \\{ H, T \\}\n$$\n\n- Student grade for *Statistical Models*: a number between $0$ and $100$\n$$\n\\Omega = \\{ x \\in \\mathbb{R} \\, \\colon \\, 0 \\leq x \\leq 100 \\} = [0,100]\n$$\n\n\n\n\n\n\n## Events {.smaller}\n\n\n::: Definition\n### Event\n\nA subset $E$ of the sample space $\\Omega$ (including $\\emptyset$ and $\\Omega$ itself)\n\n:::\n\n\n**Operations with events**:\n\n- **Union** of two events $A$ and $B$\n$$\nA \\cup B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ or } \\, x \\in B \\}\n$$\n\n- **Intersection** of two events $A$ and $B$\n$$\nA \\cap B := \\{ x \\in \\Omega \\colon x \\in A \\, \\text{ and } \\, x \\in B \\}\n$$\n\n\n\n\n## Events {.smaller}\n\n\n**More Operations with events**:\n\n- **Complement** of an event $A$\n$$\nA^c  := \\{ x \\in \\Omega \\colon x \\notin A  \\}\n$$\n\n\n- **Infinite Union** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcup_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for some } \\, i \\in I \\}\n$$\n\n- **Infinite Intersection** of a family of events $A_i$ with $i \\in I$\n\n$$\n\\bigcap_{i \\in I} A_i := \\{ x \\in \\Omega \\colon x \\in A_i \\, \\text{ for all } \\, i \\in I \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n\n**Example**: Consider sample space and events\n$$\n\\Omega := (0,1] \\,, \\quad \nA_i = \\left[\\frac{1}{i} , 1 \\right] \\,, \\quad i \\in \\mathbb{N}\n$$\nThen\n$$\n\\bigcup_{i \\in I} A_i = (0,1] \\,, \\quad \n\\bigcap_{i \\in I} A_i = \\{ 1 \\}\n$$\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Disjoint\n\nTwo events $A$ and $B$ are **disjoint** if\n$$\nA \\cap B = \\emptyset\n$$\nEvents $A_1, A_2, \\ldots$ are **pairwise disjoint** if\n$$\nA_i \\cap A_j = \\emptyset \\,, \\quad \\forall \\, i \\neq j\n$$\n:::\n\n\n\n\n\n## Events {.smaller}\n\n::: Definition\n### Partition\n\nThe collection of events  $A_1, A_2, \\ldots$ is a **partition** of $\\Omega$ if\n\n1. $A_1, A_2, \\ldots$ are pairwise disjoint\n2. $\\Omega = \\cup_{i=1}^\\infty A_i$\n\n:::\n\n\n\n\n\n\n## What's a Probability? {.smaller}\n\n\n- To each event $E \\subset \\Omega$ we would like to associate a number\n$$\nP(E) \\in [0,1]\n$$\n\n- The number $P(E)$ is called the **probability** of $E$\n\n- The number $P(E)$ models the **frequency of occurrence** of $E$:\n\n    * $P(E)$ small means $E$ has low chance of occurring\n    * $P(E)$ large means $E$ has high chance of occurring\n\n- **Technical issue**:\n\n    * One cannot associate a number $P(E)$ for all events in $\\Omega$\n    * Probability function $P$ only defined for a **smaller** family of events\n    * Such family of events is called $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n\n\n::: Definition\n### sigma-algebra\n\nLet $\\mathcal{B}$ be a collection of events. We say that $\\mathcal{B}$ is a $\\sigma$-algebra if\n\n1. $\\emptyset \\in \\mathcal{B}$\n2. If $A \\in \\mathcal{B}$ then $A^c \\in \\mathcal{B}$\n3. If $A_1,A_2 , \\ldots \\in \\mathcal{B}$ then $\\cup_{i=1}^\\infty A_i \\in \\mathcal{B}$\n\n\n:::\n\n\n\n**Remarks**:\n\n- Since $\\emptyset \\in \\mathcal{B}$ and $\\emptyset^c = \\Omega$, we deduce that\n$\\Omega \\in \\mathcal{B}$\n\n- Thanks to DeMorgan's Law we have that\n$$\nA_1,A_2 , \\ldots \\in \\mathcal{B} \\quad \\implies \\quad \\cap_{i=1}^\\infty A_i \\in \\mathcal{B}\n$$\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\nSuppose $\\Omega$ is any set: \n\n- Then\n$$\n\\mathcal{B} = \\{ \\emptyset, \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n- The power set of $\\Omega$\n$$\n\\mathcal{B} = \\operatorname{Power} (\\Omega) := \\{ A \\colon A \\subset \\Omega \\}\n$$\nis a $\\sigma$-algebra\n\n\n\n\n\n\n## $\\sigma$-algebras {.smaller}\n### Examples\n\n\n- If $\\Omega$ has $n$ elements then $\\mathcal{B} = \\operatorname{Power} (\\Omega)$ contains $2^n$ sets\n\n- If $\\Omega = \\{ 1,2,3\\}$ then\n\\begin{align*}\n\\mathcal{B} = \\operatorname{Power} (\\Omega) = \\big\\{ & \\{1\\} , \\, \\{2\\}, \\, \\{3\\}  \\\\\n                                       &  \\{1,2\\} , \\, \\{2,3\\}, \\, \\{1,3\\}      \\\\\n                                       & \\emptyset , \\{1,2,3\\}   \\big\\}\n\\end{align*}\n\n- If $\\Omega$ is **uncountable** then the power set of $\\Omega$ is not easy to describe\n\n\n\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Question\n\n$\\mathbb{R}$ is uncountable. Which $\\sigma$-algebra do we consider?\n\n:::\n\n\n::: Definition \n### Lebesgue sigma-algebra\n\n\nThe Lebesgue $\\sigma$-algebra on $\\mathbb{R}$ is the smallest $\\sigma$-algebra $\\mathcal{L}$ containing all sets of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \n$$\nfor all $a,b \\in \\mathbb{R}$\n\n:::\n\n\n\n## Lebesgue $\\sigma$-algebra {.smaller}\n\n\n::: Important\n\nTherefore the events of $\\R$ are \n\n- Intervals\n- Unions and intersection of intervals\n- Countable Unions and intersection of intervals\n\n:::\n\n\n::: Warning \n\n- I only told you that the Lebsesgue $\\sigma$-algebra $\\mathcal{L}$ **exists**\n- Explicitly showing that $\\mathcal{L}$ exists is not easy, see [@rosenthal]\n\n:::\n\n\n\n\n## Probability measure {.smaller}\n\nSuppose given:\n\n- $\\Omega$ sample space \n- $\\mathcal{B}$ a $\\sigma$-algebra on $\\Omega$\n\n::: Definition\n### Probability measure\n\nA **probability measure** on $\\Omega$ is a map\n$$\nP \\colon \\mathcal{B} \\to [0,1]\n$$\nsuch that the **Axioms of Probability** hold\n\n1. $P(\\Omega) = 1$\n2. If $A_1, A_2,\\ldots$ are pairwise disjoint then\n$P\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  = \\sum_{i=1}^\\infty P(A_i)$\n\n:::\n\n\n\n\n## Properties of Probability {.smaller}\n\nLet $A, B \\in \\mathcal{B}$. As a consequence of the Axioms of Probability:\n\n\n1. $P(\\emptyset) = 0$\n2. If $A$ and $B$ are disjoint then\n$$\nP(A \\cup B) = P(A) + P(B)\n$$\n3. $P(A^c) = 1 - P(A)$\n4. $P(A) = P(A \\cap B) + P(A \\cap B^c)$\n5. $P(A\\cup B) = P(A) + P(B) - P(A \\cap B)$\n6. If $A \\subset B$ then\n$$\nP(A) \\leq P(B)\n$$\n\n\n\n\n\n## Properties of Probability {.smaller} \n\n7. Suppose $A$ is an event and $B_1,B_2, \\ldots$ a partition of $\\Omega$. Then\n$$\nP(A) = \\sum_{i=1}^\\infty  P(A \\cap B_i)\n$$\n8. Suppose $A_1,A_2, \\ldots$ are events. Then\n$$\nP\\left( \\bigcup_{i=1}^\\infty  A_i \\right)  \\leq  \\sum_{i=1}^\\infty P(A_i)\n$$\n\n\n\n\n## Example: Fair Coin Toss {.smaller}\n\n- The sample space for **coin toss** is $\\Omega = \\{ H, T \\}$\n\n- We take as $\\sigma$-algebra the power set of $\\Omega$\n$$\n\\mathcal{B} = \\{  \\emptyset , \\, \\{H\\}  , \\, \\{T\\} , \\, \\{H,T\\}   \\}\n$$\n\n- We suppose that the coin is fair\n\n    * This means $P \\colon \\mathcal{B} \\to [0,1]$ satisfies\n    $$\n    P(\\{H\\}) = P(\\{T\\})\n    $$\n    * Assuming the above we get\n    $$\n    1 = P(\\Omega) = P(\\{H\\} \\cup \\{T\\}) = \n    P(\\{H\\}) + P(\\{T\\}) = 2 P(\\{H\\})\n    $$\n    * Therefore \n    $P(\\{H\\}) = P(\\{T\\}) = \\frac12$\n\n\n\n\n\n## Conditional Probability {.smaller}\n\n::: Definition\n### Conditional Probability\n\nLet $A,B$ be events in $\\Omega$ with \n$$\nP(B)>0\n$$\nThe **conditional probability** of $A$ given $B$ is\n$$\nP(A|B) := \\frac{P(A \\cap B)}{P(B)} \n$$\n\n:::\n\n\n- $P(A|B)$ represents the probability of $A$, knowing that $B$ happened\n- The function $A \\mapsto P(A|B)$ is a probability measure on $\\Omega$\n\n\n\n\n## Bayes' Rule {.smaller}\n\n\n- For two events $A$ and $B$ is holds\n\n$$\nP(A | B ) = P(B|A) \\frac{P(A)}{P(B)}\n$$\n\n\n- Given a partition $A_1, A_2, \\ldots$ of the sample space we have\n\n$$\nP(A_i | B ) = \\frac{ P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B | A_j) P(A_j)}\n$$\n\n\n\n\n\n## Independence {.smaller}\n\n\n::: Definition\n\nTwo events $A$ and $B$ are **independent** if \n$$\nP(A \\cap B) = P(A)P(B)\n$$\nA collection of events $A_1 , \\ldots ,A_n$ are **mutually independent** if\nfor any subcollection $A_{i_1}, \\ldots, A_{i_k}$ it holds\n$$\nP \\left(  \\bigcap_{j=1}^k A_j  \\right) = \\prod_{j=1}^k  P(A_{i_j})\n$$\n\n:::\n\n\n\n\n\n## Random Variables {.smaller}\n### Motivation\n\n- Consider the experiment of flipping a coin $50$ times\n- The sample space consists of $2^{50}$ elements\n- Elements are vectors of $50$ entries recording the outcome $H$ or $T$ of each flip\n- This is a very large sample space!\n\n\nSuppose we are only interested in\n$$\nX = \\text{ number of } \\, H \\, \\text{ in } \\, 50 \\, \\text{flips}\n$$\n\n- Then the new sample space is the set of integers\n$\\{ 0,1,2,\\ldots,50\\}$\n- This is much smaller!\n- $X$ is called a Random Variable\n\n\n\n\n## Random Variables {.smaller}\n\n\nAssume given\n\n- $\\Omega$ sample space\n- $\\mathcal{B}$ a $\\sigma$-algebra of events on $\\Omega$\n- $P \\colon \\mathcal{B} \\to [0,1]$ a probability measure\n\n\n::: Definition\n### Random variable\n\nA function $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\nWe will abbreviate Random Variable with rv\n\n\n\n\n\n## Random Variables {.smaller}\n### Technical remark \n\n\n::: Definition\n### Random variable\n\nA [**measurable**]{style=\"color:#cc0164;\"}\nfunction $X \\colon \\Omega \\to \\mathbb{R}$\n\n:::\n\n\n**Technicality**: \n$X$ is a [**measurable**]{style=\"color:#cc0164;\"}\nfunction if\n$$\n\\{ X \\in I \\} := \\{ \\omega \\in \\Omega \\colon X(\\omega) \\in I \\} \\in \\mathcal{B} \\,, \\quad \\forall \\, I \\in \\mathcal{L}\n$$\nwhere \n\n- $\\mathcal{L}$ is the Lebsgue $\\sigma$-algebra on $\\mathbb{R}$\n- $\\mathcal{B}$ is the given $\\sigma$-algebra on $\\Omega$\n\n\n\n\n## Random Variables {.smaller}\n### Notation\n\n- In particular $I \\in \\mathcal{L}$ can be of the form\n$$\n(a,b) \\,, \\quad (a,b] \\,, \\quad [a,b) \\,, \\quad [a,b] \\,, \\quad \n\\forall \\, a, b \\in \\mathbb{R}\n$$\n\n- In this case the set \n$$\n\\{X \\in I\\} \\in \\mathcal{B}\n$$ \nis denoted by, respectively:\n$$\n\\{ a < X < b \\} \\,, \\quad \\{ a < X \\leq b \\} \\,, \\quad \\{ a \\leq X < b \\} \\,, \\quad \\{ a \\leq X \\leq b \\}\n$$\n\n- If $a=b=x$ then $I=[x,x]=\\{x\\}$. Then we denote\n$$\n\\{X \\in I\\} = \\{X = x\\}\n$$\n\n\n\n## Distribution {.smaller}\n### Why do we require measurability?\n\n**Answer**: Because it allows to define a new probability measure on $\\mathbb{R}$\n\n\n::: Definition\n### Distribution\n\nThe **distribution** of a random variable $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( \\{X \\in I\\} \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n:::\n\n\n**Note**: \n\n- One can show that $P_X$ satisfies the *Probability Axioms*\n- Thus $P_X$ is a probability measure on $\\mathbb{R}$\n- In the future we will denote \n$P \\left( X \\in I \\right) := P \\left( \\{X \\in I\\} \\right)$\n\n\n\n\n\n\n\n## Distribution {.smaller}\n### Why is the distribution useful?\n\n**Answer**: Because it allows to define a random variable $X$\n\n- by specifying the distribution values\n$$\nP \\left( X \\in I \\right) \n$$\n- rather than defining an explicit function $X \\colon \\Omega \\to \\mathbb{R}$\n\n\n**Important**: More often than not\n\n- We care about the distribution of $X$\n- We do not care about how $X$ is defined\n\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Sample space $\\Omega$ given by the below values of $\\omega$\n\n+:---------:+:----:+:---:+:---:+:---:+:---:+:---:+:---:+:---:+\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n+-----------+------+-----+-----+-----+-----+-----+-----+-----+\n\n\n- The probability of each outcome is the same\n$$\nP(\\omega) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8} \\,, \\quad \\forall \\, \\omega \\in \\Omega\n$$\n\n\n- Define the random variable $X \\colon \\Omega \\to \\R$ by\n$$\nX(\\omega) :=  \\text{ Number of H in }  \\omega\n$$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The range of $X$ is $\\{0,1,2,3\\}$\n\n- Hence the only interesting values of $P_X$ are\n$$\nP(X=0) \\,, \\quad \nP(X=1) \\,, \\quad \nP(X=2) \\,, \\quad \nP(X=3)\n$$\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- We compute\n\\begin{align*}\nP(X=0) & = P(TTT) = \\frac{1}{8} \\\\\nP(X=1) & = P(TTH) + P(THT) + P(HTT) = \\frac{3}{8} \\\\\nP(X=2) & = P(HHT) + P(HTH) + P(THH) = \\frac{3}{8} \\\\\nP(X=3) & = P(HHH) = \\frac{1}{8}\n\\end{align*}\n\n\n\n\n## Example - Three coin tosses {.smaller}\n\n\n- Recall the definition of $X$\n\n|           |      |     |     |     |     |     |     |     |\n|:---------:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| $\\omega$  | HHH  | HHT | HTH | THH | TTH | THT | HTT | TTT |\n|$X(\\omega)$|  $3$ | $2$ | $2$ | $2$ | $1$ | $1$ | $1$ | $0$ | \n\n\n- The distribution of $X$ is summarized in the table below\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n\n\n\n## Cumulative Distribution Function {.smaller}\n\n\n**Recall**: The **distribution** of a rv $X \\colon \\Omega \\to \\mathbb{R}$ is the probability measure on $\\mathbb{R}$\n$$\nP_X \\colon \\mathcal{L} \\to [0,1] \\,, \\quad P_X (I) := P \\left( X \\in I \\right) \\,, \\,\\, \\forall \\, I \\in  \\mathcal{L}\n$$\n\n::: Definition\n### cdf\n\nThe **cumulative distribution function** or **cdf** of a rv $X \\colon \\Omega \\to \\mathbb{R}$ is\n$$\nF_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nF_X(x) := P_X (X \\leq x) \n$$\n\n:::\n\n\n**Intuition**: \n\n- $F_X$ is the *primitive* of $P_X$\n- $P_X$ will be the *derivative* of $F_X$^[In a suitable generalized sense]\n\n\n\n\n## Distribution Function {.smaller}\n### Example\n\n\n- Consider again 3 coin tosses and the rv\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- We computed that the distribution $P_X$ of $X$ is\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$P(X=x)$   | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n::: {.column width=\"38%\"}\n\n- One can compute\n$$\nF_X(x) = \n\\begin{cases}\n0  & \\text{if } x < 0 \\\\\n\\frac{1}{8}  & \\text{if } 0 \\leq x < 1 \\\\\n\\frac{1}{2}  & \\text{if } 1 \\leq x < 2 \\\\\n\\frac{7}{8}  & \\text{if } 2 \\leq x < 3 \\\\\n1            & \\text{if } 3 \\leq x\n\\end{cases}\n$$\n\n:::\n\n\n::: {.column width=\"56%\"}\n\n- For example\n\\begin{align*}\nF_X(2.1) & = P(X \\leq 2.1) \\\\\n         & = P(X=0,1 \\text{ or } 2) \\\\\n         & = P(X=0) + P(X=1) + P(X=2) \\\\\n         & = \\frac{1}{8} + \\frac{3}{8} + \\frac{3}{8} = \\frac{7}{8}\n\\end{align*}\n\n:::\n\n\n\n\n\n## Cumulative Distribution Function {.smaller}\n### Example\n\n\n```{r}\n# Define the step function\nstep_function <- function(x) {\n  if (x < 0) {\n    return(0)\n  } else if (x < 1) {\n    return(1/8)\n  } else if (x < 2) {\n    return(1/2)\n  } else if (x < 3) {\n    return(7/8)\n  } else {\n    return(1)\n  }\n}\n\n# Generate x values\nx_values <- seq(-1, 4, length.out = 1000)\n\n# Calculate corresponding y values using the step function\ny_values <- sapply(x_values, step_function)\n\n# Create a data frame for ggplot\nplot(x_values, \n    y_values, \n    type = 's',\n    xlab = \"\",\n    ylab = \"\")\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"F_X(x)\", side=2, line=2.5, cex=2)\n\n```\n\n\n\n::: {.column width=\"43%\"}\n\n- Plot of $F_X$: it is a **step function**\n- $F_X'=0$ except at $x=0,1,2,3$\n- $F_X$ jumps at $x=0,1,2,3$ \n\n:::\n\n::: {.column width=\"56%\"}\n\n- Size of jump at $x$ is $P(X=x)$ \n- $F_X'=P_X$ in the sense of **distributions**\n<br> (Advanced analysis concept - not covered)\n\n:::\n\n\n\n\n## Discrete Random Variables {.smaller}\n\nIn the previous example:\n\n- The cdf $F_X$ had **jumps**\n- Hence $F_X$ was **discountinuous**\n- We take this as definition of **discrete** rv\n\n\n\n::: Definition\n\n$X \\colon \\Omega \\to \\mathbb{R}$ is **discrete** if $F_X$ has jumps\n\n:::\n\n\n\n\n\n## Probability Mass Function {.smaller}\n\n- In this slide $X$ is a **discrete** rv\n- Therefore $F_X$ has **jumps**\n\n::: Definition\n\nThe **Probability Mass Function** or **pmf** of a discrete rv $X$ is\n$$\nf_X \\colon \\mathbb{R} \\to \\mathbb{R} \\,, \\quad \nf_X(x) := P(X = x) \n$$\n\n:::\n\n\n\n## Probability Mass Function {.smaller}\n### Properties\n\n::: Proposition\n\nThe **pmf** $f_X(x) = P(X=x)$ can be used to\n\n- compute probabilities\n    $$\n    P(a \\leq X \\leq b) = \\sum_{k = a}^b f_X (k) \\,, \\quad \\forall \\, a,b \\in \n    \\mathbb{Z} \\,, \\,\\, a \\leq b \n    $$\n- compute the **cdf**\n    $$\n    F_X(x) = P(X \\leq x) = \\sum_{k=-\\infty}^x f_X(k)\n    $$\n\n:::\n\n\n\n\n\n## Example 1 - Discrete RV {.smaller}\n\n- Consider again 3 coin tosses and the RV\n$X(\\omega) :=  \\text{ Number of H in }  \\omega$\n\n- The pmf of $X$ is $f_X(x):=P(X=x)$, which we have already computed\n\n|           |               |               |              |               |\n|:---------:|:-------------:|:-------------:|:------------:|:-------------:|\n| $x$       | $0$           | $1$           | $2$          |     $3$       |\n|$f_X(x)= P(X=x)$ | $\\frac{1}{8}$ | $\\frac{3}{8}$ | $\\frac{3}{8}$| $\\frac{1}{8}$ | \n: {tbl-colwidths=\"[40,15,15,15,15]\"}\n\n\n\n```{r}\n# Example data\nvalues <- c(0, 1, 2, 3)\nprobabilities <- c(1/8, 3/8, 3/8, 1/8)\n\n# Plot PMF\nplot(values, \n    probabilities, \n    xlab = \"\",\n    ylab = \"\",\n    xaxt='n',\n    type=\"h\", \n    lwd=4, \n    col=\"blue\")\n\naxis(1, at = values,labels = T)\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"f_X(x)\", side=2, line=2.5, cex=2)\n\npoints(values, probabilities, col = \"red\", pch = 16)\n\ngrid()\n```\n\n\n\n\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n- Suppose $p \\in (0,1)$ is a given probability of **success**\n- Hence $1-p$ is probability of **failure**\n- Consider the random variable \n$$\nX = \\text{ Number of attempts to obtain first success}\n$$\n- Since each trial is independent, the pmf of $X$ is\n$$\nf_X (x) = P(X=x) = (1-p)^{x-1} p \\,, \\quad \\forall \\, x \\in \\mathbb{N}\n$$\n- This is called **geometric distribution**\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n- We want to compute the cdf of $X$: For $x \\in \\mathbb{N}$ with $x > 0$\n\\begin{align*}\nF_X(x) & = P(X \\leq x) = \\sum_{k=1}^x P(X=k) = \\sum_{k=1}^x f_X(k) \\\\\n       & = \\sum_{k=1}^x (1-p)^{k-1} p \n         = \\frac{1-(1-p)^x}{1-(1-p)} p = 1 - (1-p)^x \n\\end{align*}\nwhere we used the formula for the sum of geometric series:\n$$\n\\sum_{k=1}^x t^{k-1} = \\frac{1-t^x}{1-t} \\,, \\quad t \\neq 1 \n$$\n\n\n\n## Example 2 - Geometric Distribution {.smaller}\n\n\n- $F_X$ is flat between two consecutive natural numbers:\n\\begin{align*}\nF_X(x+k) & = P(X \\leq x+k) \\\\\n         & = P(X \\leq x) \\\\\n         & = F_X(x) \n\\end{align*} \nfor all $x \\in \\mathbb{N}, k \\in [0,1)$\n\n- Therefore $F_X$ has **jumps** and $X$ is **discrete**\n\n\n\n\n\n## Continuous Random Variables {.smaller} \n\n**Recall**: $X$ is **discrete** if $F_X$ has **jumps**\n\n\n::: Definition\n### Continuous Random Variable\n\n$X \\colon \\Omega \\to \\mathbb{R}$ is **continuous** if \n$F_X$ is **continuous**\n\n:::\n\n\n\n\n\n\n\n## Probability Mass Function? {.smaller}\n\n- Suppose $X$ is a **continuous** rv\n- Therefore $F_X$ is **continuous**\n\n::: Question \n\nCan we define the Probability Mass Function for $X$?\n\n:::\n\n\n**Answer**: \n\n- Yes we can, but it would be useless - pmf carries no information\n- This is because \n$$\nf_X(x) = P(X=x) = 0 \\,, \\quad \\forall \\, x \\in \\mathbb{R}\n$$\n\n\n\n\n## Probability Mass Function? {.smaller}\n\n\n- Indeed, for all $\\varepsilon>0$ we have $\\{ X = x \\} \\subset \\{  x - \\varepsilon < X \\leq x  \\}$\n- Therefore by the properties of probabilities we have\n\\begin{align*}\nP (X = x ) & \\leq P(  x - \\varepsilon < X \\leq x ) \\\\\n           & = P(X \\leq x) - P(X \\leq x - \\varepsilon) \\\\\n           & = F_X(x) - F_X(x-\\varepsilon)\n\\end{align*}\nwhere we also used the definition of $F_X$\n- Since $F_X$ is continuous we get\n$$\n0 \\leq P(X = x) \\leq \\lim_{\\varepsilon \\to 0} F_X(x) - F_X(x-\\varepsilon) = 0\n$$\n- Then $f_X(x) = P(X=x) = 0$ for all $x \\in \\mathbb{R}$\n\n\n\n\n\n\n## Probability Density Function {.smaller}\n\n\n- **pmf** carries no information for continuous RV\n- We instead define the **pdf**\n\n\n::: Definition\n\nThe **Probability Density Function** or **pdf** of a continuous rv $X$ is\na function $f_X \\colon \\mathbb{R} \\to \\mathbb{R}$ s.t.\n$$\nF_X(x) =  \\int_{-\\infty}^x f_X(t) \\, dt  \\,, \\quad \\forall \\, x \\in\n\\mathbb{R}\n$$\n\n:::\n\n\n**Technical issue**: \n\n- If $X$ is continuous then pdf **does not exist** in general\n- Counterexamples are rare, therefore we will **assume** existence of pdf\n\n\n\n\n## Probability Density Function {.smaller}\n### Properties\n\n::: Proposition\n\nSuppose $X$ is continuous rv. They hold\n\n- The **cdf** $F_X$ is continuous and differentiable (a.e.) with\n$$\nF_X' = f_X\n$$\n\n- Probability can be computed via\n    $$\n    P(a \\leq X \\leq b) = \\int_{a}^b f_X (t) \\, dt \\,, \\quad \\forall \\, a,b \\in \n    \\mathbb{R} \\,, \\,\\, a \\leq b \n    $$\n\n:::\n\n\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n- The random variable $X$ has **logistic distribution** if its **pdf** is\n$$\nf_X(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n$$\n\n\n\n```{r}\n\n# Define the probability density function (PDF)\nf_X <- function(x) {\n  exp(-x) / (1 + exp(-x))^2\n}\n\n# Generate x values for plotting\nx_values <- seq(-10, 10, length.out = 1000)\n\n# Calculate corresponding y values\ny_values <- f_X(x_values)\n\n# Plot the distribution using base R plot function\nplot(x_values, \n    y_values, \n    type = \"l\", \n    col = \"blue\", \n    lwd = 4,\n    xlab = \"\", \n    ylab = \"\")\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"f_X\", side=2, line=2.5, cex=2)\n\n\n# Add a grid for better visualization\ngrid()\n\n```\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n- The random variable $X$ has **logistic distribution** if its **pdf** is\n$$\nf_X(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n$$\n\n\n\n- The **cdf** can be computed to be\n$$\nF_X(x) = \\int_{-\\infty}^\\infty f_X(t) \\, dt = \\frac{1}{1+e^{-x}}\n$$\n\n\n- The RHS is known as **logistic function**\n\n\n\n\n\n## Example - Logistic Distribution {.smaller}\n\n\n**Application**: Logistic function models expected score in chess (see [Wikipedia](https://en.wikipedia.org/wiki/Elo_rating_system))\n\n\n\n- $R_A$ is ELO rating of player $A$, $R_B$ is ELO rating of player $B$\n- $E_A$ is expected score of player $A$: \n$$\nE_A := P(A \\text{ wins}) + \\frac12 P(A \\text{ draws})\n$$\n- $E_A$ modelled by logistic function\n$$\nE_A := \\frac{1}{1+ 10^{(R_B-R_A)/400} }\n$$\n- Example: Beginner is rated $1000$, International Master is rated $2400$\n$$\nR_{\\rm Begin} = 1000, \\quad  R_{\\rm IM}=2400 , \\quad E_{\\rm Begin} = \\frac{1}{1 + 10^{1400/400}} = 0.00031612779\n$$\n\n\n\n\n## Characterization of pmf and pdf {.smaller}\n\n\n::: Theorem\n\nLet $f \\colon \\mathbb{R} \\to \\mathbb{R}$. Then $f$ is pmf or pdf of a RV $X$ iff\n\n1. $f(x) \\geq 0$ for all $x \\in \\mathbb{R}$\n2. $\\sum_{x=-\\infty}^\\infty f(x) = 1 \\,\\,\\,$ (pmf) $\\quad$ or $\\quad$ $\\int_{-\\infty}^\\infty f(x) \\, dx = 1\\,\\,\\,$ (pdf)\n\n:::\n\nIn the above setting: \n\n- The RV $X$ has **distribution**\n$$\nP(X = x) = f(x)  \\,\\,\\, \\text{ (pmf) } \\quad  \\text{ or }  \\quad\nP(a \\leq X \\leq b) = \\int_a^b f(t) \\, dt   \\,\\,\\, \\text{ (pdf)}\n$$\n\n- The symbol $X \\sim f$ denotes that $X$ has distribution $f$\n\n\n\n\n\n## Summary - Random Variables {.smaller}\n\n- Suppose $X \\colon \\Omega \\to \\mathbb{R}$ is RV\n\n- **Cumulative Density Function (cdf)**:\n$F_X(x) := P(X \\leq x)$\n\n\n| Discrete RV                       |        Continuous RV         |\n|--------------                     |----------------              |\n| $F_X$ has **jumps**               |  $F_X$ is **continuous**     |\n|**Probability Mass Function (pmf)**|**Probability Density Function (pdf)**|\n| $f_X(x) := P(X=x)$                | $f_X(x) := F_X'(x)$          |  \n| $f_X \\geq 0$                      |  $f_X \\geq 0$                |\n|$\\sum_{x=-\\infty}^\\infty f_X(x) = 1$| $\\int_{-\\infty}^\\infty f_X(x) \\, dx = 1$ |\n| $F_X (x) = \\sum_{k=-\\infty}^x f_X(k)$| $F_X (x) = \\int_{-\\infty}^x f_X(t) \\, dt$\n| $P(a \\leq X \\leq b) = \\sum_{k = a}^{b} f_X(k)$ | $P(a \\leq X \\leq b) = \\int_a^b f_X(t) \\, dt$ |\n\n\n\n\n\n\n\n# Part 4: <br>Moment generating functions {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n::: footer\n\n<div color=\"#cc0164\">  </div>\n\n:::\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\n- $X \\colon \\Omega \\to \\mathbb{R}$ random variable and $g \\colon \\mathbb{R} \\to \\mathbb{R}$ function\n- Then $Y:=g(X) \\colon \\Omega \\to \\mathbb{R}$ is random variable\n- For $A \\subset \\mathbb{R}$ we define the pre-image \n$$\ng^{-1}(A) := \\{  x \\in \\mathbb{R} \\colon g(x) \\in A \\}\n$$\n- For $A=\\{y\\}$ single element set we denote\n$$\ng^{-1}(\\{y\\}) = g^{-1}(y) =  \\{  x \\in \\mathbb{R} \\colon g(x) = y \\}\n$$\n- The distribution of $Y$ is\n$$\nP(Y \\in A) = P(g(X) \\in A ) = P(X \\in g^{-1}(A))\n$$\n\n\n\n## Functions of Random Variables {.smaller}\n\n**Question**: What is the relationship between $f_X$ and $f_Y$?\n\n- **$X$ discrete**: Then $Y$ is discrete and\n$$\nf_Y (y) = P(Y = y) = \\sum_{x \\in g^{-1}(y)} P(X=x) = \\sum_{x \\in g^{-1}(y)} f_X(x) \n$$\n\n- **$X$ and $Y$ continuous**: Then\n\\begin{align*}\nF_Y(y) & = P(Y \\leq y) = P(g(X) \\leq y) \\\\\n       & = P(\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}   ) = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt\n\\end{align*}\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\n**Issue**: The below set may be tricky to compute\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}\n$$\n\nHowever it can be easily computed if $g$ is **strictly monotone**:\n\n- **g strictly increasing**: Meaning that \n$$\nx_1 < x_2 \\quad \\implies \\quad g(x_1) < g(x_2)\n$$\n\n- **g strictly decreasing**: Meaning that \n$$\nx_1 < x_2 \\quad \\implies \\quad g(x_1) > g(x_2)\n$$\n\n- In both cases $g$ is **invertible**\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\nLet $g$ be **strictly increasing**:\n\n- Then\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\} = \\{ x \\in \\mathbb{R} \\colon x \\leq g^{-1}(y) \\}\n$$\n\n- Therefore\n\\begin{align*}\nF_Y(y) & = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt \n         = \\int_{\\{ x \\in \\mathbb{R} \\colon x \\leq g^{-1}(y) \\}} f_X(t) \\, dt  \\\\\n       & = \\int_{-\\infty}^{g^{-1}(y)} f_X(t) \\, dt \n         = F_X(g^{-1}(y)) \n\\end{align*}\n\n\n\n\n\n## Functions of Random Variables {.smaller}\n\nLet $g$ be **strictly decreasing**:\n\n- Then\n$$\n\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\} = \\{ x \\in \\mathbb{R} \\colon x \\geq g^{-1}(y) \\}\n$$\n\n- Therefore\n\\begin{align*}\nF_Y(y) & = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt \n         = \\int_{\\{ x \\in \\mathbb{R} \\colon x \\geq g^{-1}(y) \\}} f_X(t) \\, dt  \\\\\n       & = \\int_{g^{-1}(y)}^{\\infty} f_X(t) \\, dt \n         = 1 - \\int_{-\\infty}^{g^{-1}(y)}f_X(t) \\, dt  \\\\\n       & = 1 - F_X(g^{-1}(y)) \n\\end{align*}\n\n\n\n\n\n## Summary - Functions of Random Variables {.smaller}\n\n\n- **$X$ discrete**: Then $Y$ is discrete and\n$$\nf_Y (y) = \\sum_{x \\in g^{-1}(y)} f_X(x) \n$$\n\n- **$X$ and $Y$ continuous**: Then\n$$\nF_Y(y) = \\int_{\\{ x \\in \\mathbb{R} \\colon g(x) \\leq y \\}} f_X(t) \\, dt\n$$\n\n- **$X$ and $Y$ continuous** and\n    * **$g$ strictly increasing**: $F_Y(y) = F_X(g^{-1}(y))$\n    * **$g$ strictly decreasing**: $F_Y(y) = 1 - F_X(g^{-1}(y))$\n\n\n\n\n\n## Expected Value {.smaller}\n\nExpected value is the **average** value of a random variable\n\n\n::: Definition\n\n$X$ rv and $g \\colon \\mathbb{R} \\to \\mathbb{R}$ function. The **expected value** or **mean**\nof $g(X)$ is $\\Expect [g(X)]$\n\n- If $X$ discrete\n$$\n\\Expect [g(X)]:= \\sum_{x \\in \\mathbb{R}} g(x) f_X(x) = \\sum_{x \\in \\mathbb{R}} g(x) P(X = x)\n$$\n\n- If $X$ continuous\n$$\n\\Expect [g(X)]:= \\int_{-\\infty}^{\\infty} g(x) f_X(x) \\, dx\n$$\n\n:::\n\n\n\n\n\n## Expected Value  {.smaller}\n### Properties\n\n\nIn particular we have^[These follow by taking $g(x)=x$ in previous definitions, so that $g(X) = X$]\n\n\n- If $X$ discrete\n$$\n\\Expect [X] = \\sum_{x \\in \\mathbb{R}} x f_X(x) = \\sum_{x \\in \\mathbb{R}} x P(X = x)\n$$\n\n- If $X$ continuous\n$$\n\\Expect [X] = \\int_{-\\infty}^{\\infty} x f_X(x) \\, dx\n$$\n\n\n\n\n\n\n\n\n## Expected Value  {.smaller}\n### Properties\n\n::: Theorem\n\n$X$ rv and $a,b \\in \\mathbb{R}$. The expected value is **linear**\n\\begin{equation} \\tag{1}\n\\Expect [aX + b] = a\\Expect [X] + b\n\\end{equation}\nIn particular the expected value of a constant is the constant itself:\n\\begin{equation} \\tag{2}\n\\Expect [b] = b\n\\end{equation}\n:::\n\n\n\n\n\n## Expected Value  {.smaller}\n### Proof of Theorem\n\n\n- Note that (2) follows from (1) by setting $a=0$\n\n- We now show (1). Suppose $X$ is continuous and set $g(x):=ax+b$. Then \n\\begin{align*}\n\\Expect [aX + b] & = \\Expect [g(X)] = \\int_{\\R} g(x) f_X(x) \\, dx \\\\\n& = \\int_{\\R} (ax + b) f_X(x) \\, dx  \\\\\n& = a\\int_{\\R} x f_X(x) \\, dx  + b\\int_{\\R} f_X(x) \\, dx  \\\\\n& = a \\Expect[X] + b\n\\end{align*}\n\n- If $X$ is discrete just replace series with integrals in the above argument\n\n\n\n\n\n\n\n## Expected Value  {.smaller}\n### Further Properties\n\nBelow are further properties of $\\Expect$, which we do not prove\n\n::: Theorem\n\nSuppose $X$ and $Y$ are rv. The expected value is:\n\n- **Monotone**: $X \\leq Y  \\quad \\implies \\quad \\Expect [X] \\leq  \\Expect [Y]$\n\n- **Non-degenerate**: $\\Expect[|X|] = 0 \\quad \\implies \\quad X = 0$\n\n- $X=Y \\quad \\implies \\quad \\Expect[X]=\\Expect[Y]$ \n\n:::\n\n\n\n\n\n\n## Variance {.smaller}\n\nVariance measures how much a rv $X$ deviates from $\\Expect[X]$\n\n::: Definition\n### Variance\n\nThe **variance** of a random variable $X$ is\n$$\n\\Var[X]:= \\Expect[(X - \\Expect[X])^2]\n$$\n\n:::\n\n\n**Note**:\n\n- $\\Var [X] = 0 \\quad \\implies \\quad (X - \\Expect[X])^2 = 0 \\quad  \\implies \\quad X = \\Expect [X]$\n- If $\\Var[X]$ is small then $X$ is close to $\\Expect[X]$\n- If $\\Var[X]$ is large then $X$ is very variable\n\n\n\n\n## Variance {.smaller}\n### Equivalent formula\n\n::: Proposition\n$$\n\\Var[X] =  \\Expect[X^2] - \\Expect[X]^2\n$$\n:::\n\n**Proof:**\n\\begin{align*}\n\\Var[X] & = \\Expect[(X - \\Expect[X])^2] \\\\\n        & = \\Expect[X^2 - 2 X \\Expect[X] +  \\Expect[X]^2] \\\\  \n        & = \\Expect[X^2] - \\Expect[2 X \\Expect[X]]  + \\Expect[ \\Expect[X]^2] \\\\\n        & = \\Expect[X^2] - 2 \\Expect[X]^2 + \\Expect[X]^2 \\\\\n        & = \\Expect[X^2] - \\Expect[X]^2\n\\end{align*}\n\n\n\n\n\n## Variance {.smaller}\n### Variance is quadratic\n\n::: Proposition\n\n$X$ rv and $a,b \\in \\R$. Then\n$$\n\\Var[a X + b] = a^2 \\Var[X] \n$$\n\n:::\n\n\n**Proof:** Using linearity of $\\Expect$ and the fact that $\\Expect[c]=c$ for constants:\n\\begin{align*}\n\\Var[a X + b] & = \\Expect[ (aX + b)^2 ] - \\Expect[ aX + b ]^2 \\\\\n              & = \\Expect[ a^2X^2 + b^2 + 2abX ] - ( a\\Expect[X]  + b)^2 \\\\\n              & = a^2 \\Expect[ X^2 ] + b^2 + 2ab \\Expect[X] - \n                  a^2 \\Expect[X]^2   - b^2 - 2ab \\Expect[X] \\\\\n              & = a^2 ( \\Expect[ X^2 ] - \\Expect[ X ]^2 ) = a^2 \\Var[X]\n\\end{align*}\n\n\n\n\n\n## Variance {.smaller}\n### How to compute the Variance\n\nWe have\n$$\n\\Var[X] =  \\Expect[X^2] - \\Expect[X]^2\n$$\n\n- $X$ discrete:\n$$\nE[X] = \\sum_{x \\in \\mathbb{R}} x f_X(x) \\,, \\qquad \nE[X^2] = \\sum_{x \\in \\mathbb{R}} x^2 f_X(x)\n$$\n\n\n- $X$ continuous:\n$$\nE[X] = \\int_{-\\infty}^\\infty x f_X(x) \\, dx \\,, \\qquad \nE[X^2] = \\int_{-\\infty}^\\infty x^2 f_X(x) \\, dx\n$$\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n\nThe **Gamma distribution** with parameters $\\alpha,\\beta>0$ is\n$$\nf(x) := \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\,, \\quad x > 0\n$$\nwhere $\\Gamma$ is the **Gamma function**\n$$\n\\Gamma(a) :=\\int_0^{\\infty} x^{a-1} e^{-x} \\, dx\n$$\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n**Properties of $\\Gamma$**:\n\n- The Gamma function coincides with the factorial on natural numbers\n$$\n\\Gamma(n)=(n-1)! \\,, \\quad \\forall \\, n \\in \\N\n$$\n\n- More in general\n$$\n\\Gamma(a)=(a-1)\\Gamma(a-1) \\,, \\quad \\forall \\, a > 0\n$$\n\n\n-  Definition of $\\Gamma$ implies normalization of the Gamma distribution:\n$$\n\\int_0^{\\infty} f(x) \\,dx = \n\\int_0^{\\infty} \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  \\, dx\n=  1 \n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Definition\n\n$X$ has Gamma distribution with parameters $\\alpha,\\beta$ if \n\n- the pdf of $X$ is\n$$\nf_X(x) = \n\\begin{cases} \n\\dfrac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  & \\text{ if } x > 0 \\\\\n0                                                                 & \\text{ if } x \\leq 0\n\\end{cases}\n$$\n\n- In this case we write $X \\sim \\Gamma(\\alpha,\\beta)$\n\n- $\\alpha$ is **shape** parameter\n\n- $\\beta$ is **rate** parameter\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Plot\n\nPlotting $\\Gamma(\\alpha,\\beta)$ for parameters $(2,1)$ and $(3,2)$\n\n```{r}\n# Set the shape and rate parameters for the first gamma distribution\nalpha1 <- 2\nbeta1 <- 1\n\n# Set the shape and rate parameters for the second gamma distribution\nalpha2 <- 3\nbeta2 <- 2\n\n# Generate values for the x-axis\nx_values <- seq(0, 15, by = 0.1)\n\n# Calculate the probability density function (PDF) values for each x for the first distribution\npdf_values1 <- dgamma(x_values, shape = alpha1, rate = beta1)\n\n# Calculate the probability density function (PDF) values for each x for the second distribution\npdf_values2 <- dgamma(x_values, shape = alpha2, rate = beta2)\n\n# Plot the gamma PDFs\nplot(x_values, \n    pdf_values1, \n    type = \"l\", \n    col = \"blue\", lwd = 2,\n    xlab = \"\", \n    ylab = \"\",\n    ylim = c(0, max(pdf_values1, pdf_values2) + 0.1))\n\nlines(x_values, \n      pdf_values2, \n      col = \"red\", \n      lwd = 2)\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"pdf\", side=2, line=2.5, cex=2)\n\n# Add a legend\nlegend(\"topright\", legend = c(paste(\"Gamma(\", alpha1, \",\", beta1, \")\", sep = \"\"),\n                              paste(\"Gamma(\", alpha2, \",\", beta2, \")\", sep = \"\")),\n       col = c(\"blue\", \"red\"), lty = 1, lwd = 2, cex = 1.5)\n\n```  \n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nLet $X \\sim \\Gamma(\\alpha,\\beta)$. We have:\n\\begin{align*}\n\\Expect [X] & = \\int_{-\\infty}^\\infty x f_X(x) \\, dx  \\\\\n            & = \\int_0^\\infty  x \\, \\frac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, dx \\\\\n            & = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nRecall previous calculation:\n$$\n\\Expect [X] = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx\n$$\nChange variable $y=\\beta x$ and recall definition of $\\Gamma$:\n\\begin{align*}\n \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx & = \n  \\int_0^\\infty  \\frac{1}{\\beta^{\\alpha}} (\\beta x)^{\\alpha} e^{-\\beta{x}} \\frac{1}{\\beta} \\, \\beta \\, dx \\\\\n  & = \\frac{1}{\\beta^{\\alpha+1}} \\int_0^\\infty  y^{\\alpha} e^{-y} \\, dy \\\\\n  & = \\frac{1}{\\beta^{\\alpha+1}} \\Gamma(\\alpha+1)\n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expected value\n\nTherefore\n\\begin{align*}\n\\Expect [X] & = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha} e^{-\\beta{x}}  \\, dx \\\\\n            & =  \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) } \\, \\frac{1}{\\beta^{\\alpha+1}} \\Gamma(\\alpha+1) \\\\\n            & = \\frac{\\Gamma(\\alpha+1)}{\\beta \\Gamma(\\alpha)}\n\\end{align*}\n\nRecalling that $\\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha)$:\n$$\n\\Expect [X] = \\frac{\\Gamma(\\alpha+1)}{\\beta \\Gamma(\\alpha)} = \\frac{\\alpha}{\\beta}\n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nWe want to compute\n$$\n\\Var[X] = \\Expect[X^2] - \\Expect[X]^2\n$$\n\n- We already have $\\Expect[X]$\n- Need to compute $\\Expect[X^2]$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nProceeding similarly we have:\n\n\\begin{align*}\n\\Expect[X^2] & = \\int_{-\\infty}^{\\infty} x^2 f_X(x) \\, dx \\\\\n             & = \\int_{0}^{\\infty}  x^2 \\, \\frac{ x^{\\alpha-1} \\beta^{\\alpha} e^{- \\beta x} }{ \\Gamma(\\alpha) } \\, dx \\\\\n             & =  \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty}  x^{\\alpha+1} e^{- \\beta x} \\, dx \n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nRecall previous calculation:\n$$\n\\Expect [X^2] =  \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty}  x^{\\alpha+1} e^{- \\beta x} \\, dx \n$$\nChange variable $y=\\beta x$ and recall definition of $\\Gamma$:\n\\begin{align*}\n \\int_0^\\infty  x^{\\alpha+1} e^{-\\beta{x}}  \\, dx & = \n  \\int_0^\\infty  \\frac{1}{\\beta^{\\alpha+1}} (\\beta x)^{\\alpha + 1} e^{-\\beta{x}} \\frac{1}{\\beta} \\, \\beta \\, dx \\\\\n  & = \\frac{1}{\\beta^{\\alpha+2}} \\int_0^\\infty  y^{\\alpha + 1 } e^{-y} \\, dy \\\\\n  & = \\frac{1}{\\beta^{\\alpha+2}} \\Gamma(\\alpha+2)\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nTherefore\n$$\n\\Expect [X^2]  = \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) }\n            \\int_0^\\infty  x^{\\alpha+1} e^{-\\beta{x}}  \\, dx \n             =  \\frac{ \\beta^{\\alpha} }{ \\Gamma(\\alpha) } \\, \\frac{1}{\\beta^{\\alpha+2}} \\Gamma(\\alpha+2) \n              = \\frac{\\Gamma(\\alpha+2)}{\\beta^2 \\Gamma(\\alpha)}\n$$\nNow use following formula twice $\\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha)$:\n$$\n\\Gamma(\\alpha+2)= (\\alpha + 1) \\Gamma(\\alpha + 1) = (\\alpha + 1) \\alpha \\Gamma(\\alpha)\n$$\nSubstituting we get\n$$\n\\Expect [X^2] = \\frac{\\Gamma(\\alpha+2)}{\\beta^2 \\Gamma(\\alpha)} = \\frac{(\\alpha+1) \\alpha}{\\beta^2}\n$$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nTherefore\n$$\n\\Expect [X] = \\frac{\\alpha}{\\beta}  \\quad \\qquad\n\\Expect [X^2] = \\frac{(\\alpha+1) \\alpha}{\\beta^2}\n$$\nand the variance is\n\\begin{align*}\n\\Var[X] & = \\Expect [X^2] - \\Expect [X]^2 \\\\\n        & = \\frac{(\\alpha+1) \\alpha}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\\n        & = \\frac{\\alpha}{\\beta^2}\n\\end{align*}\n\n\n\n\n\n\n\n\n## Moment generating function {.smaller}\n\n\n- We abbreviate **Moment generating function** with **MGF**\n\n- MGF is almost the Laplace transform of the probability density function\n\n- MGF provides a short-cut to calculating mean and variance\n\n- MGF gives a way of proving distributional results for sums of independent random variables\n\n\n\n\n## Moment generating function {.smaller}\n\n\n::: Definition\n\nThe **moment generating function** or **MGF** of a rv $X$ is\n$$\nM_X(t) := \\Expect [e^{tX}] \\,, \\quad \\forall \\, t \\in \\R\n$$\n\n:::\n\nIn particular we have:\n\n::: {.column width=\"48%\"}\n\n- $X$ discrete:\n$$\nM_X(t) = \\sum_{x \\in \\R} e^{tx} f_X(x)\n$$\n\n:::\n\n::: {.column width=\"48%\"}\n\n- $X$ continuous:\n$$\nM_X(t) = \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx\n$$\n\n:::\n\n\n\n\n## Moment generating function {.smaller}\n### Computing moments\n\n\n::: Theorem\n\nIf $X$ has MGF $M_X$ then\n$$\n\\Expect[X^n] = M_X^{(n)} (0)\n$$\nwhere we denote\n$$\nM_X^{(n)} (0) :=  \\frac{d^n}{dt^n} M_X^{(n)}(t) \\bigg|_{t=0}\n$$\n\n:::\n\nThe quantity $\\Expect[X^n]$ is called **$n$-th moment** of $X$\n \n\n\n\n## Moment generating function {.smaller}\n### Proof of Theorem\n\nSuppose $X$ continuous and that we can exchange derivative and integral:\n\\begin{align*}\n\\frac{d}{dt} M_X(t) & = \\frac{d}{dt} \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx \n                      = \\int_{-\\infty}^\\infty \\left( \\frac{d}{dt} e^{tx} \\right) f_X(x) \\, dx \\\\\n                    & = \\int_{-\\infty}^\\infty xe^{tx} f_X(x) \\, dx \n                      = \\Expect(Xe^{tX})\n\\end{align*}\nEvaluating at $t = 0$:\n$$\n\\frac{d}{dt} M_X(t) \\bigg|_{t = 0} = \\Expect(Xe^{0}) = \\Expect[X]\n$$\n\n\n\n\n## Moment generating function {.smaller}\n### Proof of Theorem\n\nProceeding by induction we obtain:\n$$\n\\frac{d^n}{dt^n} M_X(t) = \\Expect(X^n e^{tX})\n$$\nEvaluating at $t = 0$ yields the thesis:\n$$\n\\frac{d^n}{dt^n} M_X(t) \\bigg|_{t = 0} = \\Expect(X^n e^{0}) = \\Expect[X^n]\n$$\n\n\n\n\n\n\n## Moment generating function {.smaller}\n### Notation\n\nFor the first 3 derivatives we use special notations:\n\n$$\nM_X'(0) := M^{(1)}_X(0) = \\Expect[X] \n$$\n$$\nM_X''(0) := M^{(2)}_X(0) = \\Expect[X^2] \n$$\n$$\nM_X'''(0) := M^{(3)}_X(0) = \\Expect[X^3] \n$$\n\n\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Definition\n\n\n- The **normal distribution** with mean $\\mu$ and variance $\\sigma^2$ is\n$$\nf(x) := \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) \\,, \\quad x \\in \\R\n$$\n\n\n- $X$ has **normal distribution** with mean $\\mu$ and variance $\\sigma^2$ if $f_X = f$\n\n    * In this case we write $X \\sim N(\\mu,\\sigma^2)$\n\n- The **standard normal distribution** is denoted $N(0,1)$\n\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Plot\n\nPlotting $N(\\mu,\\sigma^2)$ for parameters $(0,1)$ and $(3,2)$\n\n```{r}\n# Set the shape and rate parameters for the first gamma distribution\nmu1 <- 0\nsigma1 <- 1\n\n# Set the shape and rate parameters for the second gamma distribution\nmu2 <- 3\nsigma2 <- 2\n\n# Generate values for the x-axis\nx_values <- seq(-7, 7, by = 0.1)\n\n# Calculate the probability density function (PDF) values for each x for the first distribution\npdf_values1 <- dnorm(x_values, mean = mu1, sd = sqrt(sigma1))\n\n# Calculate the probability density function (PDF) values for each x for the second distribution\npdf_values2 <- dnorm(x_values, mean = mu2, sd = sqrt(sigma2))\n\n# Plot the gamma PDFs\nplot(x_values, \n    pdf_values1, \n    type = \"l\", \n    col = \"blue\", lwd = 2,\n    xlab = \"\", \n    ylab = \"\",\n    ylim = c(0, max(pdf_values1, pdf_values2) + 0.1))\n\nlines(x_values, \n      pdf_values2, \n      col = \"red\", \n      lwd = 2)\n\nmtext(\"x\", side=1, line=3, cex=2)\nmtext(\"pdf\", side=2, line=2.5, cex=2)\n\n# Add a legend\nlegend(\"topright\", legend = c(paste(\"N(\", mu1, \",\", sigma1, \")\", sep = \"\"),\n                              paste(\"N(\", mu2, \",\", sigma2, \")\", sep = \"\")),\n       col = c(\"blue\", \"red\"), lty = 1, lwd = 2, cex = 1.5)\n\n``` \n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nThe equation for the normal pdf is\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\nBeing pdf, we must have $\\int f_X(x) \\, dx = 1$. This yields:\n\\begin{equation} \\tag{1}\n\\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{x^2}{2\\sigma^2} + \\frac{\\mu{x}}{\\sigma^2} \\right) \\, dx = \\exp \\left(\\frac{\\mu^2}{2\\sigma^2} \\right) \\sqrt{2\\pi} \\sigma\n\\end{equation}\n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nWe have\n\\begin{align*}\nM_X(t) & := \\Expect (e^{tX}) \n         = \\int_{-\\infty}^{\\infty} e^{tx} f_X(x) \\, dx  \\\\\n       & = \\int_{-\\infty}^{\\infty} e^{tx} \\frac{1}{\\sqrt{2\\pi}\\sigma} \n       \\exp \\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right) \\, dx \\\\\n       & = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\int_{-\\infty}^{\\infty} e^{tx} \n       \\exp \\left( -\\frac{x^2}{2\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2} + \\frac{x\\mu}{\\sigma^2} \\right) \\, dx \\\\     \n       & = \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2} \\right) \n       \\frac{1}{\\sqrt{2\\pi}\\sigma}\n       \\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx\n\\end{align*}\n\n\n\n## Example - Normal distribution {.smaller}\n### Moment generating function\n\nWe have shown\n\\begin{equation} \\tag{2}\nM_X(t) = \\exp\\left(-\\frac{\\mu^2}{2\\sigma^2} \\right) \n       \\frac{1}{\\sqrt{2\\pi}\\sigma}\n       \\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx\n\\end{equation}\nReplacing $\\mu$ by $(t\\sigma^2 + \\mu)$ in (1) we obtain\n\\begin{equation} \\tag{3}\n\\int_{-\\infty}^{\\infty} \\exp \\left(- \\frac{x^2}{2\\sigma^2} + \\frac{(t\\sigma^2+\\mu) x}{\\sigma^2} \\right) \\, dx \n= \\exp \\left( \\frac{(t\\sigma^2+\\mu)^2}{2\\sigma^2}  \\right) \\,  \n\\frac{1}{\\sqrt{2\\pi}\\sigma}\n\\end{equation}\nSubstituting (3) in (2) and simplifying we get\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Mean\n\nRecall the mgf\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nThe first derivative is\n$$\nM_X'(t) = (\\mu + \\sigma^2 t ) \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nTherefore the mean:\n$$\n\\Expect [X] = M_X'(0) = \\mu \n$$\n\n\n\n\n## Example - Normal distribution {.smaller}\n### Variance\n\nThe first derivative of mgf is\n$$\nM_X'(t) = (\\mu + \\sigma^2 t ) \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nThe second derivative is then\n$$\nM_X''(t) = \\sigma^2  \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right) +\n(\\mu + \\sigma^2 t )^2 \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nTherefore the second moment is:\n$$\n\\Expect [X^2] = M_X''(0) = \\sigma^2 + \\mu^2 \n$$\n\n\n\n## Example - Normal distribution {.smaller}\n### Variance\n\nWe have seen that:\n$$\n\\Expect[X] = \\mu   \\quad  \\qquad \\Expect [X^2] = \\sigma^2 + \\mu^2 \n$$\nTherefore the variance is:\n\\begin{align*}\n\\Var[X] & = \\Expect[X^2] - \\Expect[X]^2 \\\\\n        & =  \\sigma^2 + \\mu^2 - \\mu^2 \\\\\n        & = \\sigma^2 \n\\end{align*}\n\n\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nSuppose $X \\sim \\Gamma(\\alpha,\\beta)$. This means\n$$\nf_X(x) = \n\\begin{cases} \n\\dfrac{x^{\\alpha-1} e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)}  & \\text{ if } x > 0 \\\\\n0                                                                 & \\text{ if } x \\leq 0\n\\end{cases}\n$$\n\n- We have seen already that \n$$\n\\Expect[X] = \\frac{\\alpha}{\\beta} \\quad  \\qquad\n\\Var[X] =  \\frac{\\alpha}{\\beta^2}\n$$\n\n\n- We want to compute mgf $M_X$ to derive again $\\Expect[X]$ and $\\Var[X]$\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nWe compute\n\\begin{align*}\nM_X(t) & = \\Expect [e^{tX}] = \\int_{-\\infty}^\\infty e^{tx} f_X(x) \\, dx \\\\\n       & = \\int_0^{\\infty} e^{tx} \\, \\frac{x^{\\alpha-1}e^{-\\beta{x}} \\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, dx \\\\\n       & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx\n\\end{align*}\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nFrom the previous slide we have\n$$\nM_X(t) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx\n$$\nChange variable $y=(\\beta-t)x$ and recall the definition of $\\Gamma$:\n\\begin{align*}\n\\int_0^{\\infty} x^{\\alpha-1} e^{-(\\beta-t)x} \\, dx & =\n\\int_0^{\\infty} \\frac{1}{(\\beta-t)^{\\alpha-1}} [(\\beta-t)x]^{\\alpha-1} e^{-(\\beta-t)x}  \\frac{1}{(\\beta-t)} (\\beta - t) \\, dx   \\\\\n& = \\frac{1}{(\\beta-t)^{\\alpha}} \\int_0^{\\infty} y^{\\alpha-1} e^{-y}  \\, dy \\\\\n& = \\frac{1}{(\\beta-t)^{\\alpha}} \\Gamma(\\alpha)\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Moment generating function\n\nTherefore\n\\begin{align*}\nM_X(t) & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\int_0^{\\infty}x^{\\alpha-1}e^{-(\\beta-t)x} \\, dx \\\\\n       & = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\cdot \\frac{1}{(\\beta-t)^{\\alpha}} \\Gamma(\\alpha) \\\\\n       & = \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expectation\n\nFrom the mgf\n$$\nM_X(t) = \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n$$\nwe compute the first derivative:\n\\begin{align*}\nM_X'(t) & = \\frac{d}{dt} [\\beta^{\\alpha}(\\beta-t)^{-\\alpha}] \\\\\n        & = \\beta^{\\alpha}(-\\alpha)(\\beta-t)^{-\\alpha-1}(-1) \\\\\n        & = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Expectation\n\nFrom the first derivative\n$$\nM_X'(t) = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n$$\nwe compute the expectation\n\\begin{align*}\n\\Expect[X] & = M_X'(0) \\\\\n           & = \\alpha\\beta^{\\alpha}(\\beta)^{-\\alpha-1} \\\\\n           & =\\frac{\\alpha}{\\beta}  \n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the first derivative\n$$\nM_X'(t) = \\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}\n$$\nwe compute the second derivative\n\\begin{align*}\nM_X''(t) & = \\frac{d}{dt}[\\alpha\\beta^{\\alpha}(\\beta-t)^{-\\alpha-1}] \\\\\n         & = \\alpha\\beta^{\\alpha}(-\\alpha-1)(\\beta-t)^{-\\alpha-2}(-1)\\\\\n         & = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta-t)^{-\\alpha-2}\n\\end{align*}\n\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the second derivative\n$$\nM_X''(t) = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta-t)^{-\\alpha-2}\n$$\nwe compute the second moment:\n\\begin{align*}\n\\Expect[X^2] & = M_X''(0) \\\\\n             & = \\alpha(\\alpha+1)\\beta^{\\alpha}(\\beta)^{-\\alpha-2} \\\\\n             & = \\frac{\\alpha(\\alpha + 1)}{\\beta^2}\n\\end{align*}\n\n\n\n## Example - Gamma distribution {.smaller}\n### Variance\n\nFrom the first and second moments:\n$$\n\\Expect[X] = \\frac{\\alpha}{\\beta} \\qquad \\qquad\n\\Expect[X^2] = \\frac{\\alpha(\\alpha + 1)}{\\beta^2}\n$$\nwe can compute the variance\n\\begin{align*}\n\\Var[X] & = \\Expect[X^2] - \\Expect[X]^2 \\\\\n        & = \\frac{\\alpha(\\alpha + 1)}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\\n        & = \\frac{\\alpha}{\\beta^2}\n\\end{align*}\n\n\n\n\n## Moment generating function {.smaller}\n### The mgf characterizes a distribution\n\n\n::: Theorem\n\nLet $X$ and $Y$ be random variables with mgfs $M_X$ and $M_Y$ respectively. Assume there exists $\\e>0$ such that\n$$\nM_X(t) = M_Y(t) \\,, \\quad \\forall \\, t \\in (-\\e , \\e)\n$$\nThen $X$ and $Y$ have the same cdf\n$$\nF_X(u) = F_Y(u) \\,, \\quad \\forall \\, x \\in \\R\n$$\n\n:::\n\nIn other words: $\\qquad$ **same mgf** $\\quad \\implies \\quad$ **same distribution**\n\n\n\n\n## Example  {.smaller}\n\n- Suppose $X$ is a random variable such that\n$$\nM_X(t) = \\exp \\left( \\mu t + \\frac{t^2 \\sigma^2}{2}  \\right)\n$$\nAs the above is the mgf of a normal distribution, by the previous Theorem we infer $X \\sim N(\\mu,\\sigma^2)$\n\n- Suppose $Y$ is a random variable such that\n$$\nM_Y(t) =  \\frac{\\beta^{\\alpha}}{(\\beta-t)^{\\alpha}}\n$$\nAs the above is the mgf of a Gamma distribution, by the previous Theorem we infer $Y \\sim \\Gamma(\\alpha,\\beta)$\n\n\n\n\n\n\n\n\n\n\n\n## References\n\n\n\n\n\n\n::: {.content-hidden}\n\n\n\n\n# Thank you! {background-color=\"#cc0164\" visibility=\"uncounted\"}\n\n\n::: footer\n\n<div color=\"#cc0164\"> </div>\n\n:::\n\n\n\n\n\n## Real data example\n### How does real data look like?\n\n::: {style=\"font-size: 0.8em\"}\nDataset with $33$ entries for Stock and Gold price pairs\n:::\n\n::: {style=\"font-size: 0.6em\"}\n\n```{r}\n#| echo: false\n#| layout-ncol: 3\n#| tbl-cap: \"Dataset with 33 entries for Stock and Gold price pairs\"\n\n# Read dataset\ndata <- read.table(\"datasets/L3eg1data.txt\")\n\n#Add column labels to data\ncolnames(data) <- c(\"Stock Price\",\"Gold Price\")\n\n# Output markdown table from data\n\nknitr::kable(data[1:11,], row.names = TRUE)\n\nknitr::kable(data[12:22,], row.names = TRUE)\n\nknitr::kable(data[23:33,], row.names = TRUE)\n\n```\n\n:::\n\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"katex","slide-level":2,"to":"revealjs","filters":["custom-numbered-blocks"],"number-sections":false,"toc":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\">\n"}],"from":"markdown+emoji","output-file":"lecture_1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.542","auto-stretch":true,"bibliography":["../teaching.bib","teaching.bib"],"csl":"../elsevier-with-titles.csl","search":false,"favicon":"images/favicon.png","custom-numbered-blocks":{"groups":{"thmlike":{"colors":["f4cce0","db4d92"],"boxstyle":"foldbox.simple","collapse":false,"listin":["mathstuff"],"numbered":false},"todos":"default"},"classes":{"Theorem":{"group":"thmlike"},"Corollary":{"group":"thmlike"},"Conjecture":{"group":"thmlike"},"Proposition":{"group":"thmlike"},"Lemma":{"group":"thmlike"},"Question":{"group":"thmlike"},"Axiom":{"group":"thmlike"},"Important":{"group":"thmlike","numbered":false},"Warning":{"group":"thmlike","numbered":false},"Problem":{"group":"thmlike","numbered":false},"Definition":{"group":"thmlike","colors":["c6e6ed","1995ad"]},"Notation":{"group":"thmlike","colors":["c6e6ed","1995ad"]},"Remark":{"group":"thmlike","colors":["bce5de","21aa93"]},"Example":{"group":"thmlike","colors":["bce5de","21aa93"]},"Proof":{"group":"thmlike","colors":["f1f1f2","c0c0c1"],"numbered":false},"TODO":{"label":"To do","colors":["e7b1b4","8c3236"],"group":"todos","listin":["stilltodo"]},"DONE":{"label":"Done","colors":["cce7b1","86b754"],"group":"todos"}}},"footer":"[Homepage](/index.html) &nbsp;&nbsp;&nbsp;&nbsp; [License](/license.html) &nbsp;&nbsp;&nbsp;&nbsp; [Contact](https://www.silviofanzon.com/contact)","theme":["default","custom-theme-slides.scss"],"transition":"none","backgroundTransition":"fade","previewLinks":"auto","slideNumber":true,"touch":true,"callout-icon":false,"history":true,"keyboard":true,"title":"Statistical Models","subtitle":"Lecture 1","author":[{"name":"Dr. Silvio Fanzon","id":"sf","email":"S.Fanzon@hull.ac.uk","url":"https://www.silviofanzon.com","affiliations":"University of Hull"},{"name":"Dr. John Fry","id":"jf","email":"J.M.Fry@hull.ac.uk","url":"https://www.hull.ac.uk/staff-directory/john-fry","affiliations":"University of Hull"}]}}},"projectFormats":["html"]}