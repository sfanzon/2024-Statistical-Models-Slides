---
title: "Statistical Models"
subtitle: "Lecture 7"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 7: <br>The chi-squared test{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 7

1. Overview
2. The chi-squared goodness of fit test
3. Computational Example 1
4. The chi-squared test of independence/no association
5. Computational Example 2








# Part 1: <br>Overview {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Problem statement {.smaller}


**Data:** in the form of **numerical counts**

**Test:** difference between observed counts and predictions of theoretical model

**Example**: Blood counts

- Suppose to have counts of blood type for some people

    | A  |  B  |  AB  |  O  |
    |:--:|:---:|:----:|:---:|
    |2162| 738 | 228  |2876 |

- We want to compare the above to the probability model

    | A  |  B  |  AB  |  O  |
    |:--:|:---:|:----:|:---:|
    |1/3 | 1/8 |  1/24| 1/2 |
    




## Counts with multiple factors {.smaller}

**Example:** Relative performance of Man Utd managers

- Each football manager has Win, Draw and Loss count


| Manager | Played  |     Won |  Drawn  | Lost    |
|:-------:|:-------:|:-------:|:-------:|:-------:|
|  Moyes  |    51   |   27    | 9       |   15    |
|Van Gaal | 103     |   54    |  25     |   24    |
| Mourinho|  144    |   84    |  32     |   28    |
|Solskjaer|  168    |    91   |   37    |   40    |


**Questions:**

- Is the number of Wins, Draws and Losses uniformly distributed?
- Are there differences between the performances of each manager?



## Chi-squared test {.smaller}

- Hypothesis testing for counts can be done using chi-squared test

- Chi-squared test is simple to use for real-world project datasets (e.g. dissertations)

- Potentially applicable to a whole range of different models

- Easy to compute by hand/software

- Motivates the more advanced study of contingency tables




# Part 2: <br>Chi-squared <br> goodness of fit test {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Chi-squared goodness of fit test {.smaller}

**Categorical Data:** 

- Finite number of possible categories or types
- Observations can only belong to one category
- $O_i$ refers to observed count of category $i$

| Type $1$| Type $2$ |  $\ldots$ | Type $n$ |
|:-------:|:--------:|:---------:|:--------:|
|$O_1$    |   $O_2$  | $\ldots$  |    $O_n$ |

- $E_i$ refers to **expected count** of category $i$

| Type $1$| Type $2$ |  $\ldots$ | Type $n$ |
|:-------:|:--------:|:---------:|:--------:|
|$E_1$    |   $E_2$  | $\ldots$  |    $E_n$ |
 


## Chi-squared goodness of fit test {.smaller}

**Goal:** Compare expected counts $E_i$ with observed counts $O_i$

**Method:**

- Start with null hypothesis that theoretical model is correct
- Look for evidence against the null hypothesis
- Null hypothesis is wrong
    * if distance between observed counts and expected counts is large
    * For example if
    $$
    (O_i - E_i)^2 \geq c
    $$
    for some chosen constant $c$



## Chi-squared statistic {.smaller}

::: Definition

Global distance between observed and expected counts is defined as
$$
\chi^2 := \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}
$$
The above is called **chi-squared statistic**

:::

**Note:** We have that 
$$
\chi^2 = 0 \qquad \iff \qquad O_i = E_i  \,\,\,\, \text{ for all } \,\,\,\, i = 1 , \, \ldots , \, n
$$


## Chi-squared statistic {.smaller}

**Remarks:**

- We expect small differences between $O_i$ and $E_i$ 
    * Therefore $\chi^2 > 0$

- However $O_i$ should not be too far away from $E_i$
    * Therefore $\chi^2$ should not be too large 

- The above imply that tests on $\chi^2$ should be one-sided





## Multinomial distribution {.smaller}

**Goal:** Find a reasonable model for counts. We use the **multinomial distribution**

**Multinomial distribution:** is a model for the following experiment
  
- The experiment consists of $m$ independent trials 
- Each trial results in one of $n$ distinct possible outcomes
- The probability of the $i$-th outcome is $p_i$ on every trial
- $X_i$ is the count of the number of times the $i$-th outcome occurred in the m trials
- For $n = 2$ this is a binomial experiment
    * Each trial has $n = 2$ possible outcomes
    * $X_1$ counts the number of *successes* 
    * $X_2 = m âˆ’ X_1$ counts the number of *failures* in $m$ trials



## Multinomial distribution {.smaller}


::: Definition

Let $m,n \in \N$ and $p_1, \ldots, p_n$ numbers such that
$$
0 \leq p_i \leq 1 \,, \qquad \quad 
\sum_{i=1}^n p_i = 1
$$
The random vector $\XX = (X_1, \ldots, X_n)$ has **multinomial distribution** with $m$ trials and cell probabilities $p_1,\ldots,p_n$ if joint pmf if
$$
f (x_1, \ldots , x_n) = \frac{m!}{x_1 ! \cdot \ldots \cdot x_n !} \ p_1^{x_1} \cdot \ldots \cdot p_n^{x_n}  \,, \qquad \forall \, x_i \in \N  \, \st \sum_{i=1}^n x_i = m
$$
We denote $\XX \sim \multinomial(m,p_1,\ldots,p_n)$

:::



## Multinomial distribution {.smaller}
### Properties

Suppose that $\XX \sim \multinomial(m,p_1,\ldots,p_n)$

- Then $X_i$ is binomial with $m$ trials and probability $p_i$
- We write $X_i \sim \binomial(m,p_i)$ and the pmf is
$$
f(x_i) = P(X = x_i) = \frac{m!}{x_i! \cdot (1-x_i)!} \, p_i^{x_i} (1-p_i)^{x_i} 
\qquad \forall \, x_i = 0 , \ldots , m 
$$
- We have
$$
\Expect[X_i] = m p_i \qquad \qquad 
\Var[X_i] = m p_i (1-p_i)
$$



## Counts with Multinomial distribution {.smaller}

- $O_i$ refers to **observed count** of category $i$


- $E_i$ refers to **expected count** of category $i$

- We suppose that Type $i$ is observed with probability $p_i$ and
$$
0 \leq p_i \leq 1 \,, \qquad \quad p_1 + \ldots + p_n = 1
$$

- If there is a total of $m$ observation we have that
$$
(O_1, \ldots, O_n) \sim \multinomial (m, p_1, \ldots, p_n)
$$




## Counts with Multinomial distribution {.smaller}

- Therefore the observed count is
$$
O_i \sim \binomial (m,p_i)
$$

- The expected count $E_i$ is hence
$$
E_i = \Expect[ O_i ] = m p_i
$$

- The chi-squared statistics becomes
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i} 
       = \sum_{i=1}^n \frac{( O_i - m p_i )^2}{ m p_i }
$$


- **Question**: What is the distribution of $\chi^2 \,$?




## Distribution of chi-squared statistic {.smaller}

::: Theorem

Suppose the counts $(O_1, \ldots, O_n) \sim \multinomial (m,p_1, \ldots, p_n)$. Then
$$
\chi^2 = \sum_{i=1}^n \frac{( O_i - m p_i )^2}{ m p_i } \ \stackrel{{\rm d}}{\longrightarrow} \ \chi_{n-1}^2
$$
when $m \to \infty$, where the convergence is in distribution
:::

- Hence the distribution of $\chi^2$ is approximately $\chi_{n-1}^2$ when $m$ is large

- The above Theorem is due to Karl Pearson in 1900 paper [link](https://www.tandfonline.com/doi/abs/10.1080/14786440009463897)

- Proof is difficult. Seven different proofs are presented in this paper [link](https://arxiv.org/abs/1808.09171)



## Distribution of chi-squared statistic {.smaller}
### Heuristic proof of Theorem


- Since $O_i \sim \binomial(m, p_i)$, the Central 
Limit Theorem implies
$$
 \frac{O_i - m p_i }{ \sqrt{m p_i(1 - p_i) } } \
 \stackrel{{\rm d}}{\longrightarrow} \ N(0,1)
$$
as $m \to \infty$

- In particular, since $(1-p_i)$ in constant, we have
$$
 \frac{O_i - m p_i }{ \sqrt{m p_i } } \ \approx \ \frac{O_i - m p_i }{ \sqrt{m p_i(1 - p_i) } } \ \approx \ N(0,1) 
$$




## Distribution of chi-squared statistic {.smaller}
### Heuristic proof of Theorem

- Squaring the above we get
$$
 \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx   \ N(0,1)^2 = \chi_1^2  
$$


- If the above random variables were pairwise independent, we would obtain
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx  \ \sum_{i=1}^n \chi_1^2 = \chi_n^2  
$$



## Distribution of chi-squared statistic {.smaller}
### Heuristic proof of Theorem

- However the $O_i$ are not independent

- This is because $O_i$ is marginal of multinomial distribution

- In particular the linear constraint 
$$
O_1 + \ldots + O_n = m
$$
has to hold, which leads to dependence

- A rather technical proof is needed to prove the actual thesis that
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx \ \chi_{n-1}^2  
$$



## Distribution of chi-squared statistic {.smaller}
### Further notes


- The approximation given in the Theorem, i.e.,
$$
\chi^2 = \sum_{i=1}^n \frac{(O_i - m p_i)^2 }{ m p_i } \ \approx \ \chi_{n-1}^2  
$$
is only good if it holds 


## Monte Carlo simulation: Example{.smaller}
### Approximating $\pi$ 


- We can approximate $\pi$ via Monte Carlo simulation

- Idea: 
    * Throw random points inside unit square
    * Count proportion of points falling inside unit circle
    * Such proportion approximates the area of the circle
    * Area of unit circle is $\pi$


## Monte Carlo simulation: Example{.smaller}
### Approximating $\pi$ 

In practice, we do the following:

- Draw $x_1, \ldots, x_N, y_1, \ldots, y_N$ from ${\rm Uniform(-1,1)}$
- Draw $y_1, \ldots, y_N$ from ${\rm Uniform(-1,1)}$
- Count the number of points $(x_i,y_i)$ falling inside circle of radius $1$
- These are points satisfying condition
$$
x_i^2 + y_i^2 \leq 1
$$
- Can estimate area of circle with proportion of points falling inside circle:
$$
Area \ = \ \pi \ \approx \frac{\text{Number of points } (x_i,y_i) \text{ inside circle}}{N}
$$





## Monte {.smaller}
### Plot of 1000 random points uniformly distributed in unit square

```{r}
#| fig-asp: 1

R <- 1
niters <- 1000
total <- 0
in_circle <- 0

# Function to plot the points
plot_points <- function(x, y, R) {
  plot(x, y, col = ifelse(x^2 + y^2 <= R^2, "blue", "red"), pch = 16, xlab = "X", ylab = "Y", asp = 1, frame.plot = TRUE, xlim = c(-1,1), ylim = c(-1,1) )
  points(0, 0, col = "green", pch = 3)  # Plotting the center of the circle
  legend("bottomright", legend = c("Inside Circle", "Outside Circle"), col = c("blue", "red"), pch = 16, cex = 2)
  circle <- seq(0, 2 * pi, length.out = 100)
  lines(R * cos(circle), R * sin(circle))  # Plotting the circle
}

x <- runif(niters, -R, R)
y <- runif(niters, -R, R)
plot_points(x, y, R)

```



## Monte carlo {.smaller}
### Implementation in R

```r
niters <- 10000
total <- 0
in_circle <- 0

for (j in 1:10) {
  for (i in 1:niters) {
    x <- runif(1, -1, 1); y <- runif(1, -1, 1);
    if (x^2 + y^2 <= 1) {
      in_circle <- in_circle + 1
    }
    total <- total + 1
  }
  
  area <- in_circle / total
  cat(sprintf("After %8d iterations area is %.08f, error is %.08f\n",
     (j * niters), area, abs(area - pi)))
}

```




## Monte carlo {.smaller}
### Implementation in R: Output

```{r}

R <- 1
niters <- 10000
total <- 0
in_circle <- 0

for (j in 1:10) {
  for (i in 1:niters) {
    x <- runif(1, -R, R)
    y <- runif(1, -R, R)
    if (x^2 + y^2 <= R^2) {
      in_circle <- in_circle + 1
    }
    total <- total + 1
  }
  
  area <- (2 * R)^2 * in_circle / total
  expected <- pi * R^2
   cat(sprintf("After %8d iterations area is %.08f, error is %.08f\n", (j * niters), area, abs(area - expected)))
}

```

