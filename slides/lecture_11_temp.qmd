---
title: "Statistical Models"
subtitle: "Lecture 11"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Part 2: <br>Autocorrelation {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Autocorrelation {.smaller}

- The general linear regression model is 

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- Consider Assumption 5
    * **Independence:** Errors $\e_1, \ldots, \e_n$ are independent and thus uncorrelated
    $$
    \Cor(\e_i , \e_j) = 0 \qquad \forall \,\, i \neq j
    $$

- **Autocorrelation:** The violation of Assumption 5

$$
\Cor(\e_i , \e_j) = 0 \qquad \text{ for some } \,\, i \neq j
$$




## Why is independence important? {.smaller}

- Recall the methods to assess linear models 

    * Coefficient $R^2$
    * $t$-tests for parameters significance
    * $F$-test for model selection

- The above methods **rely heavily** on independence



## Why is independence important? {.smaller}

- Once again let us consider the likelihood calculation
    \begin{align*}
    L & = f(y_1, \ldots, y_n) =  \prod_{i=1}^n  f_{Y_i} (y_i)  
         \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{\sum_{i=1}^n(y_i- \hat y_i)^2}{2\sigma^2}      \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
    \end{align*}


- The second equality is only possible thanks to independence of 

$$
Y_1 , \ldots, Y_n
$$



## Why is independence important? {.smaller}

- If we have **autocorrelation** then

$$
\Cor (\e_i,\e_j) \neq 0 \quad \text{ for some } \, i \neq j
$$

- In particualar we would have

$$
\e_i \, \text{ and } \, \e_j \, \text{ dependent } \quad \implies \quad Y_i \, \text{ and } \, Y_j \, \text{ dependent }
$$


- Therefore the calculation in previous slide breaks down

$$
L \neq \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$



## Why is independence important? {.smaller}

- In this case $\hat \beta$ does no longer maximize the likelihood!

- As already seen, this implies that 

$$
\ese (\beta_j) \,\, \text{ is unreliable}
$$

- **Without independence the regression maths does not work!**
    * t-tests for significance of $\beta_j$
    * confidence intervals for $\beta_j$
    * $F$-tests for model selection
    * They all break down and become unreliable!




## Causes of Autocorrelation {.smaller}
### Time-series data

- Autocorrelation means that

$$
\Cor (\e_i,\e_j) \neq 0 \quad \text{ for some } \, i \neq j
$$

- Autocorrelation if often unavoidable

- Typically associated with **time series data**
    * Observations ordered wrt time or space are usually correlated
    * This is because observations taken close together may take similar values


## Causes of Autocorrelation {.smaller}
### Financial data

- Autocorrelation is especially likely for datasets in 
    * Accounting
    * Finance
    * Economics

- Autocorrelation is likely if the data have been recorded over time
    * E.g. daily, weekly, monthly, quarterly, yearly

- Example: Datasetet on *Stock prices* and *Gold prices*
    * General linear regression model assumes uncorrelated errors
    * Not realistic to assume that price observations for say 2020 and 2021 would be independent


## Causes of Autocorrelation  {.smaller}
### Inertia

- Economic time series tend to exhibit **cyclical behaviour**

- Examples include GNP, price indices, production figures, employment statistics etc.

- Since these series tend to be quite slow moving
    * Effect of inertia is that successive observations are highly correlated

**This is an extremely common phenomenon in financial and economic time series**


## Causes of Autocorrelation  {.smaller}
### Cobweb Phenomenon

- Characteristic of industries in which a large amount of time passes between
    * the decision to produce something 
    * and its arrival on the market

- Cobweb phenomenon is common with agricultural commodities

- Economic agents (e.g. farmers) decide
    * how many goods to supply to the market
    * based on previous year price 


## Causes of Autocorrelation  {.smaller}
### Cobweb Phenomenon

- **Example:** the amount of crops farmers supply to the market at time $t$ might be

\begin{equation} \tag{3}
{\rm Supply}_t = \beta_1 + \beta_2 \, {\rm Price}_{t-1} + \e_t
\end{equation}

- Errors $\e_t$ in equation (3) are unlikely to be completely random and patternless

- This is because they represent actions of intelligent economic agents (e.g. farmers)

**Error terms are likely to be autocorrelated**



## Causes of Autocorrelation  {.smaller}
### Data manipulation


**Examples:** 

- Quarterly data may smooth out the wild fluctuations in monthly sales figures

- Low frequency economic survey data may be interpolated

**However:** Such data transformations may be inevitable

- In social sciences data quality may be variable

- This may induce systematic patterns and autocorrelation
    

**No magic solution -- Autocorrelation is unavoidable and must be considered**




## Detection of autocorrelation {.smaller}

- Statistical tests
    * Runs test
    * Durbin-Watson test

- Graphical methods
    * Simpler but can be more robust and more informative

**Graphical and statistical methods can be useful cross-check of each other!**




## Graphical tests for autocorrelation {.smaller}


- Time-series plot of residuals
    * Plot residuals $e_t$ over time
    * Check to see if any evidence of a systematic pattern exists

- Autocorrelation plot of residuals
    * Natural to think that $\e_t$ and $\e_{t-1}$ may be correlated
    * Plot residual $e_t$ against $e_{t-1}$


- Important:
    * No Autocorrelation: Plots will look **random**
    * Yes Autocorrelation: Plots will show certain **patterns**



## Graphical tests in R {.smaller}

- Code for this example is available here [autocorrelation_graph_tests.R](codes/autocorrelation_graph_tests.R)

- Stock Vs Gold prices data is available here [stock_gold.txt](datasets/stock_gold.txt)

- Read data into R and fit simple regression

```r
# Load dataset
prices <- read.table(file = "stock_gold.txt",
                    header = TRUE)

# Store data-frame into 2 vectors
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]

# Fit regression model
model <- lm(gold.price ~ stock.price)
```




##  {.smaller}

- **Time-series plot of residuals**
    * Time series plot suggests some evidence for autocorrelation
    * Look for successive runs of residuals either side of line $y = 0 \,$ (see $t = 15$)



::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute residuals
residuals <- model$resid

# Time-series plot of residuals
plot(residuals,
     xlab = "Time", 
     ylab = "Residuals",
     pch = 16,
     cex = 1.5)

# Add line y = 0 for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Time-series plot of residuals
plot(residuals, 
     xlab = "", 
     ylab= "",
     pch = 16,
     cex = 1.5)

# Add labels
mtext("Time", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

# Add line y = 0 for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::




## {.smaller}


::::: {.columns style='display: flex !important; height: 90%;'}

::: {.column width="27%" style='display: flex; align-items: center;'}

- **Autocorrelation plot of residuals**
    * Want to plot $e_t$ against $e_{t-1}$
    * Shift $e_t$ by 1 to get $e_{t-1}$
    * Can only plot magenta pairs
    * We have 1 pair less than number of residuals


:::

::: {.column width="72%" style='display: flex; justify-content: center; align-items: center;'}

![](images/shifted_vectors.png){width=110%}
![](images/shifted_vectors_2.png){width=110%}

:::
:::::





##  {.smaller}


- We want to plot $e_t$ against $e_{t-1}$
    * Residuals are stored in vector $\,\, \texttt{residuals}$
    * We need to create a shifted version of $\,\, \texttt{residuals}$
    * First compute the length of $\,\, \texttt{residuals}$

```r
# Compute length of residuals
length(residuals)
```
```{r}
# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute length of residuals
length(residuals)
```


- Need to generate the $33-1$ pairs for plotting


##  {.smaller}


- **Lag 0:** 
    * This is the original vector with no lag
    * Lose one observation from $\,\, \texttt{residuals}$ -- the first observation

```r 
residuals.lag.0 <- residuals[2:33]
```


- **Lag 1:** 
    * This is the original vector shifted by 1
    * Lose one observation from $\,\, \texttt{residuals}$ -- the last observation

```r 
residuals.lag.1 <- residuals[1:32]
```



##  {.smaller}

- **Autocorrelation plot of residuals**
    * Plot suggests positive autocorrelation of residuals
    * This means $\, e_t \, \approx \, \alpha + \beta \, \e_{t-1} \,$ with $\beta > 0$ 
    

::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Plot Lag0 Vs Lag1 residuals

plot(residuals.lag.0, 
     residuals.lag.1,
     xlab = "Residuals Lag 0", 
     ylab = "Residuals Lag 1",
     pch = 16,
     cex = 1.5)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Generate lagged version of residuals
residuals.lag.0 <- residuals[2:33]
residuals.lag.1 <-residuals[1:32]

# Plot Lag0 Vs Lag1 residuals
plot(residuals.lag.0, 
     residuals.lag.1,
     xlab = "", 
     ylab= "",
     pch = 16,
     cex = 1.5)

# Add labels
mtext("Residuals Lag 0", side=1, line=3, cex=2.1)
mtext("Residuals Lag 1", side=2, line=2.5, cex=2.1)
```

:::
:::::





## Statistical tests for Autocorrelation {.smaller}

- Runs test
    * Under the classical multiple linear regression model residuals are equally likely to be positive or negative

- Durbin-Watson test
    * Test to see if residuals are AR(1)
 









# Part 3: <br>Multicollinearity {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::


## TBA {.smaller}


