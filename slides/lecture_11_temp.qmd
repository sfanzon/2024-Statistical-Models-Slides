---
title: "Statistical Models"
subtitle: "Lecture 11"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::











# Part 3: <br>Multicollinearity {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::







## Multicollinearity {.smaller}

- The general linear regression model is 

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- Consider Assumption 6
    * The design matrix $Z$ is such that
    $$
    Z^T Z  \, \text{ is invertible}
    $$

- **Multicollinearity:** The violation of Assumption 6

$$
\det(Z^T Z ) \, \approx  \, 0  \, \quad \implies \quad Z^T Z \, \text{ is (almost) not invertible}
$$




## The nature of Multicollinearity {.smaller}

$$ 
\text{Multicollinearity = multiple (linear) relationships between the X-variables}
$$


- Multicollinearity arises when there is either 
    * **exact** linear relationship amongst the $X$-variables
    * **approximate** linear relationship amongst the $X$-variables

<br>

**$X$-variables *inter-related*** $\quad \implies \quad$ **hard to isolate individual influence on $Y$**



## Example of Multicollinear data {.smaller}


::::: {.columns style='display: flex !important; height: 90%;'}

::: {.column width="64%" style='display: flex; justify-content: center; align-items: center;'}

- Perfect collinearity for $Z_1$ and $Z_2$
    * because of exact linear relation
    $$
    Z_2 = 5 Z_1
    $$

- No perfect collinearity for $Z_1$ and $Z_3$

- But $Z_3$ is small perturbation of $Z_2$

:::

::: {.column width="34%" style='display: flex; justify-content: center; align-items: center;'}



|  $Z_1$| $Z_2$ |       $Z_3$|
|-------|-------|------------|
|  10   |   50  |      52    |
|  15   |   75  |      75    |
|  18   |   90  |      97    |
|  24   |  120  |     129    |
|  30   |  150  |     152    |



:::
:::::



## Example of Multicollinear data {.smaller}

::::: {.columns style='display: flex !important; height: 90%;'}

::: {.column width="64%" style='display: flex; justify-content: center; align-items: center;'}


- Approximate collinearity for $Z_1, Z_3$
    * Correlation between $Z_1$ and $Z_3$ is almost $1$
    * There is approximate linear relation between $Z_1$ and $Z_3$
    $$
    Z_3 \, \approx \, 5 Z_1
    $$

- **Both instances qualify as multicollinearity**

:::

::: {.column width="34%" style='display: flex; justify-content: center; align-items: center;'}



|  $Z_1$| $Z_2$ |       $Z_3$|
|-------|-------|------------|
|  10   |   50  |      52    |
|  15   |   75  |      75    |
|  18   |   90  |      97    |
|  24   |  120  |     129    |
|  30   |  150  |     152    |



:::
:::::





## Example of Multicollinear data {.smaller}

::::: {.columns style='display: flex !important; height: 90%;'}

::: {.column width="64%" style='display: flex; justify-content: center; align-items: center;'}



```{r}
#| echo: true

# Enter the data
Z1 <- c(10, 15, 18, 24, 30)
Z3 <- c(52, 75, 97, 129, 152)

# Compute correlation
cor(Z1, Z3)
```



:::

::: {.column width="34%" style='display: flex; justify-content: center; align-items: center;'}



|  $Z_1$| $Z_2$ |       $Z_3$|
|-------|-------|------------|
|  10   |   50  |      52    |
|  15   |   75  |      75    |
|  18   |   90  |      97    |
|  24   |  120  |     129    |
|  30   |  150  |     152    |



:::
:::::




## Consequences of Multicollinearity {.smaller}

- Therefore multicollinearity means that
    * Predictors $Z_j$ are (approximately) linearly dependent
    * E.g. one can be written as (approximate) linear combination of the others


- Recall that the design matrix is

$$
Z
= 
\left( 
\begin{array}{cccc}
z_{11}  & z_{12} & \ldots & z_{ip} \\
z_{21}  & z_{22} & \ldots & z_{2p} \\
\ldots  & \ldots & \ldots & \ldots \\
z_{n1}  & z_{n2} & \ldots & z_{np} \\
\end{array}
\right)
$$

- Note that $Z$ has $p$ columns



## Consequences of Multicollinearity {.smaller}

- If at least one pair $Z_i$ and $Z_j$ is collinear (linearly dependent) then

$$
{\rm rank} (Z) < p
$$


- Hence 

$$
{\rm rank} \left(  Z^T Z  \right) < p
$$


- Therefore the matrix 

$$
Z^T Z \,\, \text{ is NOT invertible} 
$$



## Consequences of Multicollinearity {.smaller}

- This means the least-squares estimator is not well defined

$$
\hat{\beta} = (Z^T Z)^{-1} Z^T y
$$


- In practice one never has exact Multicollinearity

- In case of approximate Multicollinearity it holds that
    * The matrix $Z^T Z$ can be inverted
    * The estimator $\hat \beta$ can be computed!
    * However the inversion is numerically instable


## Consequences of Multicollinearity {.smaller}

- Numerically instable means: 
    * **We may not be able to trust the estimator $\hat \beta$**



This is because of:

1. Large estimated standard errors
    - Lack of precision associated with parameter estimtes
    - Wider confidence intervals
    - May affect hypothesis tests
 
2. Correlated parameter estimates
    - Another potential source of errors
    - Potential numerical problems with computational routines
    



## Why are estimated standard errors large?  {.smaller}

- Recall formula of estimated standard error for $\beta_j$

$$
\ese = \xi_{jj}^{1/2} \, S   \,, \qquad \quad S^2 = \frac{\RSS}{n-p}
$$


- The numbers $\xi_{jj}$ are diagonal entries of $Z^T Z$

- Approximate Multicollinearity implies that
    * $Z^T Z$ is invertible
    * The inversion is numerically unstable



## Why are estimated standard errors large?  {.smaller}

- Numerically unstable inversion means that
    * Entries of $(Z^T Z)^{-1}$ might be very large
    * Therefore $\xi_{jj}$ might be very large


- As a consequence the estimated standard errors might be very large, since

$$
\ese = \xi_{jj}^{1/2} S
$$

- **It becomes harder to reject incorrect hypotheses**




## Effect of Multicollinearity on t-tests {.smaller}

- To test the null hypothesis that $\beta_j = 0$ we use $t$-statistic

$$
t = \frac{\hat{\beta}_j}{ \ese } 
$$

- But multicollinearity increases the $\ese$

- Therefore the t-statistic reduces in size:
    * t-statistic will be smaller than it should
    * It will be easier to obtain $p < 0.05$
    * Easier to reject null hypothesis


**It becomes harder to reject incorrect hypotheses!**




## Sources of multicollinearity {.smaller}

$\bullet$ \textbf{The data collection method employed}\\
$\quad$ \textbf{-} Sampling over a limited range of values taken by the regressors in the population\\
$\bullet$ \textbf{Constraints on the model or population}\\
$\quad$ \textbf{-} E.g. variables such as income and house size may be interrelated\\
$\bullet$ \textbf{Model Specification}\\
$\quad$ \textbf{-} E.g. adding polynomial terms to a model when the range of the $X$-variables is small

}





    


