---
title: "Statistical Models"
subtitle: "Lecture 11"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::







# Part 5: <br>Dummy variable <br> regression models{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Outline


1. The nature of dummy-variable regression
2. ANOVA models
3. ANOVA models with two qualitative variables
4. ANOVA models with interactions
5. ANCOVA: regression with qualitative **and** quantitative dependent variables



## Explaining the terminology {.smaller}


- **Dummy variable:** Variables $X$ which are quantitative in nature

- **ANOVA:** refers to situations where regression models contain 
    * **only** dummy variables $X$


- **ANCOVA:** refers to situations where regression models contain a combination of 
    * dummy variables **and** quantitative (the usual) variables




## Dummy variables {.smaller}


- **Dummy variable:** 
    * A variable $X$ which is qualitative in nature
    * Often called **cathegorical variables**


- Regression models can include dummy variables


- Qualitatitve **binary** variables can be represented by $X$ with
    * $X = 1 \,$ if effect present
    * $X = 0 \,$ if effect not present


- Examples of **binary** quantitative variables are
    * On / Off
    * Yes / No
    * Sample is from Population A / B



## Dummy variables {.smaller}

- Dummy variables can also take several values 
    * These values are often called **levels**
    * Such variables are represented by $X$ taking discrete values

- Examples of dummy variables with several levels
    * Season: Summer, Autumn, Winter, Spring
    * Sex: Male, Female, Non-binary, ...
    * Priority: Low, Medium, High
    * Quarterly sales data: Q1, Q2, Q3, Q4
    * UK regions: East Midlands, London Essex, North East/Yorkshire, ...




## Example: Fridge sales data {.smaller}

::: {style="font-size: 0.90em"}

- Consider the dataset on quarterly fridge sales [fridge_sales.txt](datasets/fridge_sales.txt)
    * Each entry represents sales data for 1 quarter
    * 4 consecutive entries represent sales data for 1 year


::: {style="font-size: 0.20em"}

<br>

:::

```{r}
sales <- read.table(file = "datasets/fridge_sales.txt",
                    header = TRUE)

print(sales)
```

::: 




##  {.smaller}

::: {style="font-size: 0.90em"}

- Below are the first 4 entries of the *Fridge sales dataset*
    * These correspond to 1 year of sales data

- First two variables are quantitative
    * *Fridge Sales* $=$ total quarterly fridge sales (in million \$)
    * *Duarable Goods Sales* $=$ total quarterly durable goods sales (in billion \$)

- Remaining variables are qualitative:
    * *Q1*, *Q2*, *Q3*, *Q4* $\,$ take values 0 / 1  $\quad$ (representing 4 yearly quarters)
    * *Quarter* $\,$ takes values 1 / 2 / 3 / 4  $\quad$ (equivalent representation of quarters)


::: {style="font-size: 0.40em"}

<br>

:::

| Fridge Sales | Durable Goods Sales | Q1 | Q2 | Q3 | Q4 | Quarter |
|--------------|---------------------|----|----|----|----|---------|
| 1317         | 252.6               | 1  | 0  | 0  | 0  | 1       |
| 1615         | 272.4               | 0  | 1  | 0  | 0  | 2       |
| 1662         | 270.9               | 0  | 0  | 1  | 0  | 3       |
| 1295         | 273.9               | 0  | 0  | 0  | 1  | 4       |

:::





## Encoding Quarter in regression model {.smaller}

**Two alternative approaches:**

1. Include $4-1 = 3$ dummy variables with values 0 / 1
    * Each dummy variable represents 1 Quarter
    * We need 3 variables to represent 4 Quarters (if we include intercept)

2. Include one variable which takes values 1 / 2 / 3 / 4 


**Differences between the two approaches:**

1. This method is good to first understand dummy variable regression

2. This is the most efficient way of organising cathegorical data in R
    * usese the command $\, \texttt{factor}$



## The dummy variable trap {.smaller}

- Suppose you follow the first approach:
    * Encode each quarter with a separate variable

- If you have 4 different levels you would need 
    * $4-1=3$ dummy variables
    * the intercept term

- In general: if you have $m$ different levels you would need 
    * $m-1$ dummy variables
    * the intercept term

**Question:** Why only $m - 1$ dummy variables?

**Answer:** To avoid the **dummy variable trap**



## Example: Dummy variable trap {.smaller}


To illustrate the dummy variable trap consider the following

- Encode each Quarter with one dummy variable $D_i$

$$
D_i = \begin{cases}
1 & \text{ if data belongs to Quarter i} \\
0 & \text{ otherwise} \\
\end{cases}
$$


- Consider the regression model with intercept

$$
Y = \beta_0 \cdot (1) + \beta_1 D_1 + \beta_2 D_2 + \beta_3 D_3 + \beta_4 D_4   + \e
$$

- In the above $Y$ is the quartely *Fridge sales data*



## {.smaller}

- Each data entry belongs to exactly one Quarter, so that

$$
D_1 + D_2 + D_3 + D_4 = 1
$$


- **Dummy variable trap:** Variables are collinear (linearly dependent)

- Indeed the design matrix is 

$$
Z = 
\left( 
\begin{array}{ccccc}
1 & 1 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 1 \\
1 & 1 & 0 & 0 & 0 \\
\dots & \dots & \dots & \dots & \dots \\
\end{array}
\right)
$$

- First column is the sum of remaining columns $\quad \implies \quad$ Multicollinearity



## Example: Avoiding dummy variable trap {.smaller}

- We want to avoid Multicollinearity (or dummy variable trap)

- **How?** Drop one dummy variable (e.g. the first) and consider the model

$$
Y = \beta_1 \cdot (1) + \beta_2 D_2 + \beta_3 D_3 + \beta_4 D_4  + \e
$$


- If data belongs to Q1 then $D_2 = D_3 = D_4 = 0$


- Therefore, in general, we have

$$
D_2 + D_3 + D_4 \not\equiv 1
$$

- This way we **avoid** Multicollinearity $\quad \implies \quad$ **no trap!**



## {.smaller}


- **Question:** How do we interpret the coefficients in the model 

$$
Y = \beta_1 \cdot (1)  + \beta_2 D_2 + \beta_3 D_3 + \beta_4 D_4  + \e \qquad ?
$$


- **Answer:** Recall the relationship 

$$
D_1 + D_2 + D_3 + D_4 = 1
$$

- Substituting in the regression model we get

\begin{align*}
Y & = \beta_1 \cdot ( D_1 + D_2 + D_3 + D_4 ) + \beta_2 D_2 + \beta_3 D_3 + \beta_4 D_4  + \e \\[10pts]
& = \beta_1 D_1 + (\beta_1 + \beta_2) D_2 + (\beta_1 + \beta_3) D_3 + (\beta_1 + \beta_4 ) D_4 + \e  
\end{align*}



## {.smaller}

- Therefore the regression coefficients are such that
    * $\beta_1$ describes increase for $D_1$
    * $\beta_1 + \beta_2$ describes increase for $D_2$
    * $\beta_1 + \beta_3$ describes increase for $D_3$
    * $\beta_1 + \beta_4$ describes increase for $D_4$


**Conclusion:** When fitting regression model with dummy variables

- Increase for first dummy variable $D_1$ is intercept term $\beta_1$

- Increase for successive dummy variables $D_i$ with $i > 1$ is computed by
$\beta_1 + \beta_i$


**Intercept coefficient acts as base reference point**




## General case: Dummy variable trap {.smaller}


- Suppose to have a qualitative $X$ variable which takes $m$ different levels
    * E.g. the previous example has $m = 4$ quarters


- Encode each level of $X$ in one dummy variable $D_i$

$$
D_i = \begin{cases}
1 & \text{ if X has level i} \\
0 & \text{ otherwise} \\
\end{cases}
$$


- To each data entry corresponds one and only one level of $X$, so that 

$$
D_1 + D_2 + \ldots + D_m = 1
$$


- Hence **Multicollinearity** if intercept is present $\, \implies \,$ **Dummy variable trap!**



## General case: Avoid the trap!  {.smaller}


- We drop the first dummy variable $D_1$ and consider the model

$$
Y = \beta_1 \cdot (1)  + \beta_2 D_2 + \beta_3 D_3 + \ldots + \beta_m D_m  + \e 
$$


- For data points such that $X = 1$ we have

$$
D_2 = D_3 = \ldots = D_m = 0
$$

- Therefore, in general, we get

$$
D_2 + D_3 + \ldots + D_m \not \equiv 1
$$

- This way we avoid Multicollinearity $\quad \implies \quad$ **no trap!**



## General case: Interpret the output  {.smaller}

- How to interpret the coefficients in the model

$$
Y = \beta_1 \cdot (1)  + \beta_2 D_2 + \beta_3 D_3 + \ldots + \beta_m D_m  + \e  \quad ?
$$

- We can argue similarly to the case $m = 4$ and use the constraint

$$
D_1 + D_2 + \ldots + D_m = 1
$$

- Substituting in the regression model we get

$$
Y = \beta_1 D_1 + (\beta_1 + \beta_2) D_2 + \ldots + (\beta_1 + \beta_m) D_m + \e
$$



## {.smaller}


**Conclusion:** When fitting regression model with dummy variables

- Increase for first dummy variable $D_1$ is intercept term $\beta_1$

- Increase for successive dummy variables $D_i$ with $i > 1$ is computed by
$\beta_1 + \beta_i$


**Intercept coefficient acts as base reference point**




## Factors in R {.smaller}

- Before proceeding we need to introduce **factors** in R

- **Factors:** A way to represent discrete variables taking a finite number of values


- **Example:** Suppose to have a vector of people's names

```r
firstname <- c("Liz", "Jolene", "Susan", "Boris", "Rochelle", "Tim")
```

- Let us store the sex of each person as either 
    * Numbers: $\, \texttt{1}$ represents female and $\texttt{0}$ represents male
    * Strings: $\, \texttt{"female"}$ and $\texttt{"male"}$


```r
sex.num <- c(1, 1, 1, 0, 1, 0)
sex.char <- c("female", "female", "female", "male", "female", "male")
```


## The factor command {.smaller}

- The $\, \texttt{factor}$ command turns a vector into a factor

```r
# Turn sex.num into a factor
sex.num.factor <- factor(sex.num)

# Print the factor obtained
print(sex.num.factor)
```

```{r}
sex.num <- c(1, 1, 1, 0, 1, 0)

# Turn sex.num into a factor
sex.num.factor <- factor(sex.num)

# Print the factor obtained
print(sex.num.factor)
```

::: {style="font-size: 0.20em"}

<br>

:::

- The factor $\, \texttt{sex.num.factor}$ looks like the original vector $\, \texttt{sex.num}$

- The difference is that the factor $\, \texttt{sex.num.factor}$ contains **levels**
    * In this case the levels are $\, \texttt{0}$ and $\, \texttt{1}$
    * Levels are all the (discrete) values assumed by the vector $\, \texttt{sex.num}$



## The factor command {.smaller}

- In the same way we can convert $\, \texttt{sex.char}$ into a factor

```r
# Turn sex.char into a factor
sex.char.factor <- factor(sex.char)

# Print the factor obtained
print(sex.char.factor)
```

```{r}
sex.char <- c("female", "female", "female", "male", "female", "male")

# Turn sex.char into a factor
sex.char.factor <- factor(sex.char)

# Print the factor obtained
print(sex.char.factor)
```


::: {style="font-size: 0.20em"}

<br>

:::

- Again, the factor $\, \texttt{sex.char.factor}$ looks like the original vector $\, \texttt{sex.char}$

- Again, the difference is that the factor $\, \texttt{sex.char.factor}$ contains **levels**
    * In this case the levels are strings $\, \texttt{"female"}$ and $\, \texttt{"male"}$
    * These 2 strings are all the values assumed by the vector $\, \texttt{sex.char}$



## Subsetting factors {.smaller}

- Factors can be subset exactly like vectors

```r
sex.num.factor[2:5]
```

```{r} 
sex.num <- c(1, 1, 1, 0, 1, 0)
sex.num.factor <- factor(sex.num)
print(sex.num.factor[2:5])
```





## Subsetting factors {.smaller}


- **Warning:** After subsetting a factor, all defined levels are still stored
    * This is true even if some of the levels are no longer represented in the subsetted factor

::: {style="font-size: 0.30em"}

<br>

:::

```r
subset.factor <- sex.char.factor[c(1:3, 5)]

print(subset.factor)
```

```{r}
sex.char <- c("female", "female", "female", "male", "female", "male")
sex.char.factor <- factor(sex.char)
print(sex.char.factor[c(1:3, 5)])
```

::: {style="font-size: 0.30em"}

<br>

:::


- The levels of $\, \texttt{subset.factor}$ are still $\, \texttt{"female"}$ and $\, \texttt{"male"}$

- This is despite $\, \texttt{subset.factor}$ only containing $\, \texttt{"female"}$



## The levels function {.smaller}

- The levels of a factor can be extracted with the function $\, \texttt{levels}$

```r 
levels(sex.char.factor)
```

```{r} 
sex.char <- c("female", "female", "female", "male", "female", "male")

sex.char.factor <- factor(sex.char)

# Print the factor obtained
print(levels(sex.num.factor))
```


::: {style="font-size: 0.30em"}

<br>

:::


- **Note:** Levels of a factor are always stored as **strings**, even if originally numbers


::: {style="font-size: 0.10em"}

<br>

:::

```r 
levels(sex.num.factor)
```

```{r} 
sex.num <- c(1, 1, 1, 0, 1, 0)

sex.num.factor <- factor(sex.num)

print(levels(sex.num.factor))
```


::: {style="font-size: 0.10em"}

<br>

:::

- The levels of $\, \texttt{sex.num.factor}$ are the strings $\, \texttt{"0"}$ and $\, \texttt{"1"}$

- This is despite the original vector $\, \texttt{sex.num}$ being numeric

- The command $\, \texttt{factor}$ converted numeric levels into strings





## Relabelling a factor {.smaller}

- The function $\, \texttt{levels}$ can also be used to **relabel** factors

- For example we can relabel
    * $\, \texttt{female}$ into $\, \texttt{f}$
    * $\, \texttt{male}$ into $\, \texttt{m}$


```r
# Relabel levels of sex.char.factor
levels(sex.char.factor) <- c("f", "m")

# Print relabelled factor
print(sex.char.factor)
```

```{r}
sex.char <- c("female", "female", "female", "male", "female", "male")
sex.char.factor <- factor(sex.char)

levels(sex.char.factor) <- c("f", "m")
print(sex.char.factor)
```



## Logical subsetting of factors {.smaller}

- Logical subsetting is done exactly like in the case of vectors

- **Important:** Need to remember that levels are always **strings**

- **Example:** To identify all the men in $\, \texttt{sex.num.factor}$ we do

::: {style="font-size: 0.20em"}

<br>

:::

```r
sex.num.factor == "0"
```

```{r}
sex.num <- c(1, 1, 1, 0, 1, 0)

sex.num.factor <- factor(sex.num)

sex.num.factor == "0"
```


::: {style="font-size: 0.20em"}

<br>

:::


- To retrieve names of men stored in $\, \texttt{firstname}$ use logical subsetting

::: {style="font-size: 0.20em"}

<br>

:::

```r
firstname[ sex.num.factor == "0" ]
```

```{r}
firstname <- c("Liz", "Jolene", "Susan", "Boris", "Rochelle", "Tim")

sex.num <- c(1, 1, 1, 0, 1, 0)

sex.num.factor <- factor(sex.num)

firstname[ sex.num.factor == "0" ]
```


## Analysis of variance (ANOVA) models in R {.smaller}

- The data in [fridge_sales.txt](datasets/fridge_sales.txt) links
    * *Sales of fridges* and *Sales of durable goods*
    * to the time of year (*Quarter*)

- For the moment ignore the data on the *Sales of durable goods* 

- **Goal:** Fit regression and analysis of variance models to link
    * *Fridge sales* to the *time of the year*

- There are two ways this can be achieved in R
    1. A regression approach using the command $\, \texttt{lm}$
    2. An analysis of variance (ANOVA) approach using the command $\, \texttt{aov}$





## Reading the data into R {.smaller}

- Data is in the file [fridge_sales.txt](datasets/fridge_sales.txt) 



- The first 4 rows of the data-set are given below


```{r}
sales <- read.table(file = "datasets/fridge_sales.txt",
                    header = TRUE)

head(sales, n=4)
```

::: {style="font-size: 0.20em"}

<br>

:::

- Therefore we have the variables
    * *Fridge sales*
    * *Durable goods sales*
    * *Q1, Q2, Q3, Q4*
    * *Quarter*




## Reading the data into R {.smaller}


- We read the data into a data-frame as usual

```r
# Load dataset on Fridge Sales
sales <- read.table(file = "fridge_sales.txt",
                    header = TRUE)

# Assign data-frame columns to vectors
fridge <- sales[ , 1]
durables <- sales[ , 2]
q1 <- sales[ , 3]
q2 <- sales[ , 4]
q3 <- sales[ , 5]
q4 <- sales[ , 6]
quarter <- sales[ , 7]
```




## Processing dummy variables in R {.smaller}

- The variables $\, \texttt{q1, q2, q3, q4} \,$ are vectors taking the values $\, \texttt{0}$ and $\, \texttt{1}$
    * No further data processing is needed for $\, \texttt{q1, q2, q3, q4}$
    * **Remember:** To avoid dummy variable trap only 3 of these 4 dummy variables can be included (if the model also includes an intercept term)


::: {style="font-size: 0.20em"}

<br>

:::

- The variable $\, \texttt{quarter}$ is a vector taking the values $\, \texttt{1}, \texttt{2}, \texttt{3}, \texttt{4}$
    * Need to tell R this is a qualitative variable
    * This is done with the $\, \texttt{factor}$ command

::: {style="font-size: 0.10em"}

<br>

:::


```r
factor(quarter)
```

```{r}
# Load dataset on Fridge Sales
sales <- read.table(file = "datasets/fridge_sales.txt",
                    header = TRUE)

quarter <- sales[ , 7]
print(factor(quarter))
```




## Regression and ANOVA {.smaller}

As already mentioned, there are 2 ways of fitting regression and ANOVA models in R

1. A regression approach using the command $\, \texttt{lm}$
2. An analysis of variance approach using the command $\, \texttt{aov}$


::: {style="font-size: 0.20em"}

<br>

:::

- Both approaches lead to the same numerical answers in our example








## Regression approach {.smaller}

We can proceed in 2 equivalent ways

1. Use a dummy variable approach (and arbitrarily excluding $\, \texttt{q1}$)

```r
# We drop q1 to avoid dummy variable trap
dummy.lm <- lm (fridge ~ q2 + q3 + q4)
summary(dummy.lm)
```

::: {style="font-size: 0.10em"}

<br>

:::

2. Using the $\, \texttt{factor}$ command on $\, \texttt{quarter}$

```r
# Need to convert quarter to a factor
quarter.f <- factor(quarter)

factor.lm <- lm(fridge ~ quarter.f)
summary(factor.lm)
```

::: {style="font-size: 0.10em"}

<br>

:::

- Get the same numerical answers using both approaches



## Output for dummy variable approach {.smaller}

::: {style="font-size: 0.83em"}

```verbatim
Call:
lm(formula = fridge ~ q2 + q3 + q4)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1222.12      59.99  20.372  < 2e-16 ***
q2            245.37      84.84   2.892 0.007320 ** 
q3            347.63      84.84   4.097 0.000323 ***
q4            -62.12      84.84  -0.732 0.470091    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 169.7 on 28 degrees of freedom
Multiple R-squared:  0.5318,	Adjusted R-squared:  0.4816 
F-statistic:  10.6 on 3 and 28 DF,  p-value: 7.908e-05
```

:::


::: {style="font-size: 0.1em"}

<br>

:::




## Output for factor approach {.smaller}


::: {style="font-size: 0.83em"}

```verbatim
Call:
lm(formula = fridge ~ quarter.f)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1222.12      59.99  20.372  < 2e-16 ***
quarter.f2    245.37      84.84   2.892 0.007320 ** 
quarter.f3    347.63      84.84   4.097 0.000323 ***
quarter.f4    -62.12      84.84  -0.732 0.470091    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 169.7 on 28 degrees of freedom
Multiple R-squared:  0.5318,	Adjusted R-squared:  0.4816 
F-statistic:  10.6 on 3 and 28 DF,  p-value: 7.908e-05

```

:::

::: {style="font-size: 0.1em"}

<br>

:::



- The two outputs are essentially the same (only difference is variables names)

- $\, \texttt{quarter.f}$ is a factor with four levels $\, \texttt{1}, \texttt{2}, \texttt{3}, \texttt{4}$

- Variables $\, \texttt{quarter.f2}, \texttt{quarter.f3}, \texttt{quarter.f4}$ refer to the levels 
$\, \texttt{2}, \texttt{3}, \texttt{4}$ in $\, \texttt{quarter.f}$


## Output for factor approach {.smaller}


::: {style="font-size: 0.83em"}

```verbatim
Call:
lm(formula = fridge ~ quarter.f)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1222.12      59.99  20.372  < 2e-16 ***
quarter.f2    245.37      84.84   2.892 0.007320 ** 
quarter.f3    347.63      84.84   4.097 0.000323 ***
quarter.f4    -62.12      84.84  -0.732 0.470091    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 169.7 on 28 degrees of freedom
Multiple R-squared:  0.5318,	Adjusted R-squared:  0.4816 
F-statistic:  10.6 on 3 and 28 DF,  p-value: 7.908e-05

```

:::

::: {style="font-size: 0.1em"}

<br>

:::


- $\, \texttt{lm}$ is treating  $\, \texttt{quarter.f2}, \texttt{quarter.f3}, \texttt{quarter.f4}$ as if they were dummy variables

- Note that $\, \texttt{lm}$ automatically drops $\, \texttt{quarter.f1}$ to prevent dummy variable trap

- Thus $\, \texttt{lm}$ behaves the same way as if we passed dummy variables $\, \texttt{q2}, \texttt{q3}, \texttt{q4}$




## Computing regression coefficients {.smaller}

::: {style="font-size: 0.86em"}

```verbatim
Call:
lm(formula = fridge ~ q2 + q3 + q4)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1222.12      59.99  20.372  < 2e-16 ***
q2            245.37      84.84   2.892 0.007320 ** 
q3            347.63      84.84   4.097 0.000323 ***
q4            -62.12      84.84  -0.732 0.470091    
```

:::


::: {style="font-size: 0.3em"}

<br>

:::


- Recall that $\, \texttt{Intercept}$ refers to coefficient for $\, \texttt{q1}$

- Coefficients for $\, \texttt{q2}, \texttt{q3}, \texttt{q4}$ are obtained by summing $\, \texttt{Intercept}$ to coefficient in appropriate row



## Computing regression coefficients {.smaller}

::: {style="font-size: 0.86em"}

```verbatim
Call:
lm(formula = fridge ~ q2 + q3 + q4)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1222.12      59.99  20.372  < 2e-16 ***
q2            245.37      84.84   2.892 0.007320 ** 
q3            347.63      84.84   4.097 0.000323 ***
q4            -62.12      84.84  -0.732 0.470091    
```

:::


::: {style="font-size: 0.5em"}

<br>

:::

::: {style="font-size: 0.9em"}

| Dummy variable |  Coefficient formula | Estimated coefficient        |
|:--------------:|:--------------------:|:----------------------------:|
| $\texttt{q1}$  | $\beta_1$            | $1222.12$                    |
| $\texttt{q2}$  | $\beta_1 + \beta_2$  | $1222.12 + 245.37 = 1467.49$ |
| $\texttt{q3}$  | $\beta_1 + \beta_3$  | $1222.12 + 347.63 = 1569.75$ |
| $\texttt{q4}$  | $\beta_1 + \beta_4$  | $1222.12 - 62.12 = 1160$     |


:::




## Regression formula {.smaller}

- Therefore the linear regression formula obtained is

\begin{align*}
\Expect[\text{ Fridge sales }] = & \,\, 1222.12 \times \, \text{Q1} + 
1467.49 \times \, \text{Q2} + \\[15pts]
& \,\, 1569.75 \times \, \text{Q3} + 1160 \times \, \text{Q4} 
\end{align*}



- Recall that Q1, Q2, Q3 and Q4 assume only values 0 / 1 and 

$$
\text{Q1} + \text{Q2} + \text{Q4} + \text{Q4} = 1
$$


- Therefore we obtain
    * Expected sales in Q1 are









::: {.content-hidden}

- ANOVA is lesson 11 John

- John never really defines ANOVA as generalization of t-test to multiple populations

- That's clever to be quicker. But I would do it the classic way and follow Verzani or DeGroot

- It would be good do classic ANOVA and then connect to regression

- Best way would be to do ANOVA classically + R (DeGroot and Verzani 12.1)

- Then do ANOVA and Linear regression interpretation

- Then move to dummy variable models and ANCOVA

- We have already seen how regression can be used to perform t-test
    * With two dummy variables $1_A$ and $1_B$
    * With one dummy variable $1_A$ (in this case it would be good to give exercise to compute t-statistic for $\beta$ explicitly and see it coincides with t-statistic for two-sample t-test with common variance (pooled estimator))

- Anova generalizes t-test. Therefore linear regression can be used for ANOVA in 2 ways:
    * 1 dummy variable for each population
    * $k-1$ dummy variables 

- For this see Verzani section 12.2


- He does the examples of t-test as simple regression that I covered in exercises

$$
Y_i = \alpha + \beta 1_{A}(i)  + \e_i
$$

where $1_{A}(i) = 1$ if sample i is from population $A$ and $0$ if not. Turns out $1_A$ is just a factor

- It also does the case of t-test intepreted as multiple regression with two dummy variables without intercept

$$
Y_i = \beta_1 1_{A}(i) + \beta_2 1_B(i) + \e_i
$$

where $1_{A}(i) = 1$ if sample i is from population $A$ and $0$ if not. Same for $1_B(i)$



Some slides I already made

## What is ANOVA? {.smaller}

- ANOVA stands for Analysis of Variance

- ANOVA is a generalization of the t-test for two independent samples
    * ANOVA allows to compare the means of several independent populations

- In the *Fridge Sales* example the populations are the 4 *Quarters*
    * For each quarter we have a list of *Fridge sales* values
    * We want to compare average fridge sales for each Quarter

- ANOVA hypothesis test: is there a difference in average sales for each Quarter?
    * If $\mu_{i}$ is average sales in Quarter $i$ then
    \begin{align*}
    H_0 & \colon \mu_{1} =  \mu_{2} =  \mu_{3} =  \mu_{4}  \\
    H_1 & \colon  \mu_i \neq \mu_j \, \text{ for at least one pair i and j}
    \end{align*}



## Linear Regression for ANOVA {.smaller}

- Already seen that linear regression can be used to perform two-sample t-test

- There were 2 main ideas
    * One shown in Lecture 8
    * The other shown in Homework

- The same ideas apply to ANOVA with $k$ populations

- Let us recall these ideas




## Recap: Linear Regression for t-test {.smaller}

- We have 2 populations $A$ and $B$ distributed like $N(\mu_A, \sigma_A^2)$ and $N(\mu_B, \sigma_B^2)$

- We have two samples
  * Sample of size $n_A$ from population $A$
  $$
  a = (a_1, \ldots, a_{n_A})
  $$
  * Sample of size $n_B$ from population $B$
  $$
  b = (b_1, \ldots, b_{n_B})
  $$

- Two-sample t-test compares means $\mu_A$ and $\mu_B$ by computing t-statistic
$$
t = \frac{ \overline{a} - \overline{b} }{\ese}
$$


:::




