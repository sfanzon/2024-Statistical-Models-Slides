---
title: "Statistical Models"
subtitle: "Lecture 1"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---


# Lecture 1: <br>An introduction to Statistics {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Outline of Lecture 1

1. Module info
2. Motivation
3. Probability background
4. Statistics background





# Part 1: <br>Module info {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Contact details

- **Lecturer:** Dr. Silvio Fanzon
- **Call me:**
  * Silvio
  * Dr. Fanzon
- **Email:** S.Fanzon@hull.ac.uk
- **Office:** Room 104a, Larkin Building
- **Office hours:** Wednesday 12:00-13:00
- **Meetings**: in my office or send me an Email






## References
### Main textbooks  


::: {.column width="61%"}

<br>

Slides are self-contained and based on the book

- [@bingham-fry] Bingham, N. H. and Fry, J. M. 
<br> *Regression: Linear models in statistics.* <br> Springer, 2010


:::


::: {.column width="38%"}

[![](images/bingham_fry.png){width=82%}](https://link.springer.com/book/10.1007/978-1-84882-969-5)

:::





## References
### Main textbooks 


::: {.column width="61%"}
<br>

.. and also on the book

- [@fry-burke] Fry, J. M. and Burke, M. 
<br>*Quantitative methods in finance using R.* 
<br>Open University Press, 2022

:::


::: {.column width="38%"}

[![](images/fry_burke.png){width=87%}](https://www.mheducation.co.uk/quantitative-methods-in-finance-using-r-9780335251261-emea-group)

:::







## References
### Secondary References 

::: {.column width="69%"}
- [@casella-berger] Casella, G. and Berger R. L. <br>
*Statistical inference.*
<br> Second Edition, Brooks/Cole, 2002


- [@degroot] DeGroot M. H. and Schervish M. J. <br> 
*Probability and Statistics.* 
<br> Fourth Edition, Addison-Wesley, 2012


- [@dalgaard] Dalgaard, P. 
<br> *Introductory statistics with R.*
<br> Second Edition, Springer, 2008   

:::


::: {.column width="30%"}

**Probability & Statistics manual**

**Easier Probability & Statistics manual**

**R manual**

:::



## Assessment 

This course will be assessed as follows:

<br>


|**Type of Assessment**  | **Percentage of final grade** |
|:-----                  |:-----                         |
|  Coursework Portfolio  | 70%                           |
|  Homework              | 30%                           |




## Assessment 
### Rules for Coursework


- Coursework available on Canvas from Week 2

- We will discuss coursework exercises in class

- Coursework must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Wednesday 1st May 2024**





## Assessment
### Rules for Homework


- 10 Homework papers, posted weekly on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** 

- Each Homework paper is worth 14 points

- Final Homework grade computation: 
    * Sum the top 7 scores, for a maximum of 98 points
    * Bonus 2 points will be added to the final score

- Homework papers must be submitted on **[Canvas](https://canvas.hull.ac.uk/courses/70065)** by **14:00 on Mondays** 





## Assessment
### Key submission dates


::: {.column width="48%"}

|  **Assignment** |**Due date** |
|:--------        |:----------- |
| Homework 1      | 5 Feb       |
| Homework 2      | 12 Feb      |
| Homework 3      | 19 Feb      |
| Homework 4      | 26 Feb      |
| Homework 5      | 4 Mar       |
| Homework 6      | 11 Mar      |


:::



::: {.column width="48%"}

|  **Assignment** |**Due date** |
|:--------        |:----------- |
| Homework 7      | 18 Mar      |
| Easter Break    | :sunglasses:|
| Homework 8      | 8 Apr       |
| Homework 9      | 15 Apr      |
| Homework 10     | 22 Apr      |
| Coursework      | 1 May       |

:::



## Assessment
### How to submit assignments


- Submit **PDFs only** on **[Canvas](https://canvas.hull.ac.uk/courses/70065)**

- You have two options:
	* Write on tablet and submit PDF Output
	* Write on paper and **Scan in Black and White** using a **Scanner** or **Scanner App** (Tiny Scanner, Scanner Pro, ...)







## The nature of Statistics
### Statistics is a mathematical subject

- Maths skills will give you a head start

- There are other occasions where common sense and *detective skills* can be more important

- Provides an early example of mathematics working in concert with the available computation




## The nature of Statistics
### We will use a combination of hand calculation and software

- Recognises that you are maths students
- Software (R) is really useful, particularly for dissertations
- Please bring your laptop into class
- Download R onto your laptop







## Overview of the module


Module has **11 Lectures**, divided into two parts:

- **Part I** - Mathematical statistics

- **Part II** - Applied statistics





## Overview of the module
### Part I - Mathematical statistics

1. Introduction to statistics
2. Normal distribution family and one-sample hypothesis tests
3. Two-sample hypothesis tests
4. The chi-squared test
5. Non-parametric statistics
6. The maths of regression





## Overview of the module
### Part II - Applied statistics


7. An introduction to practical regression
8. The extra sum of squares principle and regression modelling assumptions
9. Violations of regression assumptions -- Autocorrelation
10. Violation of regression assumptions -- Multicollinearity
10. Dummy variable regression models 





# Part 2: <br>Motivation {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::


## Simple but useful questions {.smaller}


::: {.column width="48%"}

**Generic data:**

- What is a *typical* observation
  * What is the **mean**?

- How spread out is the data?
  * What is the **variance**?

:::


::: {.column width="48%"}

**Regression:**

- What happens to $Y$ as $X$ increases?
  * increases?
  * decreases?
  * nothing?

:::



**Statistics answers these questions systematically**

- important for large datasets
- The same mathematical machinery (normal family of distributions) can be applied to both questions




## Analysing a general dataset

**Two basic questions:**

1. Location or mean
2. Spread or variance


**Statistics enables to answer systematically:**

1. One sample and two-sample $t$-test
2. Chi-squared test and $F$-test





## Recall the following sketch

![Curve represents data distribution](images/Fig1.png){width=90%}




## Motivating regression

**Basic question in regression:**

- What happens to $Y$ as $X$ increases?

  * increases?
  * decreases?
  * nothing?


**In this way regression can be seen as a more advanced version of high-school maths**







## Positive gradient

As $X$ increases $Y$ increases

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(0, 5), 
  ylim = c(0, 5), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)


mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 0, 
  b = 1, 
  col = "black", 
  lwd = 2)

```




## Negative gradient

As $X$ increases $Y$ decreases

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(-3, 3), 
  ylim = c(-3, 3), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)



mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 0, 
  b = -1, 
  col = "black", 
  lwd = 2)

```





## Zero gradient

Changes in $X$ do not affect $Y$

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(0, 5), 
  ylim = c(0, 5), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)


mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 2.5, 
  b = 0, 
  col = "black", 
  lwd = 2)

```





## Real data example


- Real data is more **imperfect**
- But the same basic idea applies
- Example: 
    * $X =$ Stock price
    * $Y =$ Gold price








## Real data example
### How does real data look like?

::: {style="font-size: 0.75em"}
Dataset with $33$ entries for Stock and Gold price pairs
:::


::: {style="font-size: 0.55em"}
```{r}
#| echo: false
#| layout-ncol: 1

data <- read.table("datasets/L3eg1data.txt")

#Add column labels to data
colnames(data) <- c("Stock Price","Gold Price")

# Output markdown table from data
knitr::kable(
  list(data[1:11,], data[12:22,], data[23:33,]),
  row.names = TRUE,
  format = "html", 
  table.attr = 'class="table simple table-striped table-hover"',
) 


```


:::




## Real data example {.smaller}
### Visualizing the data


::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

- Plot Stock Price against Gold Price

- Observation: 

  * As Stock price decreases, Gold price increases

- Why? This might be because:
    * Stock price decreases
    * People invest in secure assets (Gold)
    * Gold demand increases
    * Gold price increases

:::


::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

data1 <- read.table("datasets/L3eg1data.txt")
realgoldprice<-data1[,1]
realstockprice<-data1[,2]
plot(realstockprice, 
  realgoldprice, 
  xlab="", 
  ylab="")

mtext("Stock Price", side=1, line=3, cex=2)
mtext("Gold Price", side=2, line=2.5, cex=2)

```

:::

:::::




## Don't panic {.smaller}

- Regression problems can look a lot harder than they really are
  * Basic question remains the same: what happens to $Y$ as $X$ increases?

- Beware of jargon. Various authors distinguish between
  * Two variable regression model
  * Multiple regression model
  * Analysis of Variance
  * Analysis of Covariance

- Despite these apparent differences:
  * Mathematical methodology stays (essentially) the same
  * regression-fitting commands in R stay (essentially) the same





# Part 3: <br>Probability background {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Sample space {.smaller}

::: Definition
### Sample space

A set $\Omega$ of all possible outcomes of some experiment

:::


**Examples**:

- Coin toss: results in Heads $= H$ and Tails $= T$ 
$$
\Omega = \{ H, T \}
$$

- Student grade for *Statistical Models*: a number between $0$ and $100$
$$
\Omega = \{ x \in \mathbb{R} \, \colon \, 0 \leq x \leq 100 \} = [0,100]
$$






## Events {.smaller}


::: Definition
### Event

A subset $E$ of the sample space $\Omega$ (including $\emptyset$ and $\Omega$ itself)

:::


**Operations with events**:

- **Union** of two events $A$ and $B$
$$
A \cup B := \{ x \in \Omega \colon x \in A \, \text{ or } \, x \in B \}
$$

- **Intersection** of two events $A$ and $B$
$$
A \cap B := \{ x \in \Omega \colon x \in A \, \text{ and } \, x \in B \}
$$

- **Complement** of an event $A$
$$
A^c  := \{ x \in \Omega \colon x \notin A  \}
$$




## Events {.smaller}


**More Operations with events**:

- **Infinite Union** of a family of events $A_i$ with $i \in I$

$$
\bigcup_{i \in I} A_i := \{ x \in \Omega \colon x \in A_i \, \text{ for some } \, i \in I \}
$$

- **Infinite Intersection** of a family of events $A_i$ with $i \in I$

$$
\bigcap_{i \in I} A_i := \{ x \in \Omega \colon x \in A_i \, \text{ for all } \, i \in I \}
$$





## Events {.smaller}


**Example**: Consider sample space and events
$$
\Omega := (0,1] \,, \quad 
A_i = \left[\frac{1}{i} , 1 \right] \,, \quad i \in \mathbb{N}
$$
Then
$$
\bigcup_{i \in I} A_i = (0,1] \,, \quad 
\bigcap_{i \in I} A_i = \{ 1 \}
$$





## Events {.smaller}

::: Definition
### Disjoint

Two events $A$ and $B$ are **disjoint** if
$$
A \cap B = \emptyset
$$
Events $A_1, A_2, \ldots$ are **pairwise disjoint** if
$$
A_i \cap A_j = \emptyset \,, \quad \forall \, i \neq j
$$
:::


::: Definition
### Partition

The collection of events  $A_1, A_2, \ldots$ is a **partition** of $\Omega$ if

1. $A_1, A_2, \ldots$ are pairwise disjoint
2. $\Omega = \cup_{i=1}^\infty A_i$

:::




## What's a Probability? {.smaller}


- To each event $E \subset \Omega$ we would like to associate a number
$$
P(E) \in [0,1]
$$

- The number $P(E)$ is called the **probability** of $E$

- The number $P(E)$ models the **frequency of occurrence** of $E$:

    * $P(E)$ small means $E$ has low chance of occurring
    * $P(E)$ large means $E$ has high chance of occurring

- **Technical issue**:

    * One cannot associate a number $P(E)$ for all events in $\Omega$
    * Probability function $P$ only defined for a **smaller** family of events
    * Such family of events is called $\sigma$-algebra






## $\sigma$-algebras {.smaller}


::: Definition
### sigma-algebra

Let $\mathcal{B}$ be a collection of events. We say that $\mathcal{B}$ is a $\sigma$-algebra if

1. $\emptyset \in \mathcal{B}$
2. If $A \in \mathcal{B}$ then $A^c \in \mathcal{B}$
3. If $A_1,A_2 , \ldots \in \mathcal{B}$ then $\cup_{i=1}^\infty A_i \in \mathcal{B}$


:::



**Remarks**:

- Since $\emptyset \in \mathcal{B}$ and $\emptyset^c = \Omega$, we deduce that
$$
\Omega \in \mathcal{B}
$$

- Thanks to DeMorgan's Law we have that
$$
A_1,A_2 , \ldots \in \mathcal{B} \quad \implies \quad \cap_{i=1}^\infty A_i \in \mathcal{B}
$$






## $\sigma$-algebras {.smaller}


**Examples**: 

- If $\Omega$ is any set, then
$$
\mathcal{B} = \{ \emptyset, \Omega \}
$$
is a $\sigma$-algebra


- If $\Omega$ is **finite** or **countable** then the power set of $\Omega$
$$
\mathcal{B} = \operatorname{Power} (\Omega) = \{ A \colon A \subset \Omega \}
$$
is a $\sigma$-algebra






## $\sigma$-algebras {.smaller}

**Examples**: 


- If $\Omega$ has $n$ elements then $\mathcal{B} = \operatorname{Power} (\Omega)$ contains $2^n$ sets

- If $\Omega = \{ 1,2,3\}$ then
\begin{align*}
\mathcal{B} = \operatorname{Power} (\Omega) = \big\{ & \{1\} , \, \{2\}, \, \{3\}  \\
                                       &  \{1,2\} , \, \{2,3\}, \, \{1,3\}      \\
                                       & \emptyset , \{1,2,3\}   \big\}
\end{align*}

- If $\Omega$ is **uncountable** then the power set of $\Omega$ is not easy to describe






## Lebesgue $\sigma$-algebra {.smaller}


::: Question

$\mathbb{R}$ is uncountable. Which $\sigma$-algebra do we consider?

:::



::: Definition 
### Lebesgue sigma-algebra


The Lebesgue $\sigma$-algebra on $\mathbb{R}$ is the smallest $\sigma$-algebra $\mathcal{L}$ containing all sets of the form
$$
(a,b) \,, \quad (a,b] \,, \quad [a,b) \,, \quad [a,b] 
$$
for all $a,b \in \mathbb{R}$

:::



## Lebesgue $\sigma$-algebra {.smaller}


::: Important

Therefore the events of $\R$ are 

- Intervals
- Unions and intersection of intervals
- Countable Unions and intersection of intervals

:::


::: Warning 

- I only told you that the Lebsesgue $\sigma$-algebra $\mathcal{L}$ **exists**
- Explicitly showing that $\mathcal{L}$ exists is not easy, see [@rosenthal]

:::




## Probability measure {.smaller}

Suppose given:

- $\Omega$ sample space 
- $\mathcal{B}$ a $\sigma$-algebra on $\Omega$

::: Definition
### Probability measure

A **probability measure** on $\Omega$ is a map
$$
P \colon \mathcal{B} \to [0,1]
$$
such that the **Axioms of Probability** hold

1. $P(\Omega) = 1$
2. If $A_1, A_2,\ldots$ are pairwise disjoint then
$$
P\left( \bigcup_{i=1}^\infty  A_i \right)  = \sum_{i=1}^\infty P(A_i)
$$

:::




## Properties of Probability {.smaller}

Let $A, B \in \mathcal{B}$. As a consequence of the Axioms of Probability:


1. $P(\emptyset) = 0$
2. If $A$ and $B$ are disjoint then
$$
P(A \cup B) = P(A) + P(B)
$$
3. $P(A^c) = 1 - P(A)$
4. $P(A) = P(A \cap B) + P(A \cap B^c)$
5. $P(A\cup B) = P(A) + P(B) - P(A \cap B)$
6. If $A \subset B$ then
$$
P(A) \leq P(B)
$$





## Properties of Probability {.smaller} 

7. Suppose $A$ is an event and $B_1,B_2, \ldots$ a partition of $\Omega$. Then
$$
P(A) = \sum_{i=1}^\infty  P(A \cap B_i)
$$
8. Suppose $A_1,A_2, \ldots$ are events. Then
$$
P\left( \bigcup_{i=1}^\infty  A_i \right)  \leq  \sum_{i=1}^\infty P(A_i)
$$




## Example: Fair Coin Toss {.smaller}

- The sample space for **coin toss** is $\Omega = \{ H, T \}$

- We take as $\sigma$-algebra the power set of $\Omega$
$$
\mathcal{B} = \{  \emptyset , \, \{H\}  , \, \{T\} , \, \{H,T\}   \}
$$

- We suppose that the coin is fair

    * This means $P \colon \mathcal{B} \to [0,1]$ satisfies
    $$
    P(\{H\}) = P(\{T\})
    $$
    * Assuming the above we get
    $$
    1 = P(\Omega) = P(\{H\} \cup \{T\}) = 
    P(\{H\}) + P(\{T\}) = 2 P(\{H\})
    $$
    * Therefore 
    $$
    P(\{H\}) = P(\{T\}) = \frac12
    $$





## Conditional Probability {.smaller}

::: Definition
### Conditional Probability

Let $A,B$ be events in $\Omega$ with 
$$
P(B)>0
$$
The **conditional probability** of $A$ given $B$ is
$$
P(A|B) := \frac{P(A \cap B)}{P(B)} 
$$

:::


- $P(A|B)$ represents the probability of $A$, knowing that $B$ happened
- The function $A \mapsto P(A|B)$ is a probability measure on $\Omega$




## Bayes' Rule {.smaller}


- For two events $A$ and $B$ is holds

$$
P(A | B ) = P(B|A) \frac{P(A)}{P(B)}
$$


- Given a partition $A_1, A_2, \ldots$ of the sample space we have

$$
P(A_i | B ) = \frac{ P(B|A_i) P(A_i)}{\sum_{j=1}^\infty P(B | A_j) P(A_j)}
$$





## Independence {.smaller}


::: Definition

Two events $A$ and $B$ are **independent** if 
$$
P(A \cap B) = P(A)P(B)
$$
A collection of events $A_1 , \ldots ,A_n$ are **mutually independent** if
for any subcollection $A_{i_1}, \ldots, A_{i_k}$ it holds
$$
P \left(  \bigcap_{j=1}^k A_j  \right) = \prod_{j=1}^k  P(A_{i_j})
$$

:::





## Random Variables {.smaller}
### Motivation

- Consider the experiment of flipping a coin $50$ times
- The sample space consists of $2^{50}$ elements
- Elements are vectors of $50$ entries recording the outcome $H$ or $T$ of each flip
- This is a very large sample space!


Suppose we are only interested in
$$
X = \text{ number of } \, H \, \text{ in } \, 50 \, \text{flips}
$$

- Then the new sample space is the set of integers
$$
\{ 0,1,2,\ldots,50\}
$$
- This is much smaller!
- $X$ is called a Random Variable




## Random Variables {.smaller}


Assume given

- $\Omega$ sample space
- $\mathcal{B}$ a $\sigma$-algebra of events on $\Omega$
- $P \colon \mathcal{B} \to [0,1]$ a probability measure


::: Definition
### Random variable

A function $X \colon \Omega \to \mathbb{R}$

:::




## Random Variables {.smaller}
### Technical remark 


::: Definition
### Random variable

A [**measurable**]{style="color:#cc0164;"}
function $X \colon \Omega \to \mathbb{R}$

:::


**Technicality**: 
$X$ is a [**measurable**]{style="color:#cc0164;"}
function if
$$
\{ X \in I \} := \{ \omega \in \Omega \colon X(\omega) \in I \} \in \mathcal{B} \,, \quad \forall \, I \in \mathcal{L}
$$
where 

- $\mathcal{L}$ is the Lebsgue $\sigma$-algebra on $\mathbb{R}$
- $\mathcal{B}$ is the given $\sigma$-algebra on $\Omega$




## Random Variables {.smaller}
### Notation

- In particular $I \in \mathcal{L}$ can be of the form
$$
(a,b) \,, \quad (a,b] \,, \quad [a,b) \,, \quad [a,b] 
$$
for all $a,b \in \mathbb{R}$

- In this case the set 
$$
\{X \in I\} \in \mathcal{B}
$$ 
is denoted by, respectively:
$$
\{ a < X < b \} \,, \quad \{ a < X \leq b \} \,, \quad \{ a \leq X < b \} \,, \quad \{ a \leq X \leq b \}
$$



## Random Variables {.smaller}
### Why do we require measurability?

**Answer**: Because it allows to define a new probability measure on $\mathbb{R}$


::: Definition
### Distribution

The **distribution** of a random variable $X \colon \Omega \to \mathbb{R}$ is the probability measure on $\mathbb{R}$
$$
P_X \colon \mathcal{L} \to [0,1] \,, \quad P_X (I) := P \left( \{X \in I\} \right) \,, \,\, \forall \, I \in  \mathcal{L}
$$

:::


**Note**: 

- One can show that $P_X$ satisfies the *Probability Axioms*
- Thus $P_X$ is a probability measure on $\mathbb{R}$
- In the future we will denote 
$$
P \left( X \in I \right) := P \left( \{X \in I\} \right)
$$







## Random Variables {.smaller}
### Why is the distribution useful?

**Answer**: Because it allows to define a random variable $X$

- by specifying the distribution values
$$
P \left( X \in I \right) 
$$
- rather than defining an explicit function $X \colon \Omega \to \mathbb{R}$


**Important**: More often than not

- We care about the distribution of $X$
- We do not care about how $X$ is defined




## Random Variables {.smaller}

**Examples**:

- Tossing two dice




+---------+--------+------------------+
| Right   | Left   | Centered         |
+========:+:=======+:================:+
| Bananas | $1.34  | built-in wrapper |
+---------+--------+------------------+



## Examples of Continuous RV {.smaller}


- The **Normal distribution** with mean $\mu$ and variance $\sigma^2$ is denoted by 
$$
N(\mu,\sigma^2)
$$

- The **pdf** of $N(\mu,\sigma^2)$ is
$$
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}} \, \exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}
$$

- $N(0,1)$ is the **standard normal** distribution






## Examples of Continuous RV {.smaller}


::: Theorem

If $X \sim N(\mu,\sigma^2)$ then $Z \sim N(0,1)$ where
$$
Z := \frac{X-\mu}{\sigma}
$$
:::

Check

::: Proof

Check

:::




# Part 4: <br>Statistics background {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




# Thank you! {background-color="#cc0164" visibility="uncounted"}


::: footer

<div color="#cc0164"> </div>

:::



## References






::: {.content-hidden}


## Real data example
### How does real data look like?

::: {style="font-size: 0.8em"}
Dataset with $33$ entries for Stock and Gold price pairs
:::

::: {style="font-size: 0.6em"}

```{r}
#| echo: false
#| layout-ncol: 3
#| tbl-cap: "Dataset with 33 entries for Stock and Gold price pairs"

# Read dataset
data <- read.table("datasets/L3eg1data.txt")

#Add column labels to data
colnames(data) <- c("Stock Price","Gold Price")

# Output markdown table from data

knitr::kable(data[1:11,], row.names = TRUE)

knitr::kable(data[12:22,], row.names = TRUE)

knitr::kable(data[23:33,], row.names = TRUE)

```

:::

:::

