---
title: "Statistical Models"
subtitle: "Lecture 11"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 11: <br>Violation of regression <br> assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::


::: {.content-hidden}

Split this lecture over 2-3 weeks. Make a more
granular outline for each session. To do it, look 
at John's slides (Lectures 9-10-11)

:::


## Outline of Lecture 11

1. Regression modelling assumptions
2. Heteroscedasticity
3. Autocorrelation
4. Multicollinearity
5. Stepwise regression and overfitting
6. Dummy variable regression




# Part 1: <br>Regression assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Regression modelling assumptions {.smaller}

In Lecture 9 we have introduced the general linear regression model

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- There are $p$ predictor random variables

$$
Z_1 \, , \,\, \ldots \, , \, Z_p
$$


- $Y_i$ is the conditional distribution

$$
Y | Z_1 = z_{i1} \,, \,\, \ldots \,, \,\, Z_p = z_{ip}
$$

- The errors $\e_i$ are random variables 



## Regression assumptions on $Y_i$ {.smaller}

1. **Predictor is known:** The values $z_{i1}, \ldots, z_{ip}$ are known

2. **Normality:** The distribution of $Y_i$ is normal

3. **Linear mean:** There are parameters $\beta_1,\ldots,\beta_p$ such that
$$
\Expect[Y_i] = \beta_1 z_{i1} + \ldots + \beta_p z_{ip}  
$$

4. **Homoscedasticity:** There is a parameter $\sigma^2$ such that
$$
\Var[Y_i] = \sigma^2
$$

5. **Independence:** rv $Y_1 , \ldots , Y_n$ are independent and thus uncorrelated

$$
\Cor (Y_i,Y_j) = 0 \qquad \forall \,\, i \neq j
$$




## Equivalent assumptions on $\e_i$ {.smaller}

1. **Predictor is known:** The values $z_{i1}, \ldots, z_{ip}$ are known

2. **Normality:** The distribution of $\e_i$ is normal

3. **Linear mean:** The errors have zero mean
$$
\Expect[\e_i] = 0
$$

4. **Homoscedasticity:** There is a parameter $\sigma^2$ such that
$$
\Var[\e_i] = \sigma^2
$$

5. **Independence:** Errors $\e_1 , \ldots , \e_n$ are independent and thus uncorrelated

$$
\Cor (\e_i, \e_j) = 0 \qquad \forall \,\, i \neq j
$$


## Extra assumption on design matrix {.smaller}


6. The design matrix $Z$ is such that

$$
Z^T Z  \, \text{ is invertible}
$$


- Assumptions 1-6 allowed us to estimate the parameters

$$
\beta = (\beta_1, \ldots, \beta_p)
$$

- By maximizing the likelihood we obtained estimator

$$
\hat \beta = (Z^T Z)^{-1} Z^T y
$$



## Violation of Assumptions {.smaller}
### We consider 3 scenarios

i. **Heteroscedasticity:** The violation of Assumption 4 of homoscedasticity

$$
\Var [\e_i] \neq \Var[\e_j] \qquad \text{ for some } \,\, i \neq j
$$

ii. **Autocorrelation:** The violation of Assumption 5 of no-correlation

$$
\Cor( \e_i, \e_j ) \neq 0  \qquad \text{ for some } \,\, i \neq j
$$


iii. **Multicollinearity:** The violation of Assumption 6 of invertibilty of the matrix

$$
Z^T Z
$$




# Part 2: <br>Heteroscedasticity {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Heteroscedasticity {.smaller}

- The general linear regression model is 

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- Consider Assumption 4
    * **Homoscedasticity:** There is a parameter $\sigma^2$ such that
    $$
    \Var[\e_i] = \sigma^2 \qquad \forall \,\, i
    $$

- **Heteroscedasticity:** The violation of Assumption 4

$$
\Var [\e_i] \neq \Var[\e_j] \qquad \text{ for some } \,\, i \neq j
$$




## Why is homoscedasticity important? {.smaller}

- In Lecture 10 we presented a few methods to assess linear models 

    * Coefficient $R^2$
    * $t$-tests for parameters significance
    * $F$-test for model selection

- The above methods **rely heavily** on homoscedasticity




## Why is homoscedasticity important? {.smaller}

- For example the maximum likelihood estimation relied on the calculation
    \begin{align*}
    L & = \prod_{i=1}^n  f_{Y_i} (y_i)  
        = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(y_i - \hat y_i)^2}{2\sigma^2} \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{\sum_{i=1}^n(y_i- \hat y_i)^2}{2\sigma^2}      \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
    \end{align*}


- The calculation is only possible thanks to homoscedasticity

$$
\Var[Y_i] = \sigma^2 \qquad \forall \,\, i
$$



## Why is homoscedasticity important? {.smaller}

- Suppose the calculation in previous slide holds

$$
L = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$


- Then maximizing the likelihood is equivalent to solving

$$
\min_{\beta} \ \RSS
$$

- The above has the closed form solution

$$
\hat \beta = (Z^T Z)^{-1} Z^T y
$$




## Why is homoscedasticity important? {.smaller}

- Without homoscedasticity we would have

$$
L \neq \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$


- Therefore $\hat \beta$ would no longer maximize the likelihood!


- In this case $\hat \beta$ would still be an unbiased estimator for $\beta$

$$
\Expect [\hat \beta ] = \beta
$$



## Why is homoscedasticity important? {.smaller}

- However the quantity
$$
S^2 = \frac{ \RSS(\hat \beta) }{n-p}
$$
is not anymore unbiased estimator for the population variance $\sigma^2$
$$
\Expect[S^2] \neq \sigma^2
$$


- This is a problem because the estimated standard error for $\beta_j$ involves $S^2$
$$
\ese (\beta_j) = \xi_{jj}^{1/2} \, S  
$$

- Therefore $\ese$ becomes **unreliable**




## Why is homoscedasticity important? {.smaller}


- Then also t-statistic for significance of $\beta_j$ becomes unreliable

- This is because the t-statistic depends on $\ese$

$$
t = \frac{ \hat\beta_j - \beta_j }{ \ese } 
$$

- **Without homoscedasticity the regression maths does not work!**
    * t-tests for significance of $\beta_j$
    * confidence intervals for $\beta_j$
    * $F$-tests for model selection
    * They all break down and become unreliable!




## Is heteroscedastcity a serious problem? {.smaller}

- Heteroscedasticity in linear regression is no longer a big problem

- This is thanks to 1980s research on *robust standard errors* ([more info here](https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors))


- Moreover heteroscedasticity only becomes a problem when it is **severe**



## How to detect Heteroscedasticity {.smaller}

- Heteroscedasticity is commonly present in real-world datasets
    * We should be able to detect it


<br>

- There are formal tests (see [@gujarati_porter])
    * Goldfeldt-Quant test
    * White's test for heteroscedasticity



## How to detect Heteroscedasticity {.smaller}

- Alternative: **graphical checks**
    * Simpler and more robust

- They involve studying the model **residuals**

$$
e_i := y_i - \hat y_i
$$


- By definition $e_i$ is sampled from $\e_i$

- We have heteroscedasticity if 

$$
\Var [\e_i] \neq \Var [\e_j] \, \quad \, \text{ for some } \, i \neq j
$$

- Hence under heteroscedasticity the residuals $e_i$ have **different variance**







## Graphical checks {.smaller}
### First method: Histogram of residuals

- Yes Heteroscedasticity:
    * Residuals have different variance
    * Histogram will display **asymetric pattern**

- No Heteroscedasticity:
    * Homoscedasticity assumption holds
    * Residuals have same variance
    * Histogram will look like **normal** distribution $N(0,\sigma^2)$




## Interpretation of Histograms {.smaller}
### Left: Homoscedastic $\qquad\quad\quad$ Right: Heteroscedastic


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Plot histogram of transformed data
hist(data, 
     xlab = "", 
     ylab = "", 
     col = "skyblue", 
     border = "white")

# Increase label size
mtext("Residuals", side=1, line=2.5, cex=2.5)
```

:::


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

trend <- seq(-6, 3, length.out = length(data))

# Add quadratic trend
data <- data + trend ^ 2

# Plot histogram of transformed data
hist(data, 
     xlab = "", 
     ylab = "", 
     col = "skyblue", 
     border = "white")

# Increase label size
mtext("Residuals", side=1, line=2.5, cex=2.5)
```


:::





## Graphical checks {.smaller}
### Second method: Residual graphs

- Residual graphs are plots of
    * Residuals against fitted values
    * Squared residuals against fitted values


- Important:
    * No Heteroscedasticity: Plots will look **random**
    * Yes Heteroscedasticity: Plots will show certain **patterns**

- Good reference is the book [@draper_smith]




## Interepretation of Residual Graphs {.smaller}

::: {.column width="50%"}

- **No systematic pattern:**
    * Suggests no heteroscedasticity
    * Corresponds to constant variance 
    * Homoscedasticity assumption holds

- Residuals resemble sample $N(0,\sigma^2)$
    * About half residuals negative and half positive
    * Vertical spread is comparable

:::

::: {.column width="47%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```

:::


## Interepretation of Residual Graphs {.smaller}
### Patterns implying Heteroscedasticity

1. Funnelling out of residuals

2. Funnelling in of residuals

3. Linear residuals -- Proportional to $\hat y_i$

4. Quadratic residuals -- Proportional to $\hat{y}^2_i$


**In these special cases we can transform the data to avoid heteroscedasticity**





## Funnelling out of residuals {.smaller}


:::: {.columns}

::: {.column width="100%"}
![](images/funnellinplot.png){width=100%}
:::

::::



## Funnelling in of residuals {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/funnelloutplot.png){width=100%}
:::

::::





## Linear and Quadratic residuals


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

trend <- seq(-6, 3, length.out = length(data))

# Add linear trend
data <- data + trend

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```

:::


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

trend <- seq(-6, 3, length.out = length(data))

# Add quadratic trend
data <- data + trend ^ 2

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```


:::





## Remedial transformations {.smaller}

- To try and reduce heteroscedasticity we can 
    * transform the data $y$ 

- A tranformation which often helps is
    * $\, \log y$

- For linear and quadratic patterns you can try
    * $\, y^2$
    * $\, \sqrt{y}$




## Remedial transformations {.smaller}

- Heteroscedasticity can be associated with some of the $X$-variables
    * In this case plot the residuals or squared residuals against $X$

- The book [@gujarati_porter] discusses two cases

    * The error variance is proportional to $X^2_i$
    $$
    \Var [\e_i] \, \approx \, \sigma^2 \, X^2_i
    $$
    * The error variance is proportional to $X_i$
    $$
    \Var [\e_i] \, \approx \, \sigma^2 \, X_i
    $$

- **In each case divide through by the square root of the offending $X$-term**



## Error variance proportional to $X_i^2$ {.smaller}

-  Start with the model

$$
Y_i = \beta_1 + \beta_2 X_{i} + \e_i
$$

- Divide through by $X_i$

\begin{equation} \tag{1}
\frac{Y_i}{X_i} = \frac{\beta_1}{X_i}+\beta_2+\frac{\e_i}{X_i}
\end{equation}

- Estimate equation (1) with usual least squares regression approach




## Error variance proportional to $X_i$ {.smaller}

-  Start with the model

$$
Y_i = \beta_1 + \beta_2 X_{i} + \e_i
$$

- Divide through by $\sqrt{X_i}$

\begin{equation} \tag{2}
\frac{Y_i}{\sqrt{X_i}} = \frac{\beta_1}{\sqrt{X_i}}+\beta_2 \sqrt{X_i} + \frac{\e_i}{\sqrt{X_i}}
\end{equation}

- Estimate equation (2) with usual least squares regression approach




## Analysis of regression residuals in R {.smaller}

- We need R commands for **residuals** and **fitted values**

- Fit a linear model as usual

```r
# Fit a linear model
model <- lm(formula)
```

- To obtain fitted values $\hat y_i$ 

```r
# Compute fitted values
fitted.values <- model$fitted
```

- To obtain the residual values $\e_i = y_i - \hat y_i$

```r
# Compute residual values
residuals <- model$resid
```



## Example: Stock Vs Gold prices {.smaller}

- The full code for the example is available here [residual_graphs.R](codes/residual_graphs.R)

- Stock Vs Gold prices data is available here [stock_gold.txt](datasets/stock_gold.txt)

- Read data into R and fit simple regression

```r
# Load dataset on Stock Vs Gold prices
prices <- read.table(file = "stock_gold.txt",
                    header = TRUE)

# Store data-frame into 2 vectors
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]

# Fit regression model
model <- lm(gold.price ~ stock.price)
```


##  {.smaller}

- **Plot:** Residuals look heteroscedastic
    * Most points are below the line
    * Points under the line appear more distant 


::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Scatter plot
plot(stock.price, 
     gold.price, 
     xlab = "Stock Price", 
     ylab = "Gold Price",
     pch = 16) 

# Plot regression line
abline(model, 
      col = "red", 
      lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)

# Plot regression line
abline(model, col = "red", lwd = 3)
```

:::
:::::


##  {.smaller}

- **Histogram:** Confirms initial intuition of heteroscedasticity
    * Residuals are not normally distributed
    * Residuals have different variance (skewed histogram)

::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute residuals
residuals <- model$resid

# Histogram of residuals
hist(residuals,
xlab = "Residuals",
ylab = "Frequency",
col = "skyblue")
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Histogram of residuals
hist(residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::
:::::



## {.smaller}

- **Residual Graph:** Displays funnelling out pattern
    * We definitely have **heteroscedasticity**

::::: {.columns style='display: flex !important; height: 65%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "Fitted Values", 
     ylab = "Residuals",
     pch = 16)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::




##  {.smaller}

- **Remedial transformation:** To try and reduce heteroscedasticity take
    * $\, \log y$

- This means we need to fit the model

$$
\log Y_i = \alpha + \beta X_i + \e_i
$$


```r 
# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted
```



# {.smaller}

- Heteroscedasticity has definitely reduced
    * Left: Residual plot for original model
    * Right: Residual plot for $\log y$ data model



::::: {.columns style='display: flex !important; height: 50%;'}

::: {.column width="47%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="47%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted

# Plot the residual graph
plot(log.fitted, log.residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::



# {.smaller}

- Heteroscedasticity has definitely reduced
    * Left: Histogram of residuals for original model
    * Right: Histogram of residuals for $\log y$ data model



::::: {.columns style='display: flex !important; height: 50%;'}

::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Histogram of residuals
hist(residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::

::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted

# Histogram of residuals
hist(log.residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::
:::::





# Part 2: <br>Autocorrelation {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Autocorrelation {.smaller}

- The general linear regression model is 

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- Consider Assumption 5
    * **Independence:** Errors $\e_1, \ldots, \e_n$ are independent and thus uncorrelated
    $$
    \Cor(\e_i , \e_j) = 0 \qquad \forall \,\, i \neq j
    $$

- **Autocorrelation:** The violation of Assumption 5

$$
\Cor(\e_i , \e_j) = 0 \qquad \text{ for some } \,\, i \neq j
$$




## Why is independence important? {.smaller}

- Recall the methods to assess linear models 

    * Coefficient $R^2$
    * $t$-tests for parameters significance
    * $F$-test for model selection

- The above methods **rely heavily** on independence



## Why is independence important? {.smaller}

- Once again let us consider the likelihood calculation
    \begin{align*}
    L & = f(y_1, \ldots, y_n) =  \prod_{i=1}^n  f_{Y_i} (y_i)  
         \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{\sum_{i=1}^n(y_i- \hat y_i)^2}{2\sigma^2}      \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
    \end{align*}


- The second equality is only possible thanks to independence of 

$$
Y_1 , \ldots, Y_n
$$



## Why is independence important? {.smaller}

- If we have **autocorrelation** then

$$
\Cor (\e_i,\e_j) \neq 0 \quad \text{ for some } \, i \neq j
$$

- In particualar we would have

$$
\e_i \, \text{ and } \, \e_j \, \text{ dependent } \quad \implies \quad Y_i \, \text{ and } \, Y_j \, \text{ dependent }
$$


- Therefore the calculation in previous slide breaks down

$$
L \neq \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$



## Why is independence important? {.smaller}

- In this case $\hat \beta$ does no longer maximize the likelihood!

- As already seen, this implies that 

$$
\ese (\beta_j) \,\, \text{ is unreliable}
$$

- **Without independence the regression maths does not work!**
    * t-tests for significance of $\beta_j$
    * confidence intervals for $\beta_j$
    * $F$-tests for model selection
    * They all break down and become unreliable!




## Causes of Autocorrelation {.smaller}
### Time-series data

- Autocorrelation means that

$$
\Cor (\e_i,\e_j) \neq 0 \quad \text{ for some } \, i \neq j
$$

- Autocorrelation if often unavoidable

- Typically associated with **time series data**
    * Observations ordered wrt time or space are usually correlated
    * This is because observations taken close together may take similar values


## Causes of Autocorrelation {.smaller}
### Financial data

- Autocorrelation is especially likely for datasets in 
    * Accounting
    * Finance
    * Economics

- Autocorrelation is likely if the data have been recorded over time
    * E.g. daily, weekly, monthly, quarterly, yearly

- Example: Datasetet on *Stock prices* and *Gold prices*
    * General linear regression model assumes uncorrelated errors
    * Not realistic to assume that price observations for say 2020 and 2021 would be independent


## Causes of Autocorrelation  {.smaller}
### Inertia

- Economic time series tend to exhibit **cyclical behaviour**

- Examples include GNP, price indices, production figures, employment statistics etc.

- Since these series tend to be quite slow moving
    * Effect of inertia is that successive observations are highly correlated

**This is an extremely common phenomenon in financial and economic time series**


## Causes of Autocorrelation  {.smaller}
### Cobweb Phenomenon

- Characteristic of industries in which a large amount of time passes between
    * the decision to produce something 
    * and its arrival on the market

- Cobweb phenomenon is common with agricultural commodities

- Economic agents (e.g. farmers) decide
    * how many goods to supply to the market
    * based on previous year price 


## Causes of Autocorrelation  {.smaller}
### Cobweb Phenomenon

- **Example:** the amount of crops farmers supply to the market at time $t$ might be

\begin{equation} \tag{3}
{\rm Supply}_t = \beta_1 + \beta_2 \, {\rm Price}_{t-1} + \e_t
\end{equation}

- Errors $\e_t$ in equation (3) are unlikely to be completely random and patternless

- This is because they represent actions of intelligent economic agents (e.g. farmers)

**Error terms are likely to be autocorrelated**



## Causes of Autocorrelation  {.smaller}
### Data manipulation


**Examples:** 

- Quarterly data may smooth out the wild fluctuations in monthly sales figures

- Low frequency economic survey data may be interpolated

**However:** Such data transformations may be inevitable

- In social sciences data quality may be variable

- This may induce systematic patterns and autocorrelation
    

**No magic solution -- Autocorrelation is unavoidable and must be considered**




## Detection of autocorrelation {.smaller}

- Statistical tests
    * Runs test
    * Durbin-Watson test

- Graphical methods
    * Simpler but can be more robust and more informative

**Graphical and statistical methods can be useful cross-check of each other!**




## Graphical tests for autocorrelation {.smaller}


- Time-series plot of residuals
    * Plot residuals $e_t$ over time
    * Check to see if any evidence of a systematic pattern exists

- Autocorrelation plot of residuals
    * Natural to think that $\e_t$ and $\e_{t-1}$ may be correlated
    * Plot residual $e_t$ against $e_{t-1}$


- Important:
    * No Autocorrelation: Plots will look **random**
    * Yes Autocorrelation: Plots will show certain **patterns**



## Graphical tests in R {.smaller}

- Code for this example is available here [autocorrelation_graph_tests.R](codes/autocorrelation_graph_tests.R)

- Stock Vs Gold prices data is available here [stock_gold.txt](datasets/stock_gold.txt)

- Read data into R and fit simple regression

```r
# Load dataset
prices <- read.table(file = "stock_gold.txt",
                    header = TRUE)

# Store data-frame into 2 vectors
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]

# Fit regression model
model <- lm(gold.price ~ stock.price)
```




##  {.smaller}

- **Time-series plot of residuals**
    * Time series plot suggests some evidence for autocorrelation
    * Look for successive runs of residuals either side of line $y = 0 \,$ (see $t = 15$)



::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute residuals
residuals <- model$resid

# Time-series plot of residuals
plot(residuals,
     xlab = "Time", 
     ylab = "Residuals",
     pch = 16,
     cex = 1.5)

# Add line y = 0 for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Time-series plot of residuals
plot(residuals, 
     xlab = "", 
     ylab= "",
     pch = 16,
     cex = 1.5)

# Add labels
mtext("Time", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

# Add line y = 0 for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::




## {.smaller}


::::: {.columns style='display: flex !important; height: 90%;'}

::: {.column width="27%" style='display: flex; align-items: center;'}

- **Autocorrelation plot of residuals**
    * Want to plot $e_t$ against $e_{t-1}$
    * Shift $e_t$ by 1 to get $e_{t-1}$
    * Can only plot magenta pairs
    * We have 1 pair less than number of residuals


:::

::: {.column width="72%" style='display: flex; justify-content: center; align-items: center;'}

![](images/shifted_vectors.png){width=110%}
![](images/shifted_vectors_2.png){width=110%}

:::
:::::





##  {.smaller}


- We want to plot $e_t$ against $e_{t-1}$
    * Residuals are stored in vector $\,\, \texttt{residuals}$
    * We need to create a shifted version of $\,\, \texttt{residuals}$
    * First compute the length of $\,\, \texttt{residuals}$

```r
# Compute length of residuals
length(residuals)
```
```{r}
# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute length of residuals
length(residuals)
```


- Need to generate the $33-1$ pairs for plotting


##  {.smaller}


- **Lag 0:** 
    * This is the original vector with no lag
    * Lose one observation from $\,\, \texttt{residuals}$ -- the first observation

```r 
residuals.lag.0 <- residuals[2:33]
```


- **Lag 1:** 
    * This is the original vector shifted by 1
    * Lose one observation from $\,\, \texttt{residuals}$ -- the last observation

```r 
residuals.lag.1 <- residuals[1:32]
```



##  {.smaller}

- **Autocorrelation plot of residuals**
    * Plot suggests positive autocorrelation of residuals
    * This means $\, e_t \, \approx \, a + b \, e_{t-1} \,$ with $b > 0$ 
    

::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Plot Lag0 Vs Lag1 residuals

plot(residuals.lag.0, 
     residuals.lag.1,
     xlab = "Residuals Lag 0", 
     ylab = "Residuals Lag 1",
     pch = 16,
     cex = 1.5)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Generate lagged version of residuals
residuals.lag.0 <- residuals[2:33]
residuals.lag.1 <-residuals[1:32]

# Plot Lag0 Vs Lag1 residuals
plot(residuals.lag.0, 
     residuals.lag.1,
     xlab = "", 
     ylab= "",
     pch = 16,
     cex = 1.5)

# Add labels
mtext("Residuals Lag 0", side=1, line=3, cex=2.1)
mtext("Residuals Lag 1", side=2, line=2.5, cex=2.1)
```

:::
:::::





## Statistical tests for Autocorrelation {.smaller}

- Runs test
    * Under the classical multiple linear regression model residuals are equally likely to be positive or negative

- Durbin-Watson test
    * Test to see if residuals are AR(1)
 

- We do not cover these


## What to do in case of Autocorrelation? {.smaller}

- Consider the simple regression model

$$
Y_i = \alpha + \beta x_i + \e_i
$$

- Suppose that autocorrelation occurs

$$
\Cor (\e_i, \e_j ) \neq 0 \quad \text{ for some } \, i \neq j
$$

- Also suppose that autocorrelation is linear in nature

$$
e_t \, \approx \, a + b \, e_{t-1} \quad \text{ for some } \,\, a , \, b \in \R 
$$

- This was for example the case of *Stock prices* Vs *Gold prices*


## What to do in case of Autocorrelation? {.smaller}

- In this case the simple linear model is not the right thing to consider

- The right thing to do is consider **Autoregressive linear models**

$$
Y_t = \alpha + \beta x_{t} + \e_t
$$

- These models couple regression with time-series analysis (ARIMA models)

- Good reference is book by Shumway and Stoffer [@shumway]




::: {.content-hidden}

Do next time

- Statistical tests for autocorrelation

- This is Lecture 9 John Fry slides Section 4 onwards

- Do also ARIMA for time series (good reference is Shumway, Stoffer - Time Series Analysis and Its Applications 4th edition)

- Below is Rcode for Lecture 9 

#R example Section 4
#need to load the tseries package
residsign<-1*(resid01>0)
residsign<-factor(residsign)
runs.test(residsign)
#durbin-watson example
#need to load the lmtest package
dwtest(a.lm)
#R example Section 5
arima(realgoldprice, xreg=realstockprice, order=c(1, 0, 0))
coeff<-c(0.5578,     3.9406,         -0.0487)
ese<-c(0.1545,     0.5675,          0.0247)
t<-abs(coeff)/ese
2*(1-pt(t, 30))
#tutorial exercise
#part 1
oil<-c(8.597, 8.572, 8.649, 8.688, 8.879, 8.971, 8.680, 8.349, 8.140, 7.613, 7.355, 7.417, 7.171, 6.847, 6.662, 6.560, 6.465, 6.452, 6.252, 5.881, 5.882)
gas<-c(1770, 1780, 1805, 1865, 1915, 1910, 1915, 1960, 1980, 1795, 1750, 1630, 1535, 1329, 1241, 1169, 1006, 967, 940, 907, 840)
#part 2
gas.lm<-lm(gas~oil)
summary(gas.lm)
#part 3
plot(gas.lm$resid)
length(gas.lm$resid)
lines(seq(1:21), rep(0, 21))
length(gas.lm$resid)
residl1<-gas.lm$resid[1:20]
residl0<-gas.lm$resid[2:21]
plot(residl1, residl0, xlab="Lag 1 residuals", ylab="Lag 0 residuals") 
#part 4
#need to make sure you have loaded the tseries package in order to get this to work
posres<-1*(gas.lm$resid>0)
posres<-factor(posres)
runs.test(posres)
#part 5
#need to make sure you have loaded the lmtest package in order to get this to work
dwtest(gas.lm)
#part 6
arima(gas, xreg=oil, order=c(1, 0, 0))
#part 7
#Look for the same interpretation about autocorrelation coming from the two graphical tests and the three numerical tests
#using the values generated from Part 6 above
0.8978/0.0989
#degrees of freedom=sample size-number of estimated parameters
21-3
2*(1-pt(9.077856, 18))
#part 8
#To see the ordinary least squares equation use either
gas.lm
#or better would be
summary(gas.lm)

:::




## References