---
title: "Statistical Models"
subtitle: "Lecture 11"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 11: <br>Violation of regression <br> assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 11

1. Regression modelling assumptions
2. Heteroscedasticity
3. Autocorrelation
4. Multicollinearity




# Part 1: <br>Regression assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Regression modelling assumptions {.smaller}

In Lecture 9 we have introduced the general linear regression model

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- There are $p$ predictor random variables

$$
Z_1 \, , \,\, \ldots \, , \, Z_p
$$


- $Y_i$ is the conditional distribution

$$
Y | Z_1 = z_{i1} \,, \,\, \ldots \,, \,\, Z_p = z_{ip}
$$

- The errors $\e_i$ are random variables 



## Regression assumptions on $Y_i$ {.smaller}

1. **Predictor is known:** The values $z_{i1}, \ldots, z_{ip}$ are known

2. **Normality:** The distribution of $Y_i$ is normal

3. **Linear mean:** There are parameters $\beta_1,\ldots,\beta_p$ such that
$$
\Expect[Y_i] = \beta_1 z_{i1} + \ldots + \beta_p z_{ip}  
$$

4. **Homoscedasticity:** There is a parameter $\sigma^2$ such that
$$
\Var[Y_i] = \sigma^2
$$

5. **Independence:** rv $Y_1 , \ldots , Y_n$ are independent and therefore uncorrelated

$$
\Cor (Y_i,Y_j) = 0 \qquad \forall \,\, i \neq j
$$




## Equivalent assumptions on $\e_i$ {.smaller}

1. **Predictor is known:** The values $z_{i1}, \ldots, z_{ip}$ are known

2. **Normality:** The distribution of $\e_i$ is normal

3. **Linear mean:** The errors have zero mean
$$
\Expect[\e_i] = 0
$$

4. **Homoscedasticity:** There is a parameter $\sigma^2$ such that
$$
\Var[\e_i] = \sigma^2
$$

5. **Independence:** Errors $\e_1 , \ldots , \e_n$ are independent and therefore uncorrelated

$$
\Cor (\e_i, \e_j) = 0 \qquad \forall \,\, i \neq j
$$


## Extra assumption on design matrix {.smaller}


6. The design matrix $Z$ is such that

$$
Z^T Z  \, \text{ is invertible}
$$


- Assumptions 1-6 allowed us to estimate the parameters

$$
\beta = (\beta_1, \ldots, \beta_p)
$$

- By maximizing the likelihood we obtained estimator

$$
\hat \beta = (Z^T Z)^{-1} Z^T y
$$



## Violation of Assumptions {.smaller}
### We consider 3 scenarios

i. **Heteroscedasticity:** The violation of Assumption 4 of homoscedasticity

$$
\Var [\e_i] \neq \Var[\e_j] \qquad \text{ for some } \,\, i \neq j
$$

ii. **Autocorrelation:** The violation of Assumption 5 of no-correlation

$$
\Cor( \e_i, \e_j ) \neq 0  \qquad \text{ for some } \,\, i \neq j
$$


iii. **Multicollinearity:** The violation of Assumption 6 of invertibilty of the matrix

$$
Z^T Z
$$




# Part 2: <br>Heteroscedasticity {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Heteroscedasticity {.smaller}

- The general linear regression model is 

$$
Y_i = \beta_1 z_{i1} + \beta_2 z_{i2} + \ldots + \beta_p z_{ip} + \e_i
$$


- We now consider the violation of Assumption 4
    * **Homoscedasticity:** There is a parameter $\sigma^2$ such that
    $$
    \Var[\e_i] = \sigma^2 \qquad \forall \,\, i
    $$

- If Assumption 4 is violated we have **heteroscedasticity**

$$
\Var [\e_i] \neq \Var[\e_j] \qquad \text{ for some } \,\, i \neq j
$$




## Why is homoscedasticity important? {.smaller}

- In Lecture 10 we presented a few methods to assess linear models 

    * Coefficient $R^2$
    * $t$-tests for parameters significance
    * $F$-test for model selection

- The above methods however **rely heavily** on homoscedasticity




## Why is homoscedasticity important? {.smaller}

- For example the maximum likelihood estimation relied on the calculation
    \begin{align*}
    L & = \prod_{i=1}^n  f_{Y_i} (y_i)  
        = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left( -\frac{(y_i - \hat y_i)^2}{2\sigma^2} \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{\sum_{i=1}^n(y_i- \hat y_i)^2}{2\sigma^2}      \right) \\[15pts]
      & = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
    \end{align*}


- The calculation is only possible thanks to homoscedasticity

$$
\Var[Y_i] = \sigma^2 \qquad \forall \,\, i
$$



## Why is homoscedasticity important? {.smaller}

- Suppose the calculation in previous slide holds

$$
L = \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$


- Then maximizing the likelihood is equivalent to solving

$$
\min_{\beta} \ \RSS
$$

- The above has the closed form solution

$$
\hat \beta = (Z^T Z)^{-1} Z^T y
$$




## Why is homoscedasticity important? {.smaller}

- Without homoscedasticity we would have

$$
L \neq \frac{1}{(2\pi \sigma^2)^{n/2}} \, \exp   \left(   -\frac{ \RSS }{2\sigma^2}      \right)
$$


- Therefore $\hat \beta$ would no longer maximize the likelihood!


- In this case $\hat \beta$ would still be an unbiased estimator for $\beta$

$$
\Expect [\hat \beta ] = \beta
$$



## Why is homoscedasticity important? {.smaller}

- However the quantity
$$
S^2 = \frac{ \RSS(\hat \beta) }{n-p}
$$
is not anymore unbiased estimator for the population variance $\sigma^2$
$$
\Expect[S^2] \neq \sigma^2
$$


- This is a problem because the estimated standard error for $\beta_j$ involves $S^2$
$$
\ese (\beta_j) = \xi_{jj}^{1/2} \, S  
$$

- Therefore $\ese$ becomes **unreliable**




## Why is homoscedasticity important? {.smaller}


- Then also t-statistic for significance of $\beta_j$ becomes unreliable

- This is because the t-statistic depends on $\ese$

$$
t = \frac{ \hat\beta_j - \beta_j }{ \ese } 
$$

- **Without homoscedasticity the regression maths does not work!**
    * t-tests for significance of $\beta_j$
    * confidence intervals for $\beta_j$
    * $F$-tests for model selection
    * They all break down and become unreliable!




## Is heteroscedastcity a serious problem? {.smaller}

- Heteroscedasticity in linear regression is no longer a big problem

- This is thanks to 1980s research on *robust standard errors* ([more info here](https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors))


- Moreover heteroscedasticity only becomes a problem when it is **severe**



## How to detect Heteroscedasticity {.smaller}

- Heteroscedasticity is commonly present in real-world datasets
    * We should be able to detect it


<br>

- There are formal tests (see [@gujarati_porter])
    * Goldfeldt-Quant test
    * White's test for heteroscedasticity



## How to detect Heteroscedasticity {.smaller}

- Alternative: **graphical checks**
    * Simpler and more robust

- They involve studying the model **residuals**

$$
r_i := y_i - \hat y_i
$$


- By definition $r_i$ is sampled from $\e_i$

- We have heteroscedasticity if 

$$
\Var [\e_i] \neq \Var [\e_j] \, \quad \, \text{ for some } \, i \neq j
$$

- This implies the residuals $r_i$ have **different variance**







## Graphical checks {.smaller}
### First method: Histogram of residuals

- If heteroscedasticity is present then: 
    * Residuals have different variance
    * Histogram will display asymetric pattern

- If no heteroscedasticity is present then: 
    * Homoscedasticity assumption holds
    * Residuals have same variance
    * Histogram will look like normal distribution $N(0,\sigma^2)$




## Interpretation of Histograms {.smaller}
### Left: Homoscedastic $\qquad\quad\quad$ Right: Heteroscedastic


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Plot histogram of transformed data
hist(data, 
     xlab = "", 
     ylab = "", 
     col = "skyblue", 
     border = "white")

# Increase label size
mtext("Residuals", side=1, line=2.5, cex=2.5)
```

:::


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

trend <- seq(-6, 3, length.out = length(data))

# Add quadratic trend
data <- data + trend ^ 2

# Plot histogram of transformed data
hist(data, 
     xlab = "", 
     ylab = "", 
     col = "skyblue", 
     border = "white")

# Increase label size
mtext("Residuals", side=1, line=2.5, cex=2.5)
```


:::





## Graphical checks {.smaller}
### Second method: Residual graphs

- Residual graphs are plots of
    * Residuals against fitted values
    * Squared residuals against fitted values

- There are certain common patterns associated with heteroscedasticity
    * Important to recognize such patterns
    * More on this in the book [@draper_smith]




## Interepretation of Residual Graphs {.smaller}

::: {.column width="50%"}

- **No systematic pattern:**
    * Suggests no heteroscedasticity
    * Corresponds to constant variance 
    * Homoscedasticity assumption holds

- Residuals resemble sample $N(0,\sigma^2)$
    * About half residuals negative and half positive
    * Vertical spread is comparable

:::

::: {.column width="47%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```

:::


## Interepretation of Residual Graphs {.smaller}
### Patterns implying Heteroscedasticity

1. Funnelling out of residuals

2. Funnelling in of residuals

3. Linear residuals -- Proportional to $\hat y_i$

4. Quadratic residuals -- Proportional to $\hat{y}^2_i$


**In these special cases we can transform the data to avoid heteroscedasticity**





## Funnelling out of residuals {.smaller}


:::: {.columns}

::: {.column width="100%"}
![](images/funnellinplot.png){width=100%}
:::

::::



## Funnelling in of residuals {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/funnelloutplot.png){width=100%}
:::

::::





## Linear and Quadratic residuals


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

trend <- seq(-6, 3, length.out = length(data))

# Add linear trend
data <- data + trend

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```

:::


::: {.column width="48%"}
```{r}
#| echo: false
#| fig-asp: 1

# Generate random N(0,1) vector
data <- rnorm(300)

# Threshold at 2.5 standard deviation for 
# better plot

data <- data[abs(data) <= 2.5]

trend <- seq(-6, 3, length.out = length(data))

# Add quadratic trend
data <- data + trend ^ 2

plot(data, 
    xlab = "", 
    ylab = "", 
    pch = 16)

mtext("Fitted Values", side=1, line=3, cex=2.1)
mtext("Residuals", side=2, line=2.5, cex=2.1)

abline(0, 0, col = "red", lwd = 3)
```


:::





## Remedial transformations {.smaller}

- To try and reduce heteroscedasticity we can 
    * transform the data $y$ 

- A tranformation which often helps is
    * $\, \log y$

- For linear and quadratic patterns you can try
    * $\, y^2$
    * $\, \sqrt{y}$




## Remedial transformations {.smaller}

- Heteroscedasticity can be associated with some of the $X$-variables
    * In this case plot the residuals or squared residuals against $X$

- The book [@gujarati_porter] discusses two cases

    * The error variance is proportional to $X^2_i$
    $$
    \Var [\e_i] \, \approx \, \sigma^2 \, X^2_i
    $$
    * The error variance is proportional to $X_i$
    $$
    \Var [\e_i] \, \approx \, \sigma^2 \, X_i
    $$

- **In each case divide through by the square root of the offending $X$-term**



## Error variance proportional to $X_i^2$ {.smaller}

-  Start with the model

$$
Y_i = \beta_1 + \beta_2 X_{i} + \e_i
$$

- Divide through by $X_i$

\begin{equation} \tag{1}
\frac{Y_i}{X_i} = \frac{\beta_1}{X_i}+\beta_2+\frac{\e_i}{X_i}
\end{equation}

- Estimate equation (1) with usual least squares regression approach




## Error variance proportional to $X_i$ {.smaller}

-  Start with the model

$$
Y_i = \beta_1 + \beta_2 X_{i} + \e_i
$$

- Divide through by $\sqrt{X_i}$

\begin{equation} \tag{2}
\frac{Y_i}{\sqrt{X_i}} = \frac{\beta_1}{\sqrt{X_i}}+\beta_2 \sqrt{X_i} + \frac{\e_i}{\sqrt{X_i}}
\end{equation}

- Estimate equation (2) with usual least squares regression approach




## Analysis of regression residuals in R {.smaller}

- We need R commands for **residuals** and **fitted values**

- Fit a linear model as usual

```r
# Fit a linear model
model <- lm(formula)
```

- To obtain fitted values $\hat y_i$ 

```r
# Compute fitted values
fitted.values <- model$fitted
```

- To obtain the residual values $\e_i = y_i - \hat y_i$

```r
# Compute residual values
residuals <- model$resid
```



## Example: Stock Vs Gold prices {.smaller}

- The full code for the example is available here [residual_graphs.R](codes/residual_graphs.R)

- Stock Vs Gold prices data is available here [stock_gold.txt](datasets/stock_gold.txt)

- Read data into R and fit simple regression

```r
# Load dataset on Stock Vs Gold prices
prices <- read.table(file = "stock_gold.txt",
                    header = TRUE)

# Store data-frame into 2 vectors
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]

# Fit regression model
model <- lm(gold.price ~ stock.price)
```


##  {.smaller}

- **Plot:** Residuals look heteroscedastic
    * Most points are below the line
    * Points under the line appear more distant 


::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Scatter plot
plot(stock.price, 
     gold.price, 
     xlab = "Stock Price", 
     ylab = "Gold Price",
     pch = 16) 

# Plot regression line
abline(model, 
      col = "red", 
      lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)

# Plot regression line
abline(model, col = "red", lwd = 3)
```

:::
:::::


##  {.smaller}

- **Histogram:** Confirms initial intuition of heteroscedasticity
    * Residuals are not normally distributed
    * Residuals have different variance (skewed histogram)

::::: {.columns style='display: flex !important; height: 60%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute residuals
residuals <- model$resid

# Histogram of residuals
hist(residuals,
xlab = "Residuals",
ylab = "Frequency",
col = "skyblue")
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Histogram of residuals
hist(residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::
:::::



## {.smaller}

- **Residual Graph:** Displays funnelling out pattern
    * We definitely have **heteroscedasticity**

::::: {.columns style='display: flex !important; height: 65%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

```r
# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "Fitted Values", 
     ylab = "Residuals",
     pch = 16)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::




##  {.smaller}

- **Remedial transformation:** To try and reduce heteroscedasticity take
    * $\, \log y$

- This means we need to fit the model

$$
\log Y_i = \alpha + \beta X_i + \e_i
$$


```r 
# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted
```



# {.smaller}

- Heteroscedasticity has definitely improved
    * Left: Residual plot for original model
    * Right: Residual plot for $\log y$ data model



::::: {.columns style='display: flex !important; height: 50%;'}

::: {.column width="47%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Plot the residual graph
plot(fitted, residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::

::: {.column width="47%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted

# Plot the residual graph
plot(log.fitted, log.residuals,
     xlab = "", 
     ylab = "",
     pch = 16)

# Increase label size
mtext("Fitted Values", side = 1, line = 2.5, cex = 2.5)
mtext("Residuals", side = 2, line = 2.5, cex = 2.5)

# Add y = 0 line for reference
abline(0, 0, col = "red", lwd = 3)
```

:::
:::::



# {.smaller}

- Heteroscedasticity has definitely improved
    * Left: Histogram of residuals for original model
    * Right: Histogram of residuals for $\log y$ data model



::::: {.columns style='display: flex !important; height: 50%;'}

::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit regression model
model <- lm(gold.price ~ stock.price)

# Compute residuals
residuals <- model$resid

# Compute fitted values
fitted <- model$fitted

# Histogram of residuals
hist(residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::

::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Transform data via log(y)
log.gold.price <- log(gold.price)

# Fit linear model with log(y) data
log.model <- lm(log.gold.price ~ stock.price)

# Compute residuals for log model
log.residuals <- log.model$resid

# Compute fitted values for log model
log.fitted <- log.model$fitted

# Histogram of residuals
hist(log.residuals,
xlab = "",
ylab = "",
col = "skyblue")

# Increase label size
mtext("Residuals", side = 1, line = 2.5, cex = 2.5)
mtext("Frequency", side = 2, line = 2.5, cex = 2.5)
```

:::
:::::




# Part 2: <br>Autocorrelation {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::


## TBA {.smaller}



# Part 3: <br>Multicollinearity {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::


## TBA {.smaller}




## References