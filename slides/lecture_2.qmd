---
title: "Statistical Models"
subtitle: "Lecture 2"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 2: <br>Multivariate distributions<br>{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 2

1. Bivariate random vectors
2. Conditional distributions
3. Independence
4. Covariance and correlation
5. Multivariate random vectors
6. Unbiased estimators










## Univariate vs Bivariate vs Multivariate {.smaller}

- Probability models seen so far only involve 1 random variable
    * These are called **univariate models**

- We are also interested in probability models involving multiple variables:
    * Models with 2 random variables are called **bivariate**
    * Models with more than 2 random variables are called **multivariate**




## Random vectors {.smaller}
### Definition

**Recall**: a random variable is a **measurable** function
$$
X \colon \Omega \to \R \,, \quad \Omega \text{ sample space}
$$

::: Definition

A **random vector** is a measurable function $X \colon \Omega \to \R^n$. We say that

- $X$ is **univariate** if $n=1$
- $X$ is **bivariate** if $n=2$
- $X$ is **multivariate** if $n \geq 3$

:::





## Random vectors {.smaller}
### Notation

- The components of a random vector $X$ are denoted by
$$
X = (X_1, \ldots, X_n) 
$$
with $X_i \colon \Omega \to \R$ random variables


- We denote a two-dimensional bivariate random vector by 
$$
(X,Y)
$$
with $X,Y \colon \Omega \to \R$ random variables






# Part 1: <br>Bivariate random vectors {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Discrete random vectors {.smaller}
### Main definitions

::: Definition

The (bivariate) random vector $(X,Y)$ is **discrete** if $X$ and $Y$ are discrete random variables

:::

::: Definition

The **joint probability mass function** or **joint pmf** of a discrete
random vector $(X,Y)$ is the function
$$
f_{X,Y}(x,y) := P( \{X=x \} \cap \{ Y=y \}) 
$$

:::

**Notation**: $P(X=x, Y=y ) := P( \{X=x \} \cap \{ Y=y \})$



## Discrete random vectors {.smaller}
### Computing probabilities

- The joint pmf can be used to compute the probability of $A \subset \R^2$
\begin{align*}
P((X,Y) \in A) & := P( \{ \omega \in \Omega  \colon ( X(\omega), Y(\omega) ) \in A  \} ) \\ 
               &  = \sum_{(x,y) \in A} f_{X,Y} (x,y)
\end{align*}

- In particular we obtain
$$
 \sum_{(x,y) \in \R^2} f_{X,Y} (x,y) = 1
$$



## Discrete random vectors {.smaller}
### Expected value

- Suppose $(X,Y) \colon \Omega \to \R^2$ random vector and $g \colon \R^2 \to \R$ function
- Then $g(X,Y) \colon \Omega \to \R$ is random variable


::: Definition

The **expected value** of the random variable $g(X,Y)$ is
$$
\Expect[g(X,Y)] := \sum_{(x,y) \in \R^2} g(x,y) f_{X,Y}(x,y) = \sum_{(x,y) \in \R^2} g(x,y) P(X=x,Y=y)
$$

:::



## Discrete random vectors {.smaller}
### Marginals


::: Definition

Let $(X,Y)$ be a discrete random vector.
The **marginal pmfs** of $X$ and $Y$ are the functions
$$
f_X (x) := P(X = x) \quad \text{ and } \quad 
f_Y(y) := P(Y = y)
$$

:::

**Note**: The **marginal pmfs** of $X$ and $Y$ are just the usual **pmfs** of $X$ and $Y$




## Discrete random vectors {.smaller}
### Marginals


Marginals of $X$ and $Y$ can be computed from the joint pmf $f_{X,Y}$ 

::: Theorem

Let $(X,Y)$ be a discrete random vector with joint pmf $f_{X,Y}$.
The **marginal pmfs** of $X$ and $Y$ are given by
$$
f_X(x) = \sum_{y \in \R} f_{X,Y}(x,y) \quad \text{ and }
\quad 
f_Y(y) = \sum_{x \in \R} f_{X,Y}(x,y)
$$

:::



## Example - Discrete random vector {.smaller}
### Setting

- Consider experiment of tossing two dice. Then sample space is
$$
\Omega = \{ (m,n)  \colon m,n \in \{1,\ldots,6\}  \}
$$
with $m$ and $n$ being the outcomes of first and second dice, respectively
- We assume that the dice are fair. Therefore $P(\{(m,n)\})=1/36$
- Define the discrete random variables
\begin{align*}
X(m,n) & := m + n   \quad   &  \text{ sum of the dice} \\
Y(m,n) & := | m - n|  \quad &  \text{ |difference of the dice|}
\end{align*}
- For example
$$
X(3,3) = 3 + 3 = 6  \qquad \qquad  Y(3,3) = |3 - 3| = 0
$$



## Example - Discrete random vector {.smaller}
### Computing joint pmf

- To compute joint pmf one needs to consider all the cases
$$
f_{X,Y}(x,y) = P(X=x,Y=y) \,, \quad (x,y) \in \R^2
$$

- For example $X=4$ and $Y=0$ is only obtained for $(2,2)$. Hence
$$
f_{X,Y}(4,0) = P(X=4,Y=0) = P(\{(2,2)\}) = \frac{1}{6} \cdot  \frac{1}{6} = \frac{1}{36}
$$

- Similarly $X=5$ and $Y=2$ is only obtained for $(4,1)$ and $(1,4)$. Thus
$$
f_{X,Y}(5,2) = P(X=5,Y=2) = P(\{(4,1)\} \cup \{(1,4)\}) 
= \frac{1}{36} + \frac{1}{36} = \frac{1}{18} 
$$



## Example - Discrete random vector {.smaller}
### Computing joint pmf


- $f_{X,Y}(x,y)=0$ for most of the pairs $(x,y)$. Indeed $f_{X,Y}(x,y)=0$ if
$$
x \notin X(\Omega)  \quad \text{ or } \quad  y \notin Y(\Omega)
$$

- We have $X(\Omega)=\{2,3,4,5,6,7,8,9,10,11,12\}$

- We have $Y(\Omega)=\{0,1,2,3,4,5\}$

- Hence $f_{X,Y}$ only needs to be computed for pairs $(x,y)$ satisfying
$$
 2 \leq x \leq 12  \quad \text{ and } \quad  0 \leq y \leq 5
$$

- Within this range, other values will be zero. For example
$$
f_{X,Y}(3,0) = P(X=3,Y=0) = P(\emptyset) = 0
$$




## Example - Discrete random vector {.smaller}
### Table of values of joint pmf

Below are all the values for $f_{X,Y}$. Empty entries correspond to $f_{X,Y}(x,y) = 0$

|    |    |    |    |    |    |    |$x$ |    |    |    |    |    |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|    |    | 2  | 3  | 4  | 5  | 6  | 7  |  8 |  9 |  10| 11 | 12 |
|    | 0  |1/36|    |1/36|    |1/36|    |1/36|    |1/36|    |1/36|
|    | 1  |    |1/18|    |1/18|    |1/18|    |1/18|    |1/18|    |
|$y$ | 2  |    |    |1/18|    |1/18|    |1/18|    |1/18|    |    |
|    | 3  |    |    |    |1/18|    |1/18|    |1/18|    |    |    |
|    | 4  |    |    |    |    |1/18|    |1/18|    |    |    |    |
|    | 5  |    |    |    |    |    |1/18|    |    |    |    |    |




## Example - Discrete random vector {.smaller}
### Expected value

- We want to compute $\Expect[XY]$
- Hence consider the function $g(x,y):=xy$
- We obtain
\begin{align*}
\Expect[XY] & = \Expect[g(X,Y)] \\
            & = \sum_{(x,y) \in \R^2} g(x,y) f_{X,Y}(x,y)\\
            & = \sum_{(x,y) \in \R^2} xy f_{X,Y}(x,y)
\end{align*}



## Example - Discrete random vector {.smaller}
### Expected value

We can use the non-zero entries in the table for $f_{X,Y}$ to compute:
\begin{align*}
\Expect[XY] & = 3 \cdot 1 \cdot \frac{1}{18} + 5 \cdot 1 \cdot \frac{1}{18} + 7 \cdot 1 \cdot \frac{1}{18} + 
9 \cdot 1 \cdot \frac{1}{18} +  11 \cdot 1 \cdot \frac{1}{18} \\
            & + 4 \cdot 2 \cdot \frac{1}{18} + 
                6 \cdot 2 \cdot \frac{1}{18} +
                8 \cdot 2 \cdot \frac{1}{18} +
                10\cdot 2 \cdot \frac{1}{18} \\
            & + 5 \cdot 3 \cdot \frac{1}{18} + 
                7 \cdot 3 \cdot \frac{1}{18} +
                9 \cdot 3 \cdot \frac{1}{18} \\
            & + 6 \cdot 4 \cdot \frac{1}{18} + 
                8 \cdot 4 \cdot \frac{1}{18}  \\
            & + 7 \cdot 5 \cdot \frac{1}{18} \\
            & = (35 + 56 + 63 + 56 + 35 ) \frac{1}{18} = \frac{245}{18}  
\end{align*}



## Example - Discrete random vector {.smaller}
### Marginals

- We want to compute the marginal of $Y$ via the formula
$$
f_Y(y) = \sum_{x \in \R} f_{X,Y}(x,y)
$$

- Again looking at the table for $f_{X,Y}$, we get
\begin{align*}
f_Y(0) & = f_{X,Y}(2,0) + f_{X,Y}(4,0) + f_{X,Y}(6,0) \\ 
       &  + f_{X,Y}(8,0) + f_{X,Y}(10,0) + f_{X,Y}(12,0) \\
       & = 6 \cdot \frac{1}{36} = \frac{3}{18}
\end{align*}




## Example - Discrete random vector {.smaller}
### Marginals

- Similarly, we get
\begin{align*}
f_Y(1) & = f_{X,Y}(3,1) + f_{X,Y}(5,1) + f_{X,Y}(7,1) \\
       & + f_{X,Y}(9,1) +  f_{X,Y}(11,1) \\
       & = 5 \cdot \frac{1}{18} = \frac{5}{18}
\end{align*}
- And the remaining values follow a similar pattern:
$$
f_Y(2) = \frac{4}{18}  \,, \quad 
f_Y(3) = \frac{3}{18}  \,, \quad 
f_Y(4) = \frac{2}{18}  \,, \quad 
f_Y(5) = \frac{1}{18}  \,, \quad 
$$



## Example - Discrete random vector {.smaller}
### Marginals

Hence the pmf of $Y$ is given by the table below

| $y$    |   $0$       |        $1$   |      $2$     |     $3$      |      $4$     |     $5$      |
|:------:|:-----------:|:------------:|:------------:|:------------:|:------------:|:------------:|
|$f_Y(y)$|$\frac{3}{18}$|$\frac{5}{18}$|$\frac{4}{18}$|$\frac{3}{18}$|$\frac{2}{18}$|$\frac{1}{18}$|  

Note that $f_Y$ is indeed a pmf, since 
$$
\sum_{y \in \R} f_Y(y) = \sum_{y=0}^5 f_Y(y) = 1
$$





## Continuous random vectors {.smaller}


::: Definition

The random vector $(X,Y)$ is **continuous** if $X$ and $Y$ are continuous rv

:::

::: Definition

The **joint probability density function** or **joint pdf** of a continuous
random vector $(X,Y)$ is a function $f_{X,Y} \colon \R^2 \to \R$ s.t.
$$
P((X,Y) \in A) = \int_{A} f_{X,Y}(x,y) \, dxdy
$$

:::



- $\int_A$ is a double integral over $A$, like the ones you saw in Calculus
- The joint pdf is defined over the whole $\R^2$





## Continuous random vectors {.smaller}
### Expected value

- Suppose $(X,Y) \colon \Omega \to \R^2$ continuous random vector and $g \colon \R^2 \to \R$ function
- Then $g(X,Y) \colon \Omega \to \R$ is random variable


::: Definition

The **expected value** of the random variable $g(X,Y)$ is
$$
\Expect[g(X,Y)] := \int_{\R^2} g(x,y) f_{X,Y}(x,y) \, dxdy
$$

:::


**Notation**:The symbol $\int_{\R^2}$ denotes the double integral $\int_{-\infty}^\infty\int_{-\infty}^\infty$




## Continuous random vectors {.smaller}
### Marginals


::: Definition

Let $(X,Y)$ be a continuous random vector.
The **marginal pdfs** of $X$ and $Y$ are functions 
$f_X,f_Y \colon \R \to \R$ s.t.
$$
P(a \leq X \leq b)  =  \int_{a}^b f_X (x) \,dx  \quad \text{ and } \quad 
P(a \leq Y \leq b)  =  \int_{a}^b f_Y (y) \,dy 
$$

:::

**Note**: The **marginal pdfs** of $X$ and $Y$ are just the usual **pdfs** of $X$ and $Y$




## Continuous random vectors {.smaller}
### Marginals


Marginals of $X$ and $Y$ can be computed from the joint pdf $f_{X,Y}$ 

::: Theorem

Let $(X,Y)$ be a discrete random vector with joint pdf $f_{X,Y}$.
The **marginal pdfs** of $X$ and $Y$ are given by
$$
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y) \,dy \quad \text{ and }
\quad 
f_Y(y) = \int_{-\infty}^\infty f_{X,Y}(x,y) \, dx
$$

:::




## Characterization of joint pmf and pdf {.smaller}


::: Theorem

Let $f \colon \R^2 \to \R$. Then $f$ is **joint pmf** or **joint pdf** of a random vector $(X,Y)$ iff

1. $f(x,y) \geq 0$ for all $(x,y) \in \R^2$
2. $\sum_{(x,y) \in \R^2} f(x,y) = 1 \,\,\,$ (joint pmf) $\quad$ or $\quad$ $\int_{\R^2} f(x,y) \,dxdy = 1 \,\,\,$ (joint pdf)

:::

In the above setting:

- The random vector $(X,Y)$ has **distribution**
    * $P(X=x,Y=y ) = f(x,y)   \,\,\,\text{ (joint pmf)}$
    * $P((X,Y) \in A) = \int_A f (x,y) \, dxdy  \,\,\, \text{ (joint pdf)}$

- The symbol $(X,Y) \sim f$ denotes that $(X,Y)$ has distribution $f$






## Summary - Random Vectors {.smaller}



| $(X,Y)$ discrete random vector       |  $(X,Y)$ continuous random vector         |
|--------------                        |----------------                           |
| $X$ and $Y$ discrete                 |  $X$ and $Y$ continuous                   |
|**Joint pmf**                         |**Joint pdf**                              |
| $f_{X,Y}(x,y) := P(X=x,Y=y)$         | $P((X,Y) \in A) = \int_A f_X(x,y) \,dxdy$ |  
| $f_{X,Y} \geq 0$                     |  $f_{X,Y} \geq 0$                         |
|$\sum_{(x,y)\in \R^2} f_{X,Y}(x,y)=1$ | $\int_{\R^2} f_{X,Y}(x,y) \, dxdy= 1$     |
|**Marginal pmfs**                     | **Marginal pdfs**                         |
| $f_X (x) := P(X=x)$                  |$P(a \leq X \leq b) = \int_a^b f_X(x) \,dx$|
| $f_Y (y) := P(Y=y)$                  |$P(a \leq Y \leq b) = \int_a^b f_Y(y) \,dy$|
|$f_X (x)=\sum_{y \in \R} f_{X,Y}(x,y)$|$f_X(x) = \int_{\R} f_{X,Y}(x,y) \,dy$     |
|$f_Y (y)=\sum_{x \in \R} f_{X,Y}(x,y)$|$f_Y(y) = \int_{\R} f_{X,Y}(x,y) \,dx$     |





## Linearity of Expected Value {.smaller}


::: Theorem

$(X,Y)$ random vector, $g,h \colon \R^2 \to \R$ functions and $a,b,c \in \R$. The expectation is linear:
\begin{equation} \tag{1}
\Expect( a g (X,Y) + b h(X,Y)+ c ) = a \Expect[g(X,Y)] + b \Expect[h(X,Y)] + c
\end{equation}
In particular
\begin{equation} \tag{2}
\Expect[a X + b Y] = a\Expect[X] + b\Expect[Y]
\end{equation}

:::


- Proof of (1) follows by definition (see also argument in Slide 90 in Lecture 1)
- Equation (2) follows from (1) by setting 
$$ 
c=0 \,, \quad  g(x,y)=x \,, \qquad h(x,y)=y
$$






# Part 2: <br>Conditional distributions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::






## Conditional distributions - Discrete case  {.smaller}

- Suppose given a discrete random vector $(X,Y)$

- It might happen that the event $\{X=x\}$ depends on $\{Y=y\}$ 

- If $P(Y=y)>0$ we can define the **conditional probability**
$$
P(X=x|Y=y) := \frac{P(X=x,Y=y)}{P(Y=y)} = \frac{f_{X,Y}(x,y)}{f_Y(y)}
$$
where $f_{X,Y}$ is **joint pmf** of $(X,Y)$ and $f_Y$ the **marginal pmf** of $Y$




## Conditional pmf  {.smaller}


::: Definition

$(X,Y)$ discrete random vector with joint pmf $f_{X,Y}$ and marginal pmfs 
$f_X, f_Y$

- For any $x$ such that $f_X(x)=P(X=x)>0$ the **conditional pmf of $Y$ given that $X=x$** is the function $f(\cdot | x)$ defined by
$$
f(y|x) := P(Y=y|X=x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
$$

- For any $y$ such that $f_Y(y)=P(X=y)>0$ the **conditional pmf of $X$ given that $Y=y$** is the function $f(\cdot | y)$ defined by
$$
f(x|y) := P(X=x|Y=y) =\frac{f_{X,Y}(x,y)}{f_Y(y)}
$$

:::




## Conditional pmf  {.smaller}

- Conditional pmf $f(y|x)$ is indeed a **pmf**:
  - $f(y|x) \geq 0$
  - $\sum_{y} f(y|x) = \dfrac{\sum_{y} f_{X,Y}(x,y)}{f_X(x)} = \dfrac{f_X(x)}{f_X(x)} = 1$
  - Hence there exists a discrete rv $Z$ whose pmf is $f(y|x)$
  - This is true by the Theorem in Slide 80 in Lecture 1

- Similar reasoning yields that also $f(x|y)$ is a **pmf**

- **Notation**: We will often write
    * $X|Y$ to denote the distribution $f(x|y)$
    * $Y|X$ to denote the distribution $f(y|x)$




## Conditional distributions - Continuous case  {.smaller}


- In the discrete case we consider the conditional probability
$$
P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)}
$$
- However when $Y$ is continuous random variable we have
$$
P(Y=y) = 0 \quad \forall \, y \in \R
$$
- **Question**: How do we define conditional distributions in the continuous case?
- **Answer**: By replacing pmfs with pdfs



## Conditional pdf  {.smaller}


::: Definition

$(X,Y)$ continuous random vector with joint pdf $f_{X,Y}$ and marginal pdfs 
$f_X, f_Y$

- For any $x$ such that $f_X(x)>0$ the **conditional pdf of $Y$ given that $X=x$** is the function $f(\cdot | x)$ defined by
$$
f(y|x) := \frac{f_{X,Y}(x,y)}{f_X(x)}
$$

- For any $y$ such that $f_Y(y)>0$ the **conditional pdf of $X$ given that $Y=y$** is the function $f(\cdot | y)$ defined by
$$
f(x|y) := \frac{f_{X,Y}(x,y)}{f_Y(y)}
$$

:::




## Conditional pdf  {.smaller}

- Conditional pdf $f(y|x)$ is indeed a **pdf**:
  - $f(y|x) \geq 0$
  - $\int_{y \in \R} f(y|x) \, dy = \dfrac{\int_{y \in \R} f_{X,Y}(x,y) \, dy}{f_X(x)} = \dfrac{f_X(x)}{f_X(x)} = 1$
  - Hence there exists a continuous rv $Z$ whose pdf is $f(y|x)$
  - This is true by the Theorem in Slide 80 in Lecture 1

- Similar reasoning yields that also $f(x|y)$ is a **pdf**





## Conditional expectation  {.smaller}


::: Definition

$(X,Y)$ random vector and $g \colon \R \to \R$ function. The **conditional expectation** of $g(Y)$ given $X=x$ is
\begin{align*}
\Expect [g(Y) | x] & := \sum_{y} g(y) f(y|x)   \quad \text{ if } (X,Y) \text{ discrete} \\
\Expect [g(Y) | x] & := \int_{y \in \R} g(y) f(y|x) \, dy   \quad \text{ if } (X,Y) \text{ continuous}
\end{align*}

:::

- $\Expect [g(Y) | x]$ is a real number for all $x \in \R$
- $\Expect [g(Y) | X]$ denotes the Random Variable $h(X)$ where $h(x):=\Expect [g(Y) | x]$




## Conditional variance  {.smaller}


::: Definition

$(X,Y)$ random vector. The **conditional variance** of $Y$ given $X=x$ 
is
$$
\Var [Y | x]  := \Expect[Y^2|x] - \Expect[Y|x]^2 
$$

:::

- $\Var [Y | x]$ is a real number for all $x \in \R$
- $\Var [Y | X]$ denotes the Random Variable 
$$
\Var [Y | X] := \Expect[Y^2|X] - \Expect[Y|X]^2 
$$




## Example - Conditional distribution  {.smaller}



- Continuous random vector $(X,Y)$ with **joint pdf**
$$
f_{X,Y}(x,y) := e^{-y}  \,\, \text{ if } \,\, 0 < x < y  \,, \quad 
f_{X,Y}(x,y) :=0    \,\, \text{ otherwise}
$$



```{r}

library(plotly)

# Define the function
f <- function(x, y) {
  if (0 < x && x < y) {
    return(exp(-y))
  } else {
    return(0)
  }
}

# Generate data for the surface plot
x_vals <- seq(0, 2, length.out = 200)
y_vals <- seq(0, 2, length.out = 200)

z_matrix <- outer(x_vals, y_vals, Vectorize(function(x, y) f(x, y)))

# Create a 3D surface plot using plot_ly
plot_ly(
  x = x_vals,
  y = y_vals,
  z = z_matrix,
  type = "surface",
  colors = "viridis",
  showscale = FALSE
) %>% 
    layout(
    scene = list(
      xaxis = list(title = "x"),
      yaxis = list(title = "y"),
      zaxis = list(title = "f_XY"),
      camera = list(
        eye = list(x = 1.87, y = 0.88, z = 0.64)
      )
    ),
    width = 800,
    height = 600
  )


```



::: footer

<div color="#cc0164">  </div>

:::






## Example - Conditional distribution  {.smaller}

- We compute $f_X$, the **marginal pdf of $X$**:
  * If $x \leq 0$ then $f_{X,Y}(x,y)=0$. Therefore
    $$
    f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y) \, dy = 0 
    $$
  * If $x > 0$ then $f_{X,Y}(x,y)=e^{-y}$ if $y>x$, and $f_{X,Y}(x,y)=0$ if $y \leq x$. Thus
  \begin{align*}
  f_X(x) & = \int_{-\infty}^\infty f_{X,Y}(x,y) \, dy = 
  \int_{x}^\infty e^{-y} \, dy \\
  & = - e^{-y} \bigg|_{y=x}^{y=\infty}  = -e^{-\infty} + e^{-x}  = e^{-x} 
  \end{align*}





## Example - Conditional distribution  {.smaller}


- The **marginal pdf of $X$** has then **exponential distribution**
$$
f_{X}(x) = 
\begin{cases}
e^{-x} & \text{ if } x > 0 \\
0      & \text{ if } x \leq 0 
\end{cases}
$$


```{r}

# Define the probability density function (PDF)
f_X <- function(x) {
  exp(-x)
}

# Generate x values for plotting
x_values <- seq(0, 4, length.out = 100)

# Calculate corresponding y values
y_values <- f_X(x_values)

# Plot the distribution using base R plot function
plot(x_values, 
    y_values, 
    type = "l", 
    col = "blue", 
    lwd = 4,
    xlab = "", 
    ylab = "")

mtext("x", side=1, line=3, cex=2)
mtext("f_X", side=2, line=2.5, cex=2)


# Add a grid for better visualization
grid()

```




## Example - Conditional distribution  {.smaller}


- We now compute $f(y|x)$, the **conditional pdf of $Y$ given $X=x$**:
    * Note that $f_X(x)>0$ for all $x>0$
    * Hence assume fixed some $x>0$
    * If $y>x$ we have $f_{X,Y}(x,y)=e^{-y}$. Hence
    $$
    f(y|x) := \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{e^{-y}}{e^{-x}} = e^{-(y-x)}
    $$
    * If $y \leq x$ we have $f_{X,Y}(x,y)=0$. Hence
    $$
    f(y|x) := \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{0}{e^{-x}} = 0
    $$





## Example - Conditional distribution  {.smaller}


- The conditional distribution $Y|X$ is therefore **exponential**
$$
f(y|x) = 
\begin{cases}
e^{-(y-x)} & \text{ if } y > x \\
0      & \text{ if } y \leq x 
\end{cases}
$$

- The **conditional expectation** of $Y$ given $X=x$ is
\begin{align*}
\Expect[Y|x] & = \int_{-\infty}^\infty y f(y|x) \, dy 
               = \int_{x}^\infty y e^{-(y-x)} \, dy \\
             & = -(y+1) e^{-(y-x)} \bigg|_{x}^\infty =  x + 1  
\end{align*}
where we integrated by parts





## Example - Conditional distribution  {.smaller}


- Therefore **conditional expectation** of $Y$ given $X=x$ is
$$
\Expect[Y|x] = x + 1  
$$

- This can also be interpreted as the random variable
$$
\Expect[Y|X] = X + 1  
$$





## Example - Conditional distribution  {.smaller}


- The **conditional second moment** of $Y$ given $X=x$ is
\begin{align*}
\Expect[Y^2|x] & = \int_{-\infty}^\infty y^2 f(y|x) \, dy 
               = \int_{x}^\infty y^2 e^{-(y-x)} \, dy \\
             & = (y^2+2y+2) e^{-(y-x)} \bigg|_{x}^\infty = x^2 + 2x + 2 
\end{align*}
where we integrated by parts

- The **conditional variance** of $Y$ given $X=x$ is
$$
\Var[Y|x] = \Expect[Y^2|x] - \Expect[Y|x]^2 = x^2 + 2x + 2 - (x+1)^2 = 1
$$

- This can also be interpreted as the random variable
$$
\Var[Y|X] = 1
$$




## Conditional Expectation {.smaller}
### A useful formula

::: Theorem

$(X,Y)$ random vector. Then
$$
\Expect[X] = \Expect[ \Expect[X|Y] ]
$$

:::

**Note**: The above formula contains abuse of notation -- $\Expect$ has 3 meanings

- First $\Expect$ is with respect to the marginal of $X$
- Second $\Expect$ is with respect to the marginal of $Y$
- Third $\Expect$ is with respect to the conditional distribution $X|Y$



## Conditional Expectation {.smaller}
### Proof of Theorem

- Suppose $(X,Y)$ is continuous

- Recall that $\Expect[X|Y]$ denotes the random variable $g(Y)$ with 
$$
g(y):= \Expect[X|y] := \int_{\R} xf(x|y) \, dx
$$

- Also recall that by definition
$$
f_{X,Y}(x,y)= f(x|y)f_Y(y)
$$


## Conditional Expectation {.smaller}
### Proof of Theorem


- Therefore
\begin{align*}
\Expect[\Expect[X|Y]] & = \Expect[g(Y)] =  \int_{\R} g(y) f_Y(y) \, dy  \\
                      & = \int_{\R}  \left(  \int_{\R} xf(x|y) \, dx   \right) f_Y(y)\, dy
                        = \int_{\R^2}  x f(x|y) f_Y(y) \, dx dy  \\
                      & = \int_{\R^2}  x f_{X,Y}(x,y) \, dx dy 
                        = \int_{\R}  x \left( \int_{\R} f_{X,Y}(x,y)\, dy \right) \, dx   \\
                      & = \int_{\R}  x f_{X}(x) \, dx  
                        = \Expect[X]
\end{align*}


- If $(X,Y)$ is discrete the thesis follows by replacing intergrals with series


## Conditional Expectation {.smaller}
### Example - Application of the formula


- Consider again the continuous random vector $(X,Y)$ with **joint pdf**
$$
f_{X,Y}(x,y) := e^{-y}  \,\, \text{ if } \,\, 0 < x < y  \,, \quad 
f_{X,Y}(x,y) :=0    \,\, \text{ otherwise}
$$

-  We have proven that 
$$
\Expect[Y|X] = X + 1  
$$

- We have also shown that $f_X$ is exponential
$$
f_{X}(x) = 
\begin{cases}
e^{-x} & \text{ if } x > 0 \\
0      & \text{ if } x \leq 0 
\end{cases}
$$




## Conditional Expectation {.smaller}
### Example - Application of the formula


- From the knowledge of $f_X$ we can compute $\Expect[X]$
$$
\Expect[X] = \int_0^\infty x e^{-x} \, dx = -(x+1)e^{-x} \bigg|_{x=0}^{x=\infty} = 1
$$

- Using the Theorem we can compute $\Expect[Y]$ **without** computing $f_Y$:
\begin{align*}
\Expect[Y] & = \Expect[ \Expect[Y|X] ] \\
           & = \Expect[X + 1] \\
           & = \Expect[X] + 1  \\
           & = 1 + 1 = 2
\end{align*}







# Part 3: <br>Independence {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::







## Independence of random variables  {.smaller}
### Intuition

- In previous example: the conditional distribution of $Y$ given $X=x$ was
$$
f(y|x) = 
\begin{cases}
e^{-(y-x)} & \text{ if } y > x \\
0      & \text{ if } y \leq x 
\end{cases}
$$

- In particular $f(y|x)$ depends on $x$

- This means that knowledge of $X$ gives information on $Y$

- When $X$ does not give any information on $Y$ we say that 
$X$ and $Y$ are **independent**





## Independence of random variables {.smaller}
### Formal definition

::: Definition

$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$ and marginal pdfs or pmfs $f_X,f_Y$. We say that $X$ and $Y$ are **independent** random variables if
$$
f_{X,Y}(x,y)  =  f_X(x)f_Y(y) \,, \quad \forall \, (x,y) \in \R^2 
$$

:::




## Independence of random variables {.smaller}
### Conditional distributions and probabilities


If $X$ and $Y$ are **independent** then $X$ gives no information on $Y$ (and vice-versa):

- Conditional distribution: $Y|X$ is same as $Y$
$$
f(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{f_X(x)f_Y(y)}{f_X(x)} = f_Y(y)
$$

- Conditional probabilities: From the above we also obtain
\begin{align*}
P(Y \in A | x) & = \sum_{y \in A} f(y|x) = \sum_{y \in A} f_Y(y) = P(Y \in A)  & \, \text{ discrete rv} \\
P(Y \in A | x) & = \int_{y \in A} f(y|x) \, dy = \int_{y \in A} f_Y(y) \, dy = P(Y \in A)  &  \, \text{ continuous rv}
\end{align*}





## Independence of random variables {.smaller}
### Characterization of independence - Densities

::: Theorem

$(X,Y)$ random vector with joint pdf or pmf $f_{X,Y}$. They are equivalent:

- $X$ and $Y$ are independent random variables
- There exist functions $g(x)$ and $h(y)$ such that
$$
f_{X,Y}(x,y) = g(x)h(y) \,, \quad \forall \, (x,y) \in \R^2
$$

::: 





## Independence of random variables {.smaller}
### Consequences of independence

::: Theorem

Suppose $X$ and $Y$ are independent random variables. Then

- For any $A,B \subset \R$ we have
$$
P(X \in A, Y \in B) = P(X \in A) P(Y \in B)
$$

- Suppose $g(x)$ is a function of (only) $x$,
$h(y)$ is a function of (only) $y$. Then
$$
\Expect[g(X)h(Y)] = \Expect[g(X)]\Expect[h(Y)]  
$$

::: 




## Independence of random variables {.smaller}
### Proof of First Statement

- Define the function $p(x,y):=g(x)h(y)$. Then
\begin{align*}
\Expect[g(X)h(Y)] & = \Expect(p(X,Y)) = \int_{\R^2} p(x,y) f_{X,Y}(x,y)  \, dxdy \\
                  & = \int_{\R^2} g(x)h(y) f_X(x) f_Y(y)  \, dxdy \\
                  & = \left( \int_{-\infty}^\infty g(x) f_X(x) \, dx \right)
                      \left( \int_{-\infty}^\infty h(y) f_Y(y) \, dy \right) \\
                  & = \Expect[g(X)] \Expect[h(Y)]
\end{align*}

- Proof in the discrete case is the same: replace intergrals with series





## Independence of random variables {.smaller}
### Proof of Second Statement

- Define the product set 
$$
A \times B :=\{ (x,y) \in \R^2 \colon x \in A , y \in B\}
$$


- Therefore we get
\begin{align*}
P(X \in A , Y \in B) & = \int_{A \times B} f_{X,Y}(x,y) \, dxdy \\
                     & = \int_{A \times B} f_X(x) f_Y(y) \, dxdy \\
                     & = \left(\int_{A} f_X(x) \, dx \right)
                         \left(\int_{B} f_Y(y) \, dy \right) \\
                     & = P(X \in A) P(Y \in B)
\end{align*}








## Independence of random variables {.smaller}
### Moment generating functions

::: Theorem

Suppose $X$ and $Y$ are independent random variables and denote by 
$M_X$ and $M_Y$ their moment generating functions. Then
$$
M_{X + Y} (t) = M_X(t) M_Y(t)
$$

:::


**Proof**: Follows by previous Theorem
\begin{align*}
M_{X + Y} (t) & = \Expect[e^{t(X+Y)}] = \Expect[e^{tX}e^{tY}] \\
              & = \Expect[e^{tX}] \Expect[e^{tY}] \\
              & = M_X(t) M_Y(t)
\end{align*}







## Example  {.smaller}
### Sum of independent normals

- Suppose $X \sim N (\mu_1, \sigma_1^2)$ and $Y \sim N (\mu_2, \sigma_2^2)$ 
are independent normal random variables

- We have seen in Slide 119 in Lecture 1 that for normal distributions
$$
M_X(t) = \exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right) \,, \qquad
M_Y(t) = \exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)
$$

- Since $X$ and $Y$ are independent, from previous Theorem we get
\begin{align*}
M_{X+Y}(t) & =  M_{X}(t) M_{Y}(t) =
\exp \left( \mu_1 t + \frac{t^2 \sigma_1^2}{2} \right)
\exp \left( \mu_2 t + \frac{t^2 \sigma_2^2}{2} \right)   \\
& = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
\end{align*}





## Example  {.smaller}
### Sum of independent normals

- Therefore $Z := X + Y$ has moment generating function
$$
M_{Z}(t) = M_{X+Y}(t) = \exp \left( (\mu_1 + \mu_2) t + \frac{t^2 (\sigma_1^2 + \sigma_2^2)}{2} \right)
$$

- The above is the mgf of a **normal distribution** with 
$$
\text{mean }\quad \mu_1 + \mu_2 \quad \text{ and variance} \quad  \sigma_1^2 + \sigma_2^2
$$

- By the Theorem in Slide 132 of Lecture 1 we have
$$
Z \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)
$$

- **Sum of independent normals is normal**







# Part 4: <br>Covariance and correlation {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Relationship between Random Variables {.smaller}


Given two random variables $X$ and $Y$ we said that

- $X$ and $Y$ are **independent** if
$$
f_{X,Y}(x,y) = f_X(x) g_Y(y)
$$

- In this case there is no relationship between $X$ and $Y$

- This is reflected in the conditional distributions:
$$
X|Y \sim X \qquad \qquad Y|X \sim Y
$$





## Relationship between Random Variables {.smaller}


If $X$ and $Y$ are **not independent** then there is a **relationship** between them

::: Question 

How do we measure the strength of such dependence?

:::


**Answer**: By introducing the notions of

- Covariance
- Correlation




## Covariance {.smaller}
### Definition


**Notation**: Given two rv $X$ and $Y$ we denote
\begin{align*}
& \mu_X := \Expect[X]   \qquad & \mu_Y & := \Expect[Y] \\
& \sigma^2_X := \Var[X] \qquad  & \sigma^2_Y & := \Var[Y] 
\end{align*}


::: Definition 

The **covariance** of $X$ and $Y$ is the number
$$
\Cov(X,Y) := \Expect[  (X - \mu_X) (Y - \mu_Y)  ]
$$

:::




## Covariance {.smaller}
### Remark 

The sign of $\Cov(X,Y)$ gives information about the relationship between $X$ and $Y$:

- If $X$ is **large**, is $Y$ likely to be **small** or **large**?
- If $X$ is **small**, is $Y$ likely to be **small** or **large**?
- Covariance encodes the above questions



## Covariance {.smaller}
### Remark 

The sign of $\Cov(X,Y)$ gives information about the relationship between $X$ and $Y$

|                             |$X$ small: $\, X<\mu_X$|$X$ large: $\, X>\mu_X$|
|-----------------------------|----------------------------|-----------------------------|
|$Y$ small: $\, Y<\mu_Y$|     $(X-\mu_X)(Y-\mu_Y)>0$ |  $(X-\mu_X)(Y-\mu_Y)<0$     |
|$Y$ large: $\, Y>\mu_Y$|     $(X-\mu_X)(Y-\mu_Y)<0$ |  $(X-\mu_X)(Y-\mu_Y)>0$     |
: {tbl-colwidths="[30,35,35]"}

<br>

|                             |$X$ small: $\, X<\mu_X$|$X$ large: $\, X>\mu_X$|
|-----------------------------|----------------------------|-----------------------------|
|$Y$ small: $\, Y<\mu_Y$|     $\Cov(X,Y)>0$          |  $\Cov(X,Y)<0$              |
|$Y$ large: $\, Y>\mu_Y$|     $\Cov(X,Y)<0$          |  $\Cov(X,Y)>0$              |
: {tbl-colwidths="[30,35,35]"}





## Covariance {.smaller}
### Alternative Formula

::: Theorem 

The covariance of $X$ and $Y$ can be computed via
$$
\Cov(X,Y) = \Expect[XY] - \Expect[X]\Expect[Y]
$$

:::




## Covariance {.smaller}
### Proof of Theorem

Using linearity of $\Expect$ and the fact that $\Expect[c]=c$ for $c \in \R$:
\begin{align*}
\Cov(X,Y) : & = \Expect[ \,\, (X - \Expect[X]) (Y - \Expect[Y]) \,\,  ] \\
            &  = \Expect \left[ \,\, XY - X \Expect[Y] - Y \Expect[X] + \Expect[X]\Expect[Y] \,\, \right]  \\
            &  = \Expect[XY] - \Expect[ X \Expect[Y] ]
               - \Expect[ Y \Expect[X] ] + \Expect[\Expect[X] \Expect[Y]] \\
            & =  \Expect[XY] - \Expect[X] \Expect[Y]    
               - \Expect[Y] \Expect[X] + \Expect[X] \Expect[Y] \\
            & =  \Expect[XY] - \Expect[X] \Expect[Y]    
\end{align*}







## Correlation {.smaller}

**Remark**:

- $\Cov(X,Y)$ encodes only **qualitative** information about the relationship between $X$ and $Y$

- To obtain **quantitative** information we introduce the **correlation**



::: Definition 

The **correlation** of $X$ and $Y$ is the number
$$
\rho_{XY} := \frac{\Cov(X,Y)}{\sigma_X \sigma_Y} 
$$

:::






# Part 5: <br>Multivariate random vectors {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::







# Part 6: <br>Unbiased estimators {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::








## Examples of Continuous RV {.smaller}


::: Theorem

If $X \sim N(\mu,\sigma^2)$ then $Z \sim N(0,1)$ where
$$
Z := \frac{X-\mu}{\sigma}
$$
:::

Check

::: Proof

Check

:::






## Independent random variables {.smaller}

Define here



## Identically distributed random variables {.smaller}








## Mean and variance of sums of random variables {.smaller}

- Consider random variables $X_1$, $X_2$, ..., $X_n$ with means $\mu_i$ and variances $\sigma^2_i$
$$
E[X_1+...+X_n]=\mu_1+\mu_2+...\mu_n
$$
$$
\mbox{Var}[X_1+...+X_n]=\sigma^2_1+\sigma^2_2+...+\sigma^2_n+2\sum_{i{\neq}j}\mbox{Cov}(X_i, X_j)
$$

- **The mean is linear but the variance is quadratic**

- Some further simplification is possible in the case of independently and identically distributed random variables when in this case
$$
E[X_1+...+X_n]=n\mu,\ \mbox{Var}[X_1+...+X_n]=n\sigma^2
$$





## Additional linear and quadratic nature of mean and variance {.smaller}

- For constants $a$ and $b$
$$
\Expect[aX+b]=a \Expect[X]+b
$$
$$
\Var [aX+b] = a^2 \Var [X]
$$
$$
\Cov (aX, bY)=ab \Cov(X, Y)
$$



## Estimating the mean and the variance {.smaller}

- Suppose you have an iid sample $X_1$, $X_2$, ... $X_n$ with mean $\mu$ and variance $\sigma^2$
- Want to estimate the mean and the variance
- It is obvious that the sample mean is the best thing to use
- It is less obvious that the best estimator of the variance is the unbiased estimator
$$
s^2=\frac{\sum_i(x_i-\overline{x})^2}{n-1}=\frac{\sum_ix_i^2-n{\overline{x}^2}}{n-1}
$$



## Unbiased estimator {.smaller}

Define estimators and unbiassed ones


## The sample mean is unbiased {.smaller}

- Define $\overline{X}=\frac{\sum_{i=1}^nX_i}{n}$
$$
E[\overline{X}]=E\frac{\sum_{i=1}^nX_i}{n}=\frac{\sum_{i=1}^nE[X_i]}{n}=\frac{n\mu}{n}=\mu
$$

- A related result we shall need later is
$$
\mbox{Var}[\overline{X}]=\mbox{Var}\frac{\sum_{i=1}^nX_i}{n}
$$
$$
\mbox{Var}[\overline{X}]=\frac{1}{n^2}\mbox{Var}\left(\sum_{i=1}^nX_i\right)=\frac{1}{n^2}n\sigma^2=\frac{\sigma^2}{n}
$$





## The sample variance is unbiased {.smaller}

- Consider $E[\sum_{i=1}^nX^2_i-n\overline{X}^2].$ (Dividing this term by $n-1$ then gives the unbiased variance estimator $S^2$)

- For the first term we have
$$
E[X_i]=\mu,\ \mbox{Var}[X_i]=\sigma^2
$$
$$
E[X^2_i]=(E[X_i])^2+\mbox{Var}(X_i)=\mu^2+\sigma^2
$$
$$
E[\sum_{i=1}^nX_i^2]=\sum_{i=1}^nE[X_i^2]=n(\mu^2+\sigma^2)
$$



## The sample variance is unbiased {.smaller}

- Re-write $n\overline{X}^2=(\sqrt{n}\overline{X})^2$
- We have that 
$$
E[\sqrt{n}\overline{X}]=\sqrt{n}\mu
$$
\begin{align*}
\mbox{Var}[\sqrt{n}\overline{X}] 
=(\sqrt{n})^2\mbox{Var}(\overline{X})
=n\frac{\sigma^2}{n}=\sigma^2
\end{align*}

- This gives
\begin{eqnarray*}
E[(\sqrt{n}\overline{X})^2] & = & (E[\sqrt{n}\overline{X}])^2+\mbox{Var}(\sqrt{n}\overline{X})\\
& = & n\mu^2+\sigma^2
\end{eqnarray*}





## The unbiased variance estimator really is unbiased {.smaller}
$\bullet$ Combining Slides 4.5-4.6 gives
\begin{eqnarray*}
E[\sum_{i=1}^nX^2_i-n\overline{X}^2] & = & n(\mu^2+\sigma^2)-(n\mu^2+\sigma^2) \\
& = & (n-1)\sigma^2\\
E[S^2] & = & \frac{1}{n-1}E[\sum_{i=1}^nX^2_i-n\overline{X}^2]\\
& = & \frac{(n-1)}{(n-1)}\sigma^2=\sigma^2
\end{eqnarray*}
$$
E[S^2]=\frac{1}{n-1}[n\sigma^2+\sigma^2-2\sigma^2]=\sigma^2
$$



## Additional note {.smaller}

$\bullet$ The $n-1$ that occurs in the denominator of the sample variance estimator is caused by a loss of precision associated with the fact that we have to estimate $\mu$ with the sample mean $\overline{x}$\\
$\bullet$ This leads to a general statistical rule we shall use later
$$
\mbox{Lose 1 degree of freedom for each parameter estimated}
$$
$\bullet$ In this case we have to estimate one parameter (the sample mean) so we are left with 
\begin{eqnarray*}
\mbox{degrees of freedom} & = & \mbox{No. in sample}-\mbox{No. of estimated parameters}\\
& = & n-1
\end{eqnarray*}



## Example calculation {.smaller}

- Wage data on 10 Advertising Professionals Accountants
- Want to estimate the mean and the variance


|**Professional**| $x_1$ | $x_2$| $x_3$ | $x_4$ | $x_5$ | $x_6$ | $x_7$ | $x_8$ | $x_9$ |$x_{10}$|
|:--------------:|:-----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:----:|:-----:|
|     **Wage**   |   36  |  40  |  46  |  54  |  57  |  58  |  59  |  60  |  62  |  63  |




## Solution to the example {.smaller}

- \textbf{Number} $n_1$=No. of advertising professionals=10\\
- **Mean**
\begin{eqnarray*}
\bar{x}=\frac{36+40+46+{\dots}+62+63}{10}=\frac{535}{10}=53.5
\end{eqnarray*}
- **Variance**
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  s^2 &=& \frac{\sum\ x_{1, i}^2-n\bar{x}_1^2}{n_1-1} \\
  \sum\ x_{1, i}^2 &=& 36^2+40^2+46^2+{\ldots}+62^2+63^2=29435 \\
  s^2 &=& \frac{29435-10(53.5)^2}{9}=90.2778
\end{eqnarray*}







# Thank you! {background-color="#cc0164" visibility="uncounted"}


::: footer

<div color="#cc0164"> </div>

:::


