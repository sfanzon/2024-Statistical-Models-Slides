---
title: "Statistical Models"
subtitle: "Lecture 10"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 10: <br>Practical regression<br>Part 1 {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Practical regression {.smaller}
### Next 2 classes


- How to do regression in R

- Applications of regression
  * Most examples will be from finance and business

**Side Note:** Regression is very important

- Subject is active field of mathematical research
- Regression plays central role in quantitative research
- Examples of quantitative research are in *finance* and *social sciences*





## Outline of Lecture 10 {.smaller}
### Part A -- 4 aspects of regression

1. Plotting variables in R  
    * Cross-check formal statistical results with graphical analyses
    * Important in practical research work

<br>

2. Coefficient of determination $R^2$
    * $R^2$ measures proportion of variability in the data explained by the model
    * $R^2$ close to $1$ is good result
    * Any $R^2$ larger than $0.3$ is potentially worthwhile



## Outline of Lecture 10 {.smaller}
### Part B -- 4 aspects of regression

3. t-test for regression
    * Test the significance of individual parameters

<br>

4. F-test for regression
    * Test the significance of multiple parameters

<br>

5. Multiple linear regression example





## Outline of Lecture 10 {.smaller}
### Part B -- Regression modelling assumptions

1. Extra sum of squares principle
2. F-tests and mathematical formulae
3. Examples
4. Regression modelling assumptions
5. Graphical detection of heteroscedasticity 






# Part A: <br>4 aspects of regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



# Part 1: <br>Plotting variables in R {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Plotting variables in R {.smaller}


Interested in relationship between 2 variables

- Want to plot the 2 variables together

- Cross-check the results of a formal statistical analysis

- Very important in real project work



## Example: Stock and Gold prices

::: {style="font-size: 0.75em"}
Dataset with $33$ entries for Stock and Gold price pairs
:::


::: {style="font-size: 0.55em"}
```{r}
#| echo: false
#| layout-ncol: 1

data <- read.table("datasets/L3eg1data.txt")

#Add column labels to data
colnames(data) <- c("Stock Price","Gold Price")

# Output markdown table from data
knitr::kable(
  list(data[1:11,], data[12:22,], data[23:33,]),
  row.names = TRUE,
  format = "html", 
  table.attr = 'class="table simple table-striped table-hover"',
) 


```

:::



## Example: Stock and Gold prices {.smaller}


- The data is stored in a ``.txt`` file

- The file can be downloaded here [stock_gold.txt](datasets/stock_gold.txt)

::: {.column width="54%"}

- The text file looks like this
![](images/stock_gold.png){width=73%}

:::

::: {.column width="44%"}

- Remarks:
    * There is a Header
    * 1st column lists *Stock Price*
    * 2nd column lists *Gold Price*

:::



## Reading data into R {.smaller}


To read ``stock_gold.txt`` into R proceed as follows:

1. Download [stock_gold.txt](datasets/stock_gold.txt) and move file to Desktop

2. Open the R Console and change working directory to **Desktop**

```r
# In MacOS type
setwd("~/Desktop")

# In Windows type
setwd("C:/Users/YourUsername/Desktop")
```


## Reading data into R {.smaller}

3. Read ``stock_gold.txt`` into R and store it in data-frame ``prices`` with code

```r
prices = read.table(file = "stock_gold.txt",
                    header = TRUE)
```

<br>

**Note:** We are telling ``read.table()`` that

- ``stock_gold.txt`` has a header
- Headers are *optional*
- Headers are good practice to describe data



## Reading data into R {.smaller}


4. For safety, let us check we loaded the correct data file 


```r
print(prices)
```

```{r}
prices = read.table(file = "datasets/stock_gold.txt",
                    header = TRUE)

print(prices)
```




## Store data into vectors {.smaller}

- We now store Stock and Gold prices in 2 vectors
    * Stock prices are in 1st column of ``prices``
    * Gold prices are in 2nd column of ``prices``

```r
stock.price <- prices[ , 1]
gold.price <- prices[ , 2]
```

<br>

- Alternatively the same can be achieved with

```r
stock.price <- prices$stock_price
gold.price <- prices$gold_price
```



## Plot Stock Price vs Gold Price {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}



```r
plot(stock.price, 
     gold.price, 
     xlab = "Stock Price", 
     ylab = "Gold Price",
     pch = 16
    )
```

:::

::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::




## Plot Stock Price vs Gold Price {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

- ``xlab`` and ``ylab`` specify axes labels

- ``pch`` specifies type of points

- Scaling is achieved with
    * ``xlim = c(lower, upper)``
    * ``ylim = c(lower, upper)``


:::


::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::



## Examining the graph {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="38%" style='display: flex; justify-content: center; align-items: center;'}

- Graph suggests that the 2 variables are negatively correlated

- Need to cross-check with the results of a formal statistical regression analysis


:::


::: {.column width="61%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                   header = TRUE
                  )

stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Create the plot
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)
```

:::
:::::




# Part 2: <br>Coefficient of <br> determination $R^2${background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Coefficient of determination $R^2$ {.smaller}

- $R^2$ is defined as

$$
 R^2 = \frac{ \ESS }{ \TSS } = 1 - \frac{ \RSS }{ \TSS }
$$

- $R^2$ measures proportion of variability in the data explained by the model

<br>

::: Important

- $R^2$ is automatically computed by R when using ``lm``
- **High** values of $R^2$ are **better**! 

:::




## Some observations about $R^2$ {.smaller}

::: Warning

$R^2$ increases as more $X$ variables are added to a regression model

:::

This is not necessarily good

- One can add lots of variables and make $R^2 \approx 1$

- This way the model explains the data really well
    $$
    y_i \approx \hat y_i \,, \quad \forall \,\, i = 1 , \ldots, n
    $$ 

- Problem: the model will not make good predictions on new data 

- This is known as **overfitting** and it should be avoided




## Some observations about $R^2$ {.smaller}

- $R^2$ lies between $0$ and $1$
    * $R^2 = 0$ model explains nothing
    * $R^2 = 1$ model explains everything

<br>

- Generally: the higher the value of $R^2$ the better the model
    * Textbook examples often have high values
    $$
    R^2 \geq 0.7
    $$
    * **Example:** In the *Unemployment* example of Lecture 9 we found
    $$
    R^2 = 0.8655401
    $$




## Some observations about $R^2$ {.smaller}



::: Important

In practice values 
$$
R^2 \geq 0.3
$$ 
imply there is a nontrivial amount of variation in the data explained by the model

:::

    
**Example:** In the *Stock Price* Vs *Gold Price* example we have
$$
R^2 = 0.395325
$$ 

- This shows that *Stock Price* affects *Gold Price*
- Since $R^2$ is not too large, also other factors affect *Gold Price*





## Running the regression in R {.smaller}

- The basic R command used to run regression is

::: {.r-stack}

``lm(formula)``

:::

<br>

- ``lm`` stands for **linear model**



## Running simple linear regression in R {.smaller}


For simple linear regression

$$
Y_i = \alpha + \beta x_i + \e_i
$$

the command is

::: {.r-stack}

``lm(y ~ x)``

:::

<br>

- Symbol ``y ~ x`` reads as *$y$ modelled as function of $x$*

- ``y`` is vector containing the data $y_1, \ldots, y_n$

- ``x`` is vector containing the data $x_1, \ldots, x_n$




## Running multiple linear regression in R {.smaller}



For multiple linear regression

$$
Y_i = \beta_1 + \beta_2 \, x_{i2} + \ldots + \beta_p \, x_{ip} + \e_i
$$

the command is 

::: {.r-stack}

``lm (y ~ x2 + x3 + ... + xp)``

:::

<br>

- ``y`` is vector containing the data $y_1, \ldots, y_n$

- ``xj`` is vector containing the data $x_{1j}, \ldots , x_{jp}$ 



## Running the regression in R {.smaller}

The best way to run regression is

1. Run the regression analysis and store the results in a variable

```r
fit.model <- lm(formula)
```

<br>

2. Use command ``summary`` to read output of regression

```r
summary(fit.model)
```

**Note:** If you are running the code from ``.R`` file you need to print output

```r
print( summary(fit.model) )
```




## Example: Stock and Gold prices {.smaller}

- Stock price is stored in vector 
    * ``stock.price``

- Gold price is stored in vector 
    * ``gold.price``


- We want to fit the simple linear model
$$
\text{gold.price } = \alpha + \beta \, \times  \text{ stock.price } + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```


- The full code can be downloaded here [simple_regression.R](codes/simple_regression.R)




## Full output

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut.png){width=85%}
:::

::::





## Interesting parts of Output {.smaller}



:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut.png){width=79%}
:::

::::
  

- The estimated regression coefficients are under ``estimate``
    * ``(Intercept)`` refers to the coefficient $\hat \alpha \qquad \implies  \qquad  \hat \alpha = 37.917$
    * ``stock.price`` refers to the coefficient $\hat \beta \qquad \implies  \qquad  \hat \beta = - 6.169$




## Interesting parts of Output {.smaller}


:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut.png){width=79%}
:::

::::


- Other quantities of interest are:


| Coefficient $R^2$ | $\texttt{Multiple R-squared:  0.3953}$ |
|:------------------|:----------                             |
|**t-statistic for** ``stock.price`` | $\texttt{stock.price t-value:  -4.502}$ |
| **F-statistic**      |  $\texttt{F-statistic: 20.27}$           |
: {tbl-colwidths="[50,50]"}




## Plotting the regression line {.smaller}

::::: {.columns style='display: flex !important; height: 80%;'}

::: {.column width="43%" style='display: flex; justify-content: center; align-items: center;'}


```r    
# Data stored in stock.price 
# and gold.price
# Plot the data
plot(stock.price, gold.price, 
     xlab = "Stock Price", 
     ylab= "Gold Price",
     pch = 16)

# Model stored in fit.model
# Plot the regression line
abline(fit.model, 
       col = "red", 
       lwd = 3)
```

:::


::: {.column width="48%" style='display: flex; justify-content: center; align-items: center;'}

```{r}
#| echo: false
#| fig-asp: 1

# Read the data
prices <- read.table(file = "datasets/stock_gold.txt",
                     header = TRUE
                    )

# Store data into vectors
stock.price <- prices$stock_price
gold.price <- prices$gold_price

# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Plot the data
plot(stock.price,
     gold.price, 
     xlab = "", 
     ylab= "",
     pch = 16)

# Add labels
mtext("Stock Price", side=1, line=3, cex=2.1)
mtext("Gold Price", side=2, line=2.5, cex=2.1)

# Plot the regression line in red
abline(fit.model, col = "red", lwd = 3)
```

:::

:::::




## Conclusion {.smaller}

- We fit a simple linear model to *Stock Price* Vs *Gold Price*

- We obtained the regression line

$$
\Expect[Y | x] = \hat \alpha + \hat \beta x =  37.917 -  6.169 \times x
$$

- The coefficient of correlation is

$$
R^2 = 0.395325 \geq 0.3
$$ 

- Hence the linear model explains the data to a reasonable extent:
    * *Stock Price* affects *Gold Price*
    * Since $R^2$ is not too large, also other factors affect *Gold Price*






## t-test and F-test for regression {.smaller}

- From ``lm`` we also obtained

| **t-statistic for** ``stock.price`` | $\texttt{stock.price t-value:  -4.502}$ |
|:------------------|:----------                             |
| **F-statistic**      |  $\texttt{F-statistic: 20.27}$           |
: {tbl-colwidths="[50,50]"}

<br>

- t-statistic and F-statistic for regression are mathematically **HARD** topic

- In the next two parts we explain what they mean
    * We however omit mathematical details
    * If interested check out Section 11.3 of [@casella-berger] and Chapter 11 of [@degroot]




# Part 3: <br>t-test for regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Basic question in regression {.smaller}

What happens to $Y$ as $X$ increases?

- increases?

- decreases?

- nothing?




## Positive gradient {.smaller}

As $X$ increases $Y$ increases

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(0, 5), 
  ylim = c(0, 5), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)


mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 0, 
  b = 1, 
  col = "black", 
  lwd = 2)

```



## Negative gradient {.smaller}

As $X$ increases $Y$ decreases

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(-3, 3), 
  ylim = c(-3, 3), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)



mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 0, 
  b = -1, 
  col = "black", 
  lwd = 2)

```



## Zero gradient {.smaller}

Changes in $X$ do not affect $Y$

```{r}
#| echo: false
#| fig-asp: 1


# Create a scatter plot with no data points (type = "n")
plot(
  1, 1, 
  type = "n", 
  xlab = "", 
  ylab = "", 
  xlim = c(0, 5), 
  ylim = c(0, 5), 
  frame.plot = TRUE, 
  axes = FALSE, 
  asp = 1)


mtext("X", side=1, line=3, cex=3)
mtext("Y", side=2, line=2, cex=3)

# Add the line y = x
abline(
  a = 2.5, 
  b = 0, 
  col = "black", 
  lwd = 2)

```




## t-test for simple regression {.smaller}

- Consider the simple linear regression model

$$
Y_i = \alpha + \beta X_i + \e_i
$$


- We have that 

$$
X \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta \neq 0
$$

- $\beta$ is a random quantity which depends on the sample

- Therefore we can study $\beta$ with the hypothesis test

\begin{align*}
H_0 \colon  & \beta = b \\
H_1 \colon & \beta \neq b
\end{align*}





## Construction of t-test {.smaller}

- Want to test hypothesis 

\begin{align*}
H_0 \colon  & \beta = b \\
H_1 \colon & \beta \neq b
\end{align*}

- Our best guess for $\beta$ is the estimator $\hat \beta$

- To test above hypotheses, we therefore need to
    * Know the **distribution** of
    $$
    \hat \beta = \frac{ S_{xy} }{ S_{xx} }
    $$
    * Construct **t-statistic** involving $\hat \beta$




## Distribution of $\hat \beta$ {.smaller}

::: Theorem

Consider the linear regression model 

$$
Y_i = \alpha + \beta x_i + \e_i
$$

with $\e_i$ iid $N(0, \sigma^2)$. Then

$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$

:::


**Proof:** Quite difficult. If interested see Theorem 11.3.3 in [@casella-berger] 




## Construction of the t-statistic {.smaller}

- We want to construct t-statistic for $\hat \beta$


- As for the standard t-test, we want a t-statistic of the form

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese}
$$


- We know that 

$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$



## Construction of the t-statistic {.smaller}



- In particular $\hat \beta$ is an unbiased estimator for $\beta$

$$
\Expect[ \hat \beta ] = \beta
$$

- Therefore $\hat \beta$ is the estimate

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} 
  = \frac{ \hat \beta - \beta }{ \ese }
$$



## Estimated Standard Error {.smaller}

- We need to find a **Standard Error** for $\hat \beta$

- We know that 

$$
\Var [\hat \beta] = \frac{ \sigma^2 }{ S_{xx}}
$$

- Hence the standard error of $\hat \beta$ is the standard deviation

$$
\SD [\hat \beta] = \frac{ \sigma }{ S_{xx} }
$$

- $\SD$ cannot be used for testing, since $\sigma^2$ is unknown



## Estimated Standard Error {.smaller}

- We however have an estimate for $\sigma^2$

$$
\hat \sigma^2 = \frac{1}{n} \RSS = \frac1n \sum_{i=1}^n (y_i - \hat y_i)^2
$$

- $\hat \sigma^2$ was obtained from maximization of the likelihood function (Lecture 8) 

- It can be shown that (see Section 11.3.4 in [@casella-berger])

$$
\Expect[ \hat\sigma^2 ] = \frac{n-2}{n} \, \sigma^2
$$



## Estimated Standard Error {.smaller}

- Therefore $\hat\sigma^2$ is not unbiased estimator of $\sigma^2$

- We hence rescale and introduce $S^2$

$$
S^2 := \frac{n}{n-2} \, \hat\sigma^2 = \frac{\RSS}{n-2} 
$$

- This way $S^2$ is unbiased estimator for $\sigma^2$

$$
\Expect[S^2] =  \frac{n}{n-2}  \, \Expect[\hat\sigma^2] =  \frac{n}{n-2} \, \frac{n-2}{n} \, \sigma^2 = \sigma^2
$$


## Estimated Standard Error {.smaller}

- Recall that the standard deviation of $\hat \beta$ is

$$
\SD [\hat \beta] = \frac{ \sigma }{ S_{xx} }
$$

- We replace the unknown $\sigma$ with its unbiased estimator $S$ 

- We obtain the **estimated standard error**

$$
\ese = \frac{S}{\sqrt{S_{xx}}}
$$


## t-statistic to test $\hat \beta$ {.smaller}

The t-statistic for $\hat \beta$ is then

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese}
  = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }
$$


::: Theorem 

Consider the linear regression model 

$$
Y_i = \alpha + \beta x_i + \e_i
$$

with $\e_i$ iid $N(0, \sigma^2)$. Then

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
\, \sim \,
t_{n-2}
$$

:::



## How to prove the Theorem {.smaller}

- Proof of this Theorem is quite difficult and we omit it

- If you are interested in the proof, see Section 11.3.4 in [@casella-berger]

- The main idea is that t-statistic can be rewritten as

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
  = \frac{ U }{ \sqrt{ V/(n-2) } }  
$$

- Here we defined

$$
U := \frac{ \hat \beta - \beta }{ \sigma / \sqrt{S_{xx}} } \,, \qquad \quad V := \frac{ (n-2) S^2 }{ \sigma^2 }
$$


## How to prove the Theorem {.smaller}

- We know that 

$$
\hat \beta \sim N \left(\beta , \frac{ \sigma^2 }{ S_{xx} } \right)
$$


- Therefore 

$$
U = \frac{ \hat \beta - \beta }{ \sigma / \sqrt{S_{xx}} } \, \sim \, N(0,1)
$$



## How to prove the Theorem {.smaller}

- Moreover it can be shown that 

$$
V = \frac{(n-2) S^2}{\sigma^2} \, \sim \, \chi_{n-2}^2
$$

- It can also be shown that $U$ and $V$ are independent



## How to prove the Theorem {.smaller}

- In summary, we have

$$
t = \frac{ \hat \beta - \beta }{ S / \sqrt{S_{xx}} }  
  = \frac{ U }{ \sqrt{ V/(n-2) } }  
$$

- $U$ and $V$ are independent, with

$$
U \sim N(0,1) \,, \qquad \quad V \sim \chi_{n-2}^2
$$

- From the Theorem on t-distribution in Lecture 3 we conclude

$$
t \sim t_{n-2}
$$




## Summary: t-test for $\beta$ {.smaller}

**Goal**: Estimate the slope $\beta$ for the simple linear model

$$
Y_i = \alpha + \beta x_i + \e_i \,, \qquad \e_i \, \text{ iid } \, N(0,\sigma^2)
$$

<br>

**Hypotheses**: If $b$ is guess for $\beta$ the hypotheses are

\begin{align*}
H_0 & \colon \beta = b \\
H_1 & \colon \beta \neq b
\end{align*}



## Summary: t-test for $\beta$ {.smaller}


- The t-statistic is

$$
t = \frac{\hat \beta - b }{ \ese } \, \sim \, t_{n-2} \,, \qquad \quad
\ese = \frac{S }{\sqrt{S_{xx}} }
$$

- In the above we have

$$
\hat \beta = \frac{ S_{xy} }{ S_{xx} } \,, \qquad \quad S^2 := \frac{\RSS}{n-2} 
$$


- The p-value is

$$
p = 2 P( t_{n-2} > |t| )
$$





## Example: Stock and Gold prices {.smaller}

- Recall that 
    * $Y =$ Gold Price
    * $X =$ Stock Price

- We want to test if *Gold Price* affects *Stock Price* at level $0.05$
    * Consider the linear model
    $$
    Y_i =  \alpha + \beta x_i + \e_i 
    $$
    * Test the hypotheses
    \begin{align*}
    H_0 & \colon \beta = 0 \\
    H_1 & \colon \beta \neq 0
    \end{align*}




## Testing for $\beta = 0$ {.smaller}

- Recall that Stock Prices and Gold Prices are stored in R vectors
    * ``stock.price``
    * ``gold.price``


- Fit the simple linear model with the following commands

$$
\text{gold.price } = \alpha + \beta \, \times  \text{ stock.price } + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  

- $\texttt{(Intercept)}$: 1st row of the table contains statistics related to $\hat \alpha$

- $\texttt{stock.price}$: 2nd row of the table contains statistics related to $\hat \beta$
    * For larger models there will be additional rows below the 2nd
    * These will be **informative** about additional regression parameters



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The 2nd row of the table has to be interpreted as follows


| $\texttt{Estimate}$   | $\text{The value of } \hat \beta$                  |
|:------------------    |:----------                                         |
| $\texttt{Std. Error}$ | Estimated standard error $\ese$ for $\beta$        |
| $\texttt{t value}$    | t-statistic $\, t = \dfrac{\hat \beta - 0 }{\ese}$|
| $\texttt{Pr(>|t|)}$   | p-value $\, p = 2 P( t_{n-2} > |t| )$             |
| $\texttt{*}$, $\, \texttt{**}$, $\, \texttt{***}$   | Statistical significance -- More stars is better |
: {tbl-colwidths="[35,65]"}




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The above table then gives

$$
\hat \beta = -6.169 \,, \qquad \ese = 1.37 \, , \qquad t = - 4.502 \,, \qquad 
p = 8.9 \times 10^{-5} \
$$


- The $t$-statistic computed by R can also be computed by hand

$$
  t   = \frac{\hat \beta - 0}{ \ese }
      = \frac{-6.169}{1.37} = -4.502 
$$




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::
  
- The p-value cannot be computed by hand

- However we can find critical value on [Tables](files/Statistics_Tables.pdf)

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

- Critical value is $\, t_{31}(0.025) = 2.040$

$$
|t| = 4.502 > 2.040 = t_{31}(0.025) \quad \implies \quad  p < 0.05
$$


## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_2.png){width=70%}
:::

::::


**Interpretation:**

- $p$ is very small and rated $\, \texttt{***}$

- Strong evidence ($p<0.05$) that *Stock Prices* affect *Gold Prices*
- As $\hat \beta < 0$ and statistically significant, as *Stock Prices* increase *Gold Prices* decrease and vice versa



## Warning {.smaller}


- t-statistic in summary refers to two-sided t-test of whether a coefficient is 0

- If $b = 0$ or one-sided t-test is required
    * Compute t-statistic by hand
    $$
    t = \frac{\hat \beta - b}{\ese}
    $$
    * $\, \ese$ is in the 2nd row under $\, \texttt{Std. Error}$
    * Compute p-value *by hand* with $\, \texttt{pt(t, df)}$




## t-test for general regression {.smaller}

- Consider the general linear regression model

$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_{ip} z_{ip} + \e_i \,, \qquad 
\e_i \, \text{ iid } \, N(0, \sigma^2)
$$

- We have that 

$$
Z_j \,\, \text{ affects } \,\, Y \qquad \iff \qquad
\beta_j \neq 0
$$

- To see if $Z_j$ affects $Y$ we need to test the hypothesis

\begin{align*}
H_0 \colon  & \beta_j = b_j \\
H_1 \colon & \beta_j \neq b_j
\end{align*}




## Distribution of estimator $\hat \beta$ {.smaller}


- The estimator for the general model is

$$
\hat \beta = (Z^T Z)^{-1} Z^T y
$$


- It can be proven that (see Section 11.5 in [@degroot])

$$
\hat \beta_j \sim N \left( \beta_j ,  \xi_{jj} \sigma^2 \right)
$$

- The numbers $\xi_{jj}$ are the diagonal entries of the $p \times p$ matrix

$$
(Z^T Z)^{-1} =
\left(
\begin{array}{ccc}
\xi_{11} & \ldots & \xi_{1p} \\
\ldots   & \ldots & \ldots \\
\xi_{p1} & \ldots & \xi_{pp} \\
\end{array}
\right)
$$



## Construction of the t-statistic {.smaller}



- In particular $\hat \beta_j$ is an unbiased estimator for $\beta_j$

$$
\Expect[ \hat \beta_j ] = \beta_j
$$

- Therefore the t-statistic for $\hat \beta_j$ is

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} 
  = \frac{ \hat \beta_j - \beta_j }{ \ese }
$$



## Estimated Standard Error {.smaller}

- We need to find a **Standard Error** for $\hat \beta_j$

- We know that 

$$
\Var [\hat \beta_j] =  \xi_{jj} \, \sigma^2 
$$

- Hence the standard error of $\hat \beta_j$ is the standard deviation

$$
\SD [\hat \beta_j] =  \xi_{jj}^{1/2} \, \sigma
$$

- $\SD$ cannot be used for testing, since $\sigma^2$ is unknown




## Estimated Standard Error {.smaller}

- We however have an estimate for $\sigma^2$

$$
\hat \sigma^2 = \frac{1}{n} \RSS = \frac1n \sum_{i=1}^n (y_i - \hat y_i)^2
$$

- $\hat \sigma^2$ was obtained from maximization of the likelihood function (Lecture 9) 

- It can be shown that (see Section 11.5 in [@degroot])

$$
\Expect[ \hat\sigma^2 ] = \frac{n-p}{n} \, \sigma^2
$$



## Estimated Standard Error {.smaller}

- Therefore $\hat\sigma^2$ is not unbiased estimator of $\sigma^2$

- We hence rescale and introduce $S^2$

$$
S^2 := \frac{n}{n-p} \, \hat\sigma^2 = \frac{\RSS}{n-p} 
$$

- This way $S^2$ is unbiased estimator for $\sigma^2$

$$
\Expect[S^2] =  \frac{n}{n-p}  \, \Expect[\hat\sigma^2] =  \frac{n}{n-p} \, \frac{n-p}{n} \, \sigma^2 = \sigma^2
$$


## Estimated Standard Error {.smaller}

- Recall that the standard deviation of $\hat \beta$ is

$$
\SD [\hat \beta] = \xi_{jj}^{1/2} \, \sigma
$$

- We replace the unknown $\sigma$ with its unbiased estimator $S$ 

- We obtain the **estimated standard error**

$$
\ese =\xi_{jj}^{1/2} \,  S
$$



## t-statistic to test $\beta_j$ {.smaller}


::: Theorem 

Consider the general linear regression model 

$$
Y_i = \beta_1 z_{i1} + \ldots +\beta_p z_{ip} + \e_i
$$

with $\e_i$ iid $N(0, \sigma^2)$. Then

$$
t = \frac{\text{Estimate } - \text{ Hypothesised Value}}{\ese} = \frac{ \hat \beta_j - \beta_j }{ \xi_{jj}^{1/2} \, S}  
\, \sim \,
t_{n-p}
$$

:::

**Proof:** See section 11.5 in [@degroot]


## Summary: t-test for $\beta_j$ {.smaller}

**Goal**: Estimate the coefficient $\beta_j$ for the general linear model

$$
Y_i = \beta_1 z_{i1} + \ldots + \beta_p z_{ip} + \e_i \,, \qquad \e_i \, \text{ iid } \, N(0,\sigma^2)
$$

<br>

**Hypotheses**: If $b_j$ is guess for $\beta_j$ the hypotheses are

\begin{align*}
H_0 & \colon \beta_j = b_j \\
H_1 & \colon \beta_j \neq b_j
\end{align*}



## Summary: t-test for $\beta_j$ {.smaller}

- The t-statistic is

$$
t = \frac{\hat \beta_j - b_j }{ \ese } \, \sim \, t_{n-p} \,, \qquad \quad
\ese = \xi_{jj}^{1/2} \, S 
$$

- In the above $\xi_{jj}$ are diagonal entries of $(Z^TZ)^{-1}$ and

$$
\hat \beta = (Z^TZ)^{-1} Z^T y \,, \qquad \quad S^2 := \frac{\RSS}{n-p} 
$$

- The p-value is

$$
p = 2P (t_{n-p} > |t|)
$$



## t-test in R {.smaller}

Fit the general regression model with ``lm``

- If $b_j = 0$ and a two-sided t-test is required
    * t-statistic is in $j$-th variable row under $\, \texttt{t value}$
    * p-value is in $j$-th variable row under $\, \texttt{Pr(>|t|)}$


- If $b_j = 0$ or one-sided t-test is required
    * Compute t-statistic by hand
    $$
    t = \frac{\hat \beta_j - b_j}{\ese}
    $$
    * $\,\, \ese$ for $\hat \beta_j$ is in $j$-th variable row under $\, \texttt{Std. Error}$
    * Compute p-value *by hand* with $\, \texttt{pt(t, df)}$



## Example: $\ese$ for simple linear regression {.smaller}

- Consider the simple regression model

$$
Y_i = \alpha + \beta x_i + \e_i
$$


- The design matrix is

$$
Z = \left( 
\begin{array}{cc}
1 & x_1 \\
\ldots & \ldots \\
1 & x_n \\
\end{array}
\right)
$$


## Example: $\ese$ for simple linear regression {.smaller}

- We have seen in Lecture 9 that

$$
(Z^T Z)^{-1} = 
\frac{1}{n S_{xx} }
\left(
\begin{array}{cc}
\sum_{i=1}^n x^2_i & -n \overline{x}\\
-n\overline{x} & n
\end{array}
\right)
$$


- Hence the $\ese$ for $\hat \alpha$ and $\hat \beta$ are

\begin{align*}
\ese (\hat \alpha) & = \xi_{11}^{1/2} \, S^2 = \sqrt{ \frac{ \sum_{i=1}^n x_i^2 }{ n S_{xx} } } \, S^2 \\[7pt]
\ese (\hat \beta) & = \xi_{22}^{1/2} \, S^2 = \frac{ S^2 }{ \sqrt{ S_{xx} } }
\end{align*}

- **Note:** $\, \ese(\hat\beta)$ coincides with the $\ese$ in Slide 50




# Part 4: <br>F-test for regression {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## F-test for overall significance {.smaller}

- Want to test the **overall significance** of the model

$$
Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

- This means answering the question:

$$
\text{ Does at least one } X_i \text{ affect } Y \text{ ?}
$$

- How to do this?
    * Could perform a sequence of t-tests on the $\beta_j$
    * For statistical reasons this is not really desirable
    * To assess **overall** significance we can perform **F-test**




## F-test for overall significance {.smaller}

The F-test for overall significance has 3 steps:

1. Define a larger **full model** (with more parameters)


2. Define a smaller nested **reduced model** (with fewer parameters)


3. Use an F-statistic to decide between larger or smaller model



## Overall significance for multiple regression {.smaller}

- The larger **full model** is

$$
Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

- The smaller **reduced model** is

$$
Y_i = \beta_1 + \e_i
$$


- Choosing the smaller model is equivalent to accepting $H_0$

\begin{align*}
H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
\end{align*}



## Overall significance for simple regression {.smaller}

- The larger **full model** is

$$
Y_i = \alpha  + \beta x_i  + \e_i
$$

- The smaller **reduced model** is

$$
Y_i = \alpha + \e_i
$$


- Choosing the smaller model is equivalent to accepting $H_0$

\begin{align*}
H_0 & \colon \beta = 0 \\
H_1 & \colon \beta \neq 0 
\end{align*}



## Construction of F-statistic {.smaller}

- Consider the full model with $p$ parameters

$$
Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

- Predictions for the full model are

$$
\hat y_i := \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}  
$$

- Define the residual sum of squares for the full model

$$
\RSS(p) := \sum_{i=1}^n (y_i - \hat y_i)^2 
$$



## Construction of F-statistic {.smaller}


- Consider now the reduced model

$$
Y_i = \beta_1 + \e_i
$$


- Predictions of the reduced model are constant

$$
\hat y_i = \beta_1 
$$

- Define the residual sum of squares for the full model

$$
\RSS(1) := \sum_{i=1}^n (y_i - \beta_1)^2
$$



## Construction of F-statistic {.smaller}

- Suppose the parameters of the full model
$$
\beta_2, \ldots, \beta_p
$$
are not important

- In this case the predictions of full and reduced model will be similar

- Therefore the $\RSS$ for the 2 models are similar

$$
\RSS (1) \, \approx \, \RSS(p)
$$



## Construction of F-statistic {.smaller}

- Recall that $\RSS$ is defined via minimization

$$
\RSS(k) := \min_{\beta_1 , \ldots , \beta_k} \ \sum_{i=1}^n ( y_i - \hat y_i)^2 \,, \qquad 
\hat y_i := \beta_1 + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}
$$

- Therefore $\RSS$ cannot increase if we add parameters to the model

$$
\RSS(1) \geq \RSS(p)
$$

- To measure how influential the parameters $\beta_2, \ldots, \beta_p$ are, we study

$$
\frac{\RSS(1) - \RSS (p)}{\RSS(p)}
$$



## Construction of F-statistic {.smaller}

- We now suitably rescale

$$
\frac{\RSS(1) - \RSS (p)}{\RSS(p)}
$$

- To this end, note that the degrees of freedom of reduced model are 

$$
\df (1) = n - 1
$$

- The degrees of freedom of the full model are 

$$
\df (p) = n - p
$$



## F-statistic for overall significance {.smaller}


::: Definition

The **F-statistic** for overall significance is

\begin{align*}
F & := \frac{\RSS(1) - \RSS (p)}{ \df(1) - \df (p) } \bigg/ 
\frac{\RSS(p)}{\df(p)}  \\[15pt] 
  & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}
\end{align*}

:::


**Theorem:** The F-statistic for overall significance has F-distribution

$$
F \, \sim \, F_{p-1,n-p}
$$






## Rewriting the F-statistic {.smaller}

::: Proposition

The F-statistic can be rewritten as

\begin{align*}
F & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}  \\[15pt]
  & = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} 
\end{align*}

:::



## Proof of Proposition {.smaller}

- Notice that $\TSS$ does not depend on $p$ since

$$
\TSS = \sum_{i=1}^n (y_i - \overline{y})^2
$$


- Recall the definition of $R^2$

$$
R^2 = 1 - \frac{\RSS (p)}{\TSS} 
$$


- From the above we obtain

$$
\RSS(p) = (1 - R^2) \TSS
$$




## Proof of Proposition {.smaller}

- By definition we have that 

$$
\RSS(1) = \min_{\beta_1} \ \sum_{i=1}^n (y_i - \beta_1)^2 
$$

- **Exercise:** Check that the unique solution to the above problem is

$$
\beta_1 = \overline{y}
$$

- Therefore we have

$$
\RSS(1) =  \sum_{i=1}^n (y_i - \overline{y})^2 = \TSS
$$




## Proof of Proposition {.smaller}

- We just obtained the two identities

$$
\RSS(p) = (1 - R^2) \TSS \,, \qquad \quad \RSS(1) = \TSS
$$


- From the above we conclude the proof

\begin{align*}
F & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}  \\[15pt]
  & = \frac{\TSS - (1 - R^2) \TSS}{ p - 1 } \bigg/ 
\frac{(1 - R^2) \TSS}{n - p}\\[15pt]
  & = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} 
\end{align*}





## F-statistic for simple regression {.smaller}

::: Proposition

1. The $F$-statistic for overall significance in simple regression is
$$
F = t^2 \,, \qquad \quad t = \frac{\hat \beta}{ S / \sqrt{S_{xx}}}
$$
where $t$ is the t-statistic for $\hat \beta$.

2. In particular the p-values for t-test and F-test coincide
$$
p = P( t_{n-2} > |t| ) = P( F_{1,n-2} > F )
$$

:::

**Proof:** Will be left as an exercise




## Summary: F-test for overall significance {.smaller}

**Goal:** Test the **overall significance** of the model

$$
Y_i = \beta_1  + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \e_i
$$

This means answering the question:

$$
\text{ Does at least one } X_i \text{ affect } Y \text{ ?}
$$


**Hypotheses:** The above question is equivalent to testing

\begin{align*}
H_0 & \colon \, \beta_2 = \beta_3 = \ldots = \beta_p = 0 \\
H_1 & \colon \text{ At least one of the } \beta_i \text{ is non-zero}
\end{align*}



## Summary: F-test for overall significance {.smaller}

- F-statistic is

\begin{align*}
F & = \frac{\RSS(1) - \RSS (p)}{ p - 1 } \bigg/ 
\frac{\RSS(p)}{n - p}  \\[15pt]
  & = \frac{R^2}{1 - R^2} \, \cdot \, \frac{n - p}{p - 1} 
\end{align*}

- We have that

$$
F \, \sim  \, F_{p-1,n-2}
$$





## Summary: F-test for overall significance {.smaller}

- The p-value is

$$
p = P ( F_{p-1,n-2} > F)
$$


- **t-test in R:**
    * Fit the multiple regression model with ``lm``
    * F-statistic is listed in the summary
    * p-value is listed in the summary





## Example: Stock and Gold prices {.smaller}

- Recall that 
    * $Y =$ Gold Price
    * $X =$ Stock Price

- We want to test the **overall significance** of the model

    $$
    Y_i =  \alpha + \beta x_i + \e_i 
    $$

- To this end, perform F-test for the hypotheses

\begin{align*}
H_0 & \colon \beta = 0 \\
H_1 & \colon \beta \neq 0
\end{align*}





## F-test for overall significance {.smaller}

- Recall that Stock Prices and Gold Prices are stored in R vectors
    * ``stock.price``
    * ``gold.price``


- Fit the simple linear model with the following commands

$$
\text{gold.price } = \alpha + \beta \, \times  \text{ stock.price } + \text{ error}
$$


```r
# Fit simple linear regression model
fit.model <- lm(gold.price ~ stock.price)

# Print result to screen
summary(fit.model)
```



## Output: F-statistic and p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  

- $\texttt{(Intercept)}$: 1st row of the table contains statistics related to $\hat \alpha$

- $\texttt{stock.price}$: 2nd row of the table contains statistics related to $\hat \beta$
    * For larger models there will be additional rows below the 2nd
    * These will be **informative** about additional regression parameters



## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  
- The 2nd row of the table has to be interpreted as follows


| $\texttt{Estimate}$   | $\text{The value of } \hat \beta$                  |
|:------------------    |:----------                                         |
| $\texttt{Std. Error}$ | Estimated standard error $\ese$ for $\beta$        |
| $\texttt{t value}$    | t-statistic $\, t = \dfrac{\hat \beta - 0 }{\ese}$|
| $\texttt{Pr(>|t|)}$   | p-value $\, p = 2 P( t_{n-2} > |t| )$             |
| $\texttt{*}$, $\, \texttt{**}$, $\, \texttt{***}$   | Statistical significance -- More stars is better |
: {tbl-colwidths="[35,65]"}




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  
- The above table then gives

$$
\hat \beta = -6.169 \,, \qquad \ese = 1.37 \, , \qquad t = - 4.502 \,, \qquad 
p = 8.9 \times 10^{-5} \
$$


- The $t$-statistic computed by R can also be computed by hand

$$
  t   = \frac{\hat \beta - 0}{ \ese }
      = \frac{-6.169}{1.37} = -4.502 
$$




## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::
  
- The p-value cannot be computed by hand

- However we can find critical value on [Tables](files/Statistics_Tables.pdf)

$$
n = \text{No. of data points} = 33 \,, \qquad  \text{df} = n - 2 = 31
$$

- Critical value is $\, t_{31}(0.025) = 2.040$

$$
|t| = 4.502 > 2.040 = t_{31}(0.025) \quad \implies \quad  p < 0.05
$$


## Output: $\ese$, t-statistic, p-value {.smaller}

:::: {.columns}

::: {.column width="100%"}
![](images/simple_regression_output_cut_3.png){width=80%}
:::

::::


**Interpretation:**

- $p$ is very small and rated $\, \texttt{***}$

- Strong evidence ($p<0.05$) that *Stock Prices* affect *Gold Prices*
- As $\hat \beta < 0$ and statistically significant, as *Stock Prices* increase *Gold Prices* decrease and vice versa







# Part 5: <br>Multiple linear <br> regression example  {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::









# Part B: <br> Regression modelling assumptions {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




# Part 1: <br> Extra sum of <br> squares principle {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## References