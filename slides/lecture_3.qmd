---
title: "Statistical Models"
subtitle: "Lecture 3"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::


# Lecture 3: <br>Random samples{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Outline of Lecture 3

1. Random samples
2. Unbiased estimators
3. Chi squared distribution
4. Sampling from normal distribution
5. t distribution
6. t test
7. R crash course





# Part 1: <br>Random samples {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Independent identically distributed random variables {.smaller}


::: Definition

The random variables $X_1,\ldots,X_n$ are **independent identically distributed** or **iid** with pdf or pmf $f(x)$ if

- $X_1,\ldots,X_n$ are mutually independent
- The marginal pdf or pmf of each $X_i$ satisfies
$$
f_{X_i}(x) = f(x) \,, \quad \forall \, x \in \R
$$

:::




## Random sample {.smaller}

- Suppose the data in an experiment consists of **observations** on a **population**
- Suppose the **population** has distribution $f(x)$
- Each observation is labelled $X_i$
- We always assume that the population is **infinite**
- Therefore each $X_i$ has distribution $f(x)$
- We also assume the observations are **independent**

::: Definition

The random variables $X_1,\ldots,X_n$ are a **random sample** of size $n$ from the population $f(x)$ if $X_1,\ldots,X_n$ are iid with pdf or pmf $f(x)$

:::



## Random sample {.smaller}


**Remark**: Let $X_1,\ldots,X_n$ be a random sample of size $n$ from the population $f(x)$. Then the joint distribution of $\XX = (X_1,\ldots,X_n)$ is
$$
f_{\XX}(x_1,\ldots,x_n) = f(x_1) \cdot \ldots \cdot f(x_n) = \prod_{i=1}^n f(x_i)
$$

::: Definition

We call $f_{\XX}$ the **joint sample distribution**

:::




## Random sample {.smaller}


**Notation**: 

- When the population distribution $f(x)$ depends on a parameter $\theta$ we write
$$
f = f(x|\theta)
$$

- In this case the joint sample distribution is 
$$
f_{\XX}(x_1,\ldots,x_n | \theta) =  \prod_{i=1}^n f(x_i | \theta)
$$



## Example {.smaller}

- Suppose a population has $\exponential(\beta)$ distribution
$$
f(x|\beta) = \frac{1}{\beta} e^{-x/\beta}
$$
- Suppose $X_1,\ldots,X_n$ is random sample from the population $f(x|\beta)$
- The joint sample distribution is then
\begin{align*}
f_{\XX}(x_1,\ldots,x_n | \beta) & = \prod_{i=1}^n f(x_i|\beta) \\
                                & = \prod_{i=1}^n \frac{1}{\beta} e^{-x_i/\beta} \\
                                & = \frac{1}{\beta^n} e^{-(x_1 + \ldots + x_n)/\beta}
\end{align*}




## Example {.smaller}

- We have 
$$
P(X_1 > 2) = \int_{2}^\infty f(x|\beta) \, dx 
           = \int_{2}^\infty \frac{1}{\beta} e^{-x/\beta} \, dx = e^{-2/\beta}
$$

- Thanks to iid assumption we can easily compute
\begin{align*}
P(X_1 > 2 , \ldots, X_n > 2) & = \prod_{i=1}^n P(X_i > 2) \\
                             & = \prod_{i=1}^n P(X_1 > 2) \\
                             & = P(X_1 > 2)^n \\
                             & = e^{-2n/\beta}
\end{align*}






# Part 2: <br>Unbiased estimators {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Point estimation {.smaller}

Suppose to have a population with distribution
$$
f(x|\theta)
$$

- $\theta$ is generally **unknown**
- Knowledge of $\theta$ is sufficient to characterize $f(x|\theta)$

**Example**: A population could be normally distributed
$$
f(x|\mu,\sigma^2)
$$

- Here $\mu$ is the mean and $\sigma^2$ the variance
- Knowing $\mu$ and $\sigma^2$ completely characterizes the normal distribution




## Point estimation {.smaller}


::: Definition 

Suppose the population has distribution
$$
f(x|\theta)
$$

- **Point estimation** is the procedure of estimating $\theta$ from random sample
- A **point estimator** is any function of a random sample
$$
W(X_1,\ldots,X_n)
$$

:::

**Notation**: Point estimators are also called **statistics**





## Unbiased estimator {.smaller}

::: Definition

Suppose $W$ is a point estimator of a parameter $\theta$

- The **bias** of $W$ is the quantity $\rm{Bias}_{\theta} := \Expect[W] - \theta$

- $W$ is an **unbiased estimator** if $\rm{Bias}_{\theta} = 0$, that is,
$$
\Expect[W] = \theta
$$

:::


**Note**: A point estimator 
$$
W = W(X_1, \ldots, X_n)
$$
is itself a random variable. Thus $\Expect[W]$ is the mean of such random variable





## Examples {.smaller}

We give two examples of unbiased estimators:

- Sample mean
- Sample variance




## Estimating the mean {.smaller}

::: Problem

Suppose to have a population 
$$
f(x|\theta)
$$
We want to estimate the **population mean**
$$
\mu := \int_{\R} x f(x|\theta) \, dx
$$

:::





## Sample mean {.smaller}


::: Definition

The **sample mean** of a random sample $X_1,\ldots,X_n$ is the statistic
$$
W(X_1,\ldots,X_n) := \overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$

:::




## Sample mean {.smaller}
### Sample mean is unbiased estimator of mean

::: Theorem

The sample mean $\overline{X}$ is an unbiased estimator of the population mean $\mu$, that is,
$$
\Expect[\overline{X}] = \mu
$$

:::



## Sample mean {.smaller}
### Proof of theorem


- $X_1,\ldots,X_n$ is a random sample from $f(x|\theta)$
- Therefore $X_i \sim f(x|\theta)$ and
$$
\Expect[X_i] = \int_{\R} x f(x|\theta) \, dx = \mu
$$
- By linearity of expectation we have
$$
\Expect[\overline{X}] = \frac{1}{n} \sum_{i=1}^n \Expect[X_i] 
                      = \frac{1}{n} \sum_{i=1}^n \mu 
                      = \mu
$$




## Sample mean {.smaller}
### Computing the variance

It is useful to compute the variance of the sample mean $\overline{X}$

::: Lemma

$X_1,\ldots,X_n$ random sample from population with mean $\mu$ and 
variance $\sigma^2$. Then
$$
\Var[\overline{X}] = \frac{\sigma^2}{n}
$$
:::




## Sample mean {.smaller}
### Proof of Lemma

- By assumption we have
$$
\Expect[X_i] = \mu \,, \quad \Var[X_i] = \sigma^2
$$

- Since the variance is quadratic and $X_i$ are independent
\begin{align*}
\Var[\overline{X}] & = \Var \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] 
                     = \frac{1}{n^2} \sum_{i=1}^n \Var[X_i] \\
                   & = \frac{1}{n^2} \cdot n \sigma^2
                     = \frac{\sigma^2}{n} 
\end{align*}





## Estimating the variance {.smaller}

::: Problem

Suppose to have a population 
$$
f(x|\theta)
$$
with mean $\mu$ and variance $\sigma^2$. We want to estimate the **population variance**

:::




## Sample variance {.smaller}

::: Definition

The **sample variance** of a random sample $X_1,\ldots,X_n$ is the statistic
$$
S^2 := \frac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2 
$$
where $\overline{X}$ is the sample mean
$$
\overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$

:::



## Sample variance {.smaller}
### Equivalent formulation


::: Proposition

It holds that
$$
S^2 := \frac{ \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2}{n-1}  = 
\frac{ \sum_{i=1}^n  X_i^2  - n\overline{X}^2  }{n-1}
$$

:::



## Sample variance {.smaller}
### Proof of Proposition

- We have
\begin{align*}
\sum_{i=1}^n \left( X_i - \overline{X}  \right)^2  & =
\sum_{i=1}^n \left(X_i^2 + \overline{X}^2 - 2 X_i \overline{X}  \right) 
 = \sum_{i=1}^n X_i^2 + n\overline{X}^2 - 2  \overline{X}  \sum_{i=1}^n X_i \\ 
& = \sum_{i=1}^n X_i^2 + n\overline{X}^2 - 2 n \overline{X}^2 
 = \sum_{i=1}^n X_i^2 -n \overline{X}^2 
\end{align*}


- Dividing by $n-1$ yields the desired identity
$$
S^2 = \frac{ \sum_{i=1}^n X_i^2 -n \overline{X}^2 }{n-1}
$$





## Sample variance {.smaller}
### Sample variance is unbiased estimator of variance

::: Theorem

The sample variance $S^2$ is an unbiased estimator of the population variance $\sigma^2$, that is,
$$
\Expect[S^2] = \sigma^2
$$

:::






## Sample variance {.smaller}
### Proof of theorem


- $X_1,\ldots,X_n$ is a random sample from $f(x|\theta)$


- By linearity of expectation we infer
\begin{align*}
\Expect[(n-1)S^2] & = \Expect \left[  \sum_{i=1}^n X_i^2 - n\overline{X}^2  \right] \\
                  & = \sum_{i=1}^n \Expect[X_i^2] - n \Expect[\overline{X}^2] 
\end{align*}




## Sample variance {.smaller}
### Proof of theorem


- Since $X_i \sim f(x|\theta)$, we have
$$
\Expect[X_i] = \mu \,, \quad \Var[X_i] = \sigma^2
$$

- Therefore by definition of variance we get
$$
\Expect[X_i^2] = \Var[X_i] + \Expect[X]^2 = \sigma^2 + \mu^2
$$





## Sample variance {.smaller}
### Proof of theorem


- Also recall that
$$
\Expect[\overline{X}] = \mu \,, \quad \Var[\overline{X}] = \frac{\sigma^2}{n} 
$$


- By definition of variance we get
$$
\Expect[\overline{X}^2] = \Var[\overline{X}] + \Expect[\overline{X}]^2
                        = \frac{\sigma^2}{n} + \mu^2
$$




## Sample variance {.smaller}
### Proof of theorem


- Hence
\begin{align*}
\Expect[(n-1)S^2] & = \sum_{i=1}^n \Expect[X_i^2] - n \Expect[\overline{X}^2] \\
                  & = \sum_{i=1}^n \left( \mu^2 + \sigma^2 \right) - 
                      n \left(  \mu^2 + \frac{\sigma^2}{n}  \right)  \\
                  & = n\mu^2 + n\sigma^2 - 
                      n \mu^2 - \sigma^2   \\
                  & = (n-1) \sigma^2
\end{align*}

- Dividing both sides by $(n-1)$ yields the thesis
$$
\Expect[S^2] = \sigma^2
$$








## Notation {.smaller}


- The realization of a random sample $X_1,\ldots,X_n$ is denoted by
$$
x_1, \ldots, x_n
$$

- The realization of the sample mean $\overline{X}$ is denoted
$$
\overline{x} := \frac{1}{n} \sum_{i=1}^n x_i
$$

- The realization of the sample variance $S^2$ is denoted 
$$
s^2=\frac{\sum_{i=1}{n}(x_i-\overline{x})^2}{n-1}=\frac{\sum_{i=1}^n x_i^2-n{\overline{x}^2}}{n-1}
$$






## Additional note {.smaller}

- The $n-1$ factor in the denominator of the sample variance estimator
$$
s^2=\frac{\sum_{i=1}{n}(x_i-\overline{x})^2}{n-1}=\frac{\sum_{i=1}^n x_i^2-n{\overline{x}^2}}{n-1}
$$
is caused by a loss of precision:
    * We have to estimate $\mu$ with the sample mean $\overline{x}$

- This leads to a general statistical rule:
$$
\text{Lose 1 degree of freedom for each parameter estimated}
$$

- In this we have to estimate one parameter (the sample mean \overline{x}). Hence
\begin{align*}
\text{degrees of freedom} & = \text{Sample size}-\text{No. of estimated parameters} \\
                          & = n-1
\end{align*}




## Example calculation {.smaller}

- Wage data on 10 Advertising Professionals Accountants

|**Professional**| $x_1$ | $x_2$| $x_3$ | $x_4$ | $x_5$ | $x_6$ | $x_7$ | $x_8$ | $x_9$ |$x_{10}$|
|:--------------:|:-----:|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:----:|:-----:|
|     **Wage**   |   36  |  40  |  46  |  54  |  57  |  58  |  59  |  60  |  62  |  63  |


<br>

- **Task**: Estimate **population mean** and **variance**




## Solution to the example {.smaller}

- **Number** of advertising professionals $n=10$

- **Sample Mean:**
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i = \frac{36+40+46+{\dots}+62+63}{10}=\frac{535}{10}=53.5
$$

- **Sample Variance**:
\begin{align*}
  s^2 &  = \frac{\sum_{i=1}^n  x_{i}^2 - n \overline{x}^2}{n-1} \\
  \sum_{i=1}^n x_i^2 & = 36^2+40^2+46^2+{\ldots}+62^2+63^2 = 29435 \\
  s^2 & = \frac{29435-10(53.5)^2}{9} = 90.2778
\end{align*}







# Part 3: <br>Chi squared distribution {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::




## Overview {.smaller}

Chi-squared distribution:

- defined in terms of squares of $N(0, 1)$ random variables
- designed to describe variance estimation
- used to define other members of the normal family
    * Student $t$ distribution
    * $F$-distribution





## Why the normal family is important {.smaller}

- Classical hypothesis testing and regression problems
- The same maths solves apparently unrelated problems
- Easy to compute
    * Statistics tables
    * Software
- Enables the development of approximate methods in more complex (and interesting) problems





## Reminder: Normal distribution {.smaller}


- $X$ has **normal distribution** with mean $\mu$ and variance $\sigma^2$ if pdf is
$$
f(x) := \frac{1}{\sqrt{2\pi\sigma^2}} \, \exp\left( -\frac{(x-\mu)^2}{2\sigma^2}\right) \,, \quad x \in \R
$$


- In this case we write
$$
X \sim N(\mu,\sigma^2)
$$ 

- The **standard normal distribution** is denoted $N(0,1)$




## Chi-squared distribution {.smaller}
### Definition

::: Definition

Let $Z_1,\ldots,Z_r$ be iid $N(0, 1)$ random variables. The **chi-squared distribution** with **$r$ degrees of freedom** is the distribution
$$
\chi^2_r \sim  Z^2_1+...+Z^2_r
$$

:::


## Chi-squared distribution {.smaller}
### Pdf characterization 


::: Theorem

The $\chi^2_r$ distribution is equivalent to a Gamma distribution
$$
\chi^2_r \sim \Gamma(r/2, 1/2)
$$
Therefore the pdf of $\chi^2_r$ can be written in closed form as
$$
f_{\chi^2_r}(x)=\frac{x^{(r/2)-1} \, e^{-x/2}}{\Gamma(r/2) 2^{r/2}} \,, \quad x>0
$$

:::





## Chi-squared distribution {.smaller}
### Plots of chi-squared pdf for different choices of r


```{r}
# Set the degrees of freedom
degree1 <- 1
degree2 <- 3
degree3 <- 6

# Generate values for the x-axis
x_values <- seq(0.1, 10, by = 0.1)

# Calculate the probability density function (PDF) values for each x for the 3 distributions
pdf_values1 <- dchisq(x_values, df = degree1)
pdf_values2 <- dchisq(x_values, df = degree2)
pdf_values3 <- dchisq(x_values, df = degree3)


# Plot the gamma PDFs
plot(x_values, 
    pdf_values1, 
    type = "l", 
    col = "blue", lwd = 2,
    xlab = "", 
    ylab = "",
    ylim = c(0, max(pdf_values2, pdf_values3) + 0.1))

lines(x_values, 
      pdf_values2, 
      col = "red", 
      lwd = 2)

lines(x_values, 
      pdf_values3, 
      col = "black", 
      lwd = 2)

mtext("x", side=1, line=3, cex=2)
mtext("pdf", side=2, line=2.5, cex=2)

# Add a legend
legend("topright", legend = c(paste("r =", degree1),
                              paste("r =", degree2),
                              paste("r =", degree3)),
       col = c("blue", "red", "black"), lty = 1, lwd = 2, cex = 1.5)

``` 




## Chi-squared distribution {.smaller}
### Proof of Theorem

- We start with case the $r=1$
- Need to prove that
$$
\chi^2_1 \sim \Gamma(1/2, 1/2)
$$
- Therefore we need to show that the pdf of $\chi^2_1$ is
$$
f_{\chi^2_1}(x)=\frac{x^{-1/2} \, e^{-x/2}}{\Gamma(1/2) 2^{1/2}} \,, \quad x>0
$$


## Chi-squared distribution {.smaller}
### Proof of Theorem

- To this end, notice that by definition
$$
\chi^2_1 \sim Z^2 \,, \qquad Z \sim N(0,1)
$$
- Hence, for $x>0$ we can compute **cdf** via
\begin{align*}
F_{\chi^2_1}(x) & = P(\chi^2_1 \leq x) \\
                & = P(Z^2 \leq x ) \\
                & = P(- \sqrt{x} \leq Z  \leq \sqrt{x} ) \\
                & = 2 P (Z \leq \sqrt{x})
\end{align*}
where in the last equality we used symmetry of $Z$ around $x=0$



## Chi-squared distribution {.smaller}
### Proof of Theorem

- Recalling the definition of **standard normal pdf** we get
\begin{align*}
F_{\chi^2_1}(x) & = 2 P (Z \leq \sqrt{x}) \\
                & = 2 \frac{1}{\sqrt{2\pi}} 
                    \int_0^{\sqrt{x}} e^{-t^2/2} \, dt \\
                & = 2 \frac{1}{\sqrt{2\pi}} G( \sqrt{x} )
\end{align*}
where we set 
$$
G(x) := \int_0^{x} e^{-t^2/2} \, dt
$$



## Chi-squared distribution {.smaller}
### Proof of Theorem

- We can now compute pdf of $\chi_1^2$ by differentiating the cdf

- By the Fundamental Theorem of Calculus we have
$$
G'(x) = \frac{d}{dx} \left( \int_0^{x} e^{-t^2/2} \, dt \right) = e^{-x^2/2} \quad \implies \quad 
G'(\sqrt{x}) = e^{-x/2}
$$

- Chain rule yields
\begin{align*}
f_{\chi^2_1}(x) & = \frac{d}{dx} F_{\chi^2_1}(x) =
\frac{d}{dx} \left(  2 \frac{1}{\sqrt{2\pi}} G( \sqrt{x} )  \right)  \\
& = 2 \frac{1}{\sqrt{2\pi}} G'( \sqrt{x} ) \frac{x^{-1/2}}{2} 
= \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \sqrt{\pi}}
\end{align*}


## Chi-squared distribution {.smaller}
### Proof of Theorem

- It is well known that 
$$
\Gamma(1/2) = \sqrt{\pi}
$$
- Hence we conclude
$$
f_{\chi^2_1}(x) = \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \sqrt{pi}} = \frac{x^{-1/2} e^{-x/2}}{2^{1/2} \Gamma(1/2)}
$$
- This shows
$$
\chi_1^2 \sim \Gamma(1/2,1/2)
$$




## Chi-squared distribution {.smaller}
### Proof of Theorem

- Now consider the case $r \geq 2$. We need to prove that
$\chi^2_r \sim \Gamma(r/2, 1/2)$

- By definition
$$
\chi^2_r \sim Z^2_1 + \ldots + Z^2_r \,, \qquad 
\chi^2_1 \sim Z_1^2
$$
- It follows that a $\chi^2_r$ random variable can be constructed as 
$$
\chi^2_r = \sum_{i=1}^r X_i, \qquad X_i \sim \chi^2_1
$$
- By the Theorem in Slide 80 in Lecture 2 we have 
$$
Z_1,\ldots,Z_r \,\,\, \text{iid} \quad \implies \quad
Z_1^2,\ldots,Z_r^2 \,\,\, \text{iid} \quad \implies \quad
X_1,\ldots,X_r \,\,\,  \text{iid}
$$


## Chi-squared distribution {.smaller}
### Proof of Theorem


- Therefore 
$$
\chi^2_r = \sum_{i=1}^r X_i \,, \qquad X_i \sim \chi_1^2 \,\, \,\text{ iid}
$$ 

- Recall that the MGF of sum of iid random variables is the product of the MGFs - see Theorem in Slide 81 of Lecture 2

- Thus the MGF of $\chi^2_r$ can be calculated as 
$$
M_{\chi_r^2}(t) = \prod_{i=1}^r M_{X_i} (t) 
=  \left[M_{\chi^2_1}(t) \right]^r
$$





## Chi-squared distribution {.smaller}
### Proof of Theorem


- We have proven that $\chi_1^2 \sim \Gamma(1/2,1/2)$

- In Lecture 1 we have also computed that 
$$
Y \sim \Gamma(\alpha,\beta) \qquad \implies \qquad 
M_Y(t) = \frac{\beta^\alpha}{(\beta - t)^\alpha}
$$

- Hence 
$$
M_{\chi_1^2}(t) =  \frac{2^{-1/2}}{(1/2 - t)^{1/2}}
$$




## Chi-squared distribution {.smaller}
### Proof of Theorem


- Therefore 
$$
M_{\chi_r^2}(t) = \left[M_{\chi^2_1}(t) \right]^r 
                = \left[  
                    \frac{2^{-1/2}}{ (1/2 - t)^{1/2}}
                     \right]^r
                = \frac{2^{-r/2}}{(1/2 - t)^{r/2}}
$$

- The above is the MGF of a $\Gamma(r/2,1/2)$

- Since MGF characterizes a distribution, we conclude our thesis
$$
\chi_r^2 \sim \Gamma(r/2,1/2)
$$





# Part 4: <br>Sampling from normal distribution {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::





## Sampling from Normal distribution {.smaller}


**Sample mean and variance**: For a random sample $X_1,\ldots,X_n$ defined by
$$
S^2 := \frac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X}  \right)^2 \,, \qquad 
\overline{X} := \frac{1}{n} \sum_{i=1}^n X_i
$$


::: Question

Assume the sample is normal
$$
X_i \sim N(\mu,\sigma^2) \,, \quad \forall \, i = 1 , \ldots, n
$$
What are the distributions of $\overline{X}$ and $S^2$?

:::




## Properties of Sample Mean and Variance {.smaller}

::: Theorem

Let $X_1,\ldots,X_n$ be a random sample from $N(\mu,\sigma^2)$. Then

- $\overline{X}$ and $S^2$ are independent random variables
- $\overline{X}$ and $S^2$ are distributed as follows
$$
\overline{X} \sim  N(\mu,\sigma^2/n) \,, \qquad
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$


:::



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- To prove independence of $\overline{X}$ and $S^2$ we make use of the following lemma
- Proof of this Lemma is technical and omitted
- For a proof see Lemma 5.3.3 in [@casella-berger]

::: Lemma

Let $X$ and $Y$ be normal random variables. Then
$$
X \text{ and } Y \text{ independent } \quad \iff \quad 
\Cov(X,Y) = 0 
$$

:::




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem


- We now apply the Lemma to $X_i - \overline X$ and $\overline{X}$

- Note that $X_i - \overline{X}$ and $\overline{X}$ are 
normally distributed, being sums of iid normals

- Therefore we can apply the Lemma

- To this end, recall that $\Var[\overline X] = \sigma^2/n$

- Also note that, by independence of $X_1,\ldots,X_n$
$$
\Cov(X_i,X_j) = 
\begin{cases}
\Var[X_i] & \text{ if } \, i = j \\
0         & \text{ if } \, i \neq j \\
\end{cases}
$$



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- Using bilinearity of covariance (i.e. linearity in both arguments)
\begin{align*}
\Cov(X_i - \overline X, \overline X) & = \Cov(X_i,\overline{X}) - \Cov(\overline X,\overline{X}) \\
& = \frac{1}{n} \sum_{j=1}^n \Cov(X_i,X_j) - \Var[\overline X] \\
& = \frac{1}{n} \Var[X_i] - \Var[\overline X] \\
& = \frac{1}{n} \sigma^2 - \frac{\sigma^2}{n} = 0
\end{align*}

- By the Lemma we infer independence of $X_i - \overline X$ and $\overline X$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- We have shown 
$$
X_i - \overline X \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$

- By the Theorem in Slide 80 in Lecture 2 we hence have 
$$
(X_i - \overline X)^2 \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$


- By the same Theorem we also get
$$
\sum_{i=1}^n (X_i - \overline X)^2 = (n-1)S^2 \quad \text{and} \quad  \overline X \quad 
\text{independent}
$$

- Again the same Theorem, finally implies independence of
$S^2$ and $\overline X$





## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem


- We now want to show that $\overline{X} \sim  N(\mu,\sigma^2/n)$

- We are assuming that $X_1,\ldots,X_n$ are iid with 
$$
\Expect[X_i] = \mu \,, \qquad \Var[X_i] = \sigma^2
$$

- We have already seen in Slides 15 and 17 that, in this case,
$$
\Expect[\overline X] = \mu \,, \quad \Var[\overline{X}] = \frac{\sigma^2}{n}
$$

- Sum of independent normals is normal (see Theorem in slide 82 in Lecture 2)

- Therefore $\overline{X}$ is normal, with mean $\mu$ and variance $\sigma^2/n$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- We are left to prove that 
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$
    * This is somewhat technical and we don't actually prove it 
    * For a proof see Theorem 5.3.1 in [@casella-berger]
    * We however want to provide some intuition on why it holds

- Recall that the chi-squared distribution with $r$ degrees of freedom is
$$
\chi_r^2 \sim Z_1^2 + \ldots + Z_r^2
$$
with $Z_i$ iid and $N(0,1)$




## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- By definition of $S^2$ we have
$$
\frac{(n-1)S^2}{\sigma^2}  = 
\sum_{i=1}^n \frac{(X_i - \overline X)^2}{\sigma^2}
$$

- If we replace the sample mean $\overline X$ with the actual mean $\mu$ we get the approximation
$$
\frac{(n-1)S^2}{\sigma^2}  = 
\sum_{i=1}^n \frac{(X_i - \overline X)^2}{\sigma^2} \approx
\sum_{i=1}^n \frac{(X_i - \mu)^2}{\sigma^2}
$$



## Properties of Sample Mean and Variance {.smaller}
### Proof of Theorem

- Since $X_i \sim N(\mu,\sigma^2)$, we have that
$$
Z_i := \frac{X_i - \mu}{\sigma} \sim N(0,1)
$$

- Therefore 
$$
\frac{(n-1)S^2}{\sigma^2}   \approx \sum_{i=1}^n \frac{(X_i - \mu)^2}{\sigma^2} = \sum_{i=1}^n Z_i^2 \sim \chi_n^2
$$

- This is just an approximation: Replacing $\mu$ with $\overline X$ **loses of 1 degree of freedom**
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$





# Part 5: <br>t distribution{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Estimating the Mean  {.smaller}

::: Problem

We want to estimate the mean $\mu$ of a **normal  population**

:::

**What to do?**

- We can collect normal samples $X_1, \ldots, X_n$ with
$X_i \sim N(\mu,\sigma^2)$

- We then compute the sample mean
$$
\overline X := \frac{1}{n} \sum_{i=1}^n X_i
$$
- We know that $\Expect[\overline X] = \mu$



## Estimating the Mean  {.smaller}


Therefore we expect $\overline X$ to approximate $\mu$

::: Question

How good is this approximation? How to quantify it?

:::


**Answer**: We could look at the **Test Statistic**
$$
T := \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
$$

- We know that $T \sim N(0,1)$
- If $\sigma$ is known, then the only unknown in $T$ is $\mu$ 
- Therefore $T$ can be used to estimate $\mu$





## Estimating the Mean  {.smaller}

::: Question

In what way can we use the test statistic
$$
T = \frac{\overline{X}-\mu}{\sigma/\sqrt{n}}
$$ 
to estimate $\mu$?

:::


**Answer**: Hypothesis testing





## Estimating the Mean  {.smaller}
### Hypothesis testing algorithm

- We suppose that $\mu=\mu_0$ for some guessed $\mu_0   \qquad$ ($\mu_0$ is called **null hypothesis**)
- Using the data collected $\XX = (X_1,\ldots,X_n)$ we compute
$$
t := \frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}} \,, \qquad 
\overline{X} = \frac{1}{n} \sum_{i=1}^n X_i
$$
- Since $T \sim N(0,1)$, we can compute the probability of $T$ being close to $t$
$$
p := P(T \approx t)
$$





## Estimating the Mean  {.smaller}
### Hypothesis testing algorithm



Given the value $p := P(T \approx t)$ we have 2 options:

- $p$ is small $\quad \implies \quad$  **reject** $\mu_0$
    * $p$ small means it is unlikely to observe such value of $t$
    * Recall that $t$ depends on the data $\XX$ and our guess $\mu_0$
    * We conclude that our guess must be wrong $\quad \implies \quad \mu \neq \mu_0$

- $p$ is large $\quad \implies \quad$  **do not reject** $\mu_0$
    * $p$ small means that $t$ happens with reasonably high probability
    * There is no reason to believe our guess $\mu_0$ was wrong
    * But we also do not have sufficient reason to believe $\mu_0$ was correct



## Estimating the Mean  {.smaller}

- We know that $T \sim N(0,1)$
- Therefore we can compute
$$
p := P(T \geq t) + P(T \leq t)
$$
$$
T = \frac{\overline{X}-\mu_0}{\sigma/\sqrt{n}}
$$


## Estimating the Mean  {.smaller}

::: Problem

In practice we often do not know the population variance $\sigma^2$

:::





# Part 6: <br>t test{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::








# Part 7: <br>R crash course{background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::







[Install R](https://cloud.r-project.org)

[RStudio](https://posit.co/download/rstudio-desktop/)

[VisualStudio Code](https://code.visualstudio.com/docs/languages/r)





## References