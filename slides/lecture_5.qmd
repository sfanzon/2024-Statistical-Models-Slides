---
title: "Statistical Models"
subtitle: "Lecture 5"
from: markdown+emoji
author: 
    - name: Dr. Silvio Fanzon
      id: sf
      email: S.Fanzon@hull.ac.uk
      url: https://www.silviofanzon.com
      affiliations: University of Hull
    - name: Dr. John Fry
      id: jf
      email: J.M.Fry@hull.ac.uk
      url: https://www.hull.ac.uk/staff-directory/john-fry
      affiliations: University of Hull
---



::: {.content-hidden}
$
{{< include macros.tex >}}
$
:::




# Lecture 5: <br>Hypothesis tests in R <br>Part 2 {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::






## Outline of Lecture 5

1. One-sample variance test
2. F distribution






# Part 1: <br>One-sample variance test {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Estimating mean and variance {.smaller}

- Suppose to have a population with normal distribution $N(\mu,\sigma^2)$
    * Mean $\mu$ and variance $\sigma^2$ are **unknown**


- **Questions** about $\mu$ and $\sigma^2$
    1. What is my best guess of the value?
    2. How far away from the true value am I likely to be?


- **Answers:**
    * The one-sample **t-test** answers questions about $\mu$
    * The one-sample **variance ratio test** answers questions about $\sigma^2$





## Chi-squared distribution {.smaller}

- The-one sample variance test uses **chi-squared distribution**

- **Recall:** Chi-squared distribution with $p$ degrees of freedom is
$$
\chi_p^2 = Z_1^2 + \ldots + Z_p^2
$$
where $Z_1, \ldots, Z_n$ are iid $N(0, 1)$





## One-sample one-sided variance ratio test {.smaller}
### Goal
 

- Estimate variance $\sigma^2$ of normal population $N(\mu,\sigma^2)$

- Suppose $\sigma_0$ is guess for $\sigma$ 

- The one-sided hypotheses test is
$$
H_0 \colon \sigma \leq \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$



## One-sample one-sided variance ratio test {.smaller}
### What to do?

- Consider the sample variance $S^2$

- The sample variance $S^2$ cannot be too far from the true variance $\sigma$

- Since we believe $H_0$, true variance is 
$$
\sigma \leq \sigma_0
$$

- Therefore we **cannot** have that 
$$
S^2 \gg \sigma_0^2
$$
(bacause then $S^2 \gg \sigma^2$)



## One-sample one-sided variance ratio test {.smaller}
### What to do?


- Therefore we **reject** $H_0$ if
$$
S^2 \gg \sigma_0^2
$$



- The **rejection** condition $S^2 \gg \sigma_0^2$ is equivalent to 
$$
\frac{(n-1)S^2}{\sigma_0^2}  \gg 1
$$




## One-sample one-sided variance ratio test {.smaller}
### What to do?


- Assuming $\sigma=\sigma_0$, the **rejection** condition becomes
$$
\frac{(n-1)S^2}{\sigma_0^2} = \frac{(n-1)S^2}{\sigma^2}  \gg 1
$$


- Recall that 
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$




## One-sample one-sided variance ratio test {.smaller}
### What to do?

- We define our **test statistic** as
$$
\chi^2 := \frac{(n-1)s^2}{\sigma_0^2}
$$

- We **reject** $H_0$ if $\chi^2$ is too large

- As $\chi^2$ is observed from $\chi_{n-1}^2$, we decide to rejct if
$$
\chi^2 > \chi_{n-1}^2(0.05)   
$$


- By definition the **critical value** $\chi_{n-1}^2(0.05)$ is such that
$$
P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) ) = 0.05
$$




## One-sample one-sided variance ratio test {.smaller}
### Critical values

- $x^* := \chi_{n-1}^2(0.05)$ is point on $x$-axis such that $P(\chi_{n-1}^2 > x^* ) = 0.05$

- In the picture we have $n = 12$ and $\chi_{11}^2(0.05) = 19.68$

```{r}
# Degrees of freedom
df <- 11

# Values for x-axis
x <- seq(0, 30, length.out = 1000)  # Adjust the range according to chi-squared distribution

# Calculate PDF of chi-squared distribution
pdf <- dchisq(x, df)

# Plot PDF
plot(x, pdf, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "Density")


# Shade area where p-value > 0.95
x_fill_right <- x[x >= qchisq(0.95, df)]
y_fill_right <- pdf[x >= qchisq(0.95, df)]
polygon(c(x_fill_right, rev(x_fill_right)), c(y_fill_right, rep(0, length(y_fill_right))), col = "gray", border = NA)

# Add critical value for 0.95 quantile
x_star <- qchisq(0.95, df)
points(x_star, dchisq(x_star, df), col = "red", pch = 19, cex = 1.3)

# Add text above x_star containing its value
text(x_star * 1.05, dchisq(x_star, df) * 1.35, paste("x* =", round(x_star, 2)), pos = 3, col = "red", cex = 1.3)

# Add legend
legend("topright", legend = c("area = 0.05"), fill = "gray", cex = 1.3)

```




## One-sample one-sided variance ratio test {.smaller}
### Critical values of chi-squared -- Table 13.5 ([file here](files/Statistics_Tables.pdf))

- Look at the row with Degree of Freedom $n-1$ (or its closest value)
- Find **critical value** $\chi^2_{n-1}(0.05)$ in column $\alpha = 0.05$
- **Example**: $n=12$, DF $=11$, $\chi^2_{11}(0.05) = 19.68$

![](images/chi_squared_test_statistic_table.png){width=82%}




## One-sample one-sided variance ratio test {.smaller}
### p-value


- Given the test statistic $\chi^2$ the **p-value** is defined as
$$
p := P( \chi_{n-1}^2 > \chi^2 )
$$


- Notice that 
$$
p < 0.05 \qquad \iff \qquad  \chi^2 > \chi_{n-1}^2(0.05)
$$

> This is because $\chi^2 > \chi_{n-1}^2(0.05)$ iff
> $$
> p = P(\chi_{n-1}^2 > \chi^2)
>  < P(\chi_{n-1}^2 > \chi_{n-1}^2(0.05) )
>  = 0.05
> $$




## One-sample one-sided variance ratio test {.smaller}
### Computing p-value in R

- We need to compute p-value for chi-squared distribution
- R has built in probability distrubutions accessible with commands:



## One-sample one-sided variance ratio test {.smaller}
### Conclusion

Suppose $\sigma_0$ is guess for $\sigma$. The one-sided hypothesis is
$$
H_0 \colon \sigma \leq \sigma_0 \qquad H_1 \colon \sigma > \sigma_0
$$
The **variance ratio test** or **chi-squared test** consists of 3 steps:

1. **Calculation**: Compute the chi-squared statistics
$$
\chi^2 = \frac{(n-1) s^2}{\sigma_0^2}
$$
where sample mean and variance are
$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i \,, \qquad 
s^2 = \frac{\sum_{i=1}^n x_i^2 - n \overline{x}^2}{n-1}
$$



## One-sample one-sided variance ratio test {.smaller}
### Conclusion

2. **Tables or R**: Find either
    * Critical value in [Table 13.5](files/Statistics_Tables.pdf)
    $$
    \chi_{n-1}^2(0.05)
    $$
    * p-value in R
    $$
    p := P( \chi_{n-1}^2 > \chi^2 )
    $$




## One-sample one-sided variance ratio test {.smaller}
### Conclusion

3. **Interpretation**:
    * Reject $H_0$ if 
    $$
    \chi^2 > \chi_{n-1}^2(0.05)  \qquad \text{ or } \qquad p < 0.05
    $$
    * Do not reject $H_0$ if 
    $$
    \chi^2 \leq \chi_{n-1}^2(0.05) \qquad \text{ or } \qquad 
    p \geq 0.05
    $$





## One-sample Two-sided t-test {.smaller}
### p-value

- $p<0.05$ means that the test statistic $t$ is **extreme**: $\,\, P(t_{n-1}> |t|)<0.025$

- $t$ falls in the **grey areas** in the $t_{n-1}$ plot below: Each grey area measures $0.025$


```{r}
# Degrees of freedom
df <- 11

# Values for x-axis
x <- seq(-4, 4, length.out = 1000)

# Calculate PDF of t-distribution
pdf <- dt(x, df)

# Plot PDF
plot(x, pdf, type = "l", col = "blue", lwd = 2, xlab = "x", ylab = "Density")

# Shade area where p-value < 0.025
x_fill_left <- x[x <= qt(0.025, df)]
y_fill_left <- pdf[x <= qt(0.025, df)]
polygon(c(x_fill_left, rev(x_fill_left)), c(y_fill_left, rep(0, length(y_fill_left))), col = "gray", border = NA)

# Shade area where p-value > 0.975
x_fill_right <- x[x >= qt(0.975, df)]
y_fill_right <- pdf[x >= qt(0.975, df)]
polygon(c(x_fill_right, rev(x_fill_right)), c(y_fill_right, rep(0, length(y_fill_right))), col = "gray", border = NA)

# Add critical value on plot
text(qt(0.975, df), dt(qt(0.975, df), df) + 0.05, paste("x* =", round(qt(0.975, df), 4)), pos = 3, col = "red", cex = 1.3, adj = c(5.5, 4.5))

# Add legend
legend("topright", legend = c("area = 0.025 x 2"), fill = "gray", cex = 1.3)


```






## One-sample Two-sided t-test {.smaller}
### p-value


- How to compute $p$?
    * Use statistical tables -- Available [here](files/Statistics_Tables.pdf)
    * Use R -- Next sections




## One-sample Two-sided t-test {.smaller}
### Reference statistical tables


Find the table **One-sided critical values of the student t distribution** in this
[file](files/Statistics_Tables.pdf)

- Look at the row with Degree of Freedom $n-1$ (or its closest value)
- Find **critical value** $t^* := t_{n-1}(0.025)$ in column $0.025$
- **Example**: $n=10$, DF $=9$, $t^*=t_{9}(0.025)=2.262$

![](images/t_test_statistic_table.png){width=82%}





## One-sample Two-sided t-test {.smaller}
### Reference statistical tables 


- The critical value $t^* = t_{n-1}(0.025)$ found in the table satisfies
$$
P(t_{n-1}>t^*) = 0.025
$$

- By definition of $p$-value for two-sided t-test we have
$$
p := 2P(t_{n-1}>|t|) 
$$

- Therefore, for $|t|>t^*$
\begin{align*}
p & := 2P(t_{n-1}>|t|) \\
  & <  2P(t_{n-1}>t^*) = 2 \cdot (0.025) = 0.05
\end{align*}

- **Conclusion**: $\qquad |t|>t^* \quad \iff \quad p<0.05$




## One-sample Two-sided t-test {.smaller}
### Interpretation


Recall that $p = 2P ( \text{Observing t } | \mu = \mu_0)$. We have two possibilities: 

- $|t|>t^*$
  * In this case $p<0.05$
  * The observed statistic $t$ is very unlikely under $H_0$
  * We **reject** $H_0$


- $|t| \leq t^*$
  * In this case $p>0.05$
  * The observed statistic $t$ is not unlikely under $H_0$
  * We **do not reject** $H_0$








# Part 2: <br>F distribution {background-color="#cc0164" visibility="uncounted"}

::: footer

<div color="#cc0164">  </div>

:::



## Overview {.smaller}

**Recall**: Chi-squared distribution with $p$ degrees of freedom is
$$
\chi_p^2 = Z_1^2 + \ldots + Z_p^2
$$
where $Z_1, \ldots, Z_n$ are iid $N(0, 1)$




## Overview {.smaller}

Chi-squared distribution was used to:

- Describe distribution of sample variance $S^2$:
$$
\frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2
$$

- Define t distribution:
$$
t_p \sim \frac{U}{\sqrt{V/p}}
$$
where $U \sim N(0,1)$ and $V \sim \chi_p^2$ 





## F distribution {.smaller}

F-distribution is:

- Defined in terms of $\chi_p^2$
- Describes **variance estimators** for independent samples




## Variance estimators {.smaller}

Suppose given random samples from 2 normal populations:

- $X_1, \ldots, X_n$ iid random sample from $N(\mu_X, \sigma_X^2)$
- $Y_1, \ldots, Y_m$ iid random sample from $N(\mu_Y, \sigma_Y^2)$


**Problem**:

- We want to compare **variability** of the 2 populations
- A way to do it is by estimating the variances ratio
$$
\frac{\sigma_X^2}{\sigma_Y^2}
$$



## Variance estimators {.smaller}

**Question**: 

- Suppose the variances $\sigma_X^2$ and $\sigma_Y^2$ are **unknown**
- How can we estimate the ratio $\sigma_X^2 /\sigma_Y^2 \,$ ?

. . . 

**Answer**: 

- Estimate the ratio $\sigma_X^2 /\sigma_Y^2 \,$ using sample variances
$$
S^2_X / S^2_Y
$$

- The F-distribution allows to compare the quantities 
$$
\sigma_X^2 /\sigma_Y^2 \qquad \text{and} \qquad S^2_X / S^2_Y
$$




## F distribution {.smaller}

::: Definition


The r.v. $F$ has F-distribution with $p$ and $q$ degrees of freedom
if the pdf is
$$
f_F(x) = \frac{ \Gamma \left(\frac{p+q}{2} \right) }{ \Gamma \left( \frac{p}{2} \right) \Gamma \left( \frac{q}{2} \right) } 
\left( \frac{p}{q} \right)^{p/2} \, 
\frac{ x^{ (p/2) - 1 } }{ [ 1 + (p/q) x ]^{(p+q)/2} } \,, \quad x > 0
$$


:::


**Notation**:  F-distribution with $p$ and $q$ degrees of freedom is denoted by $F_{p,q}$





## Variance ratio distribution {.smaller}

::: Theorem

Suppose given random samples from 2 normal populations:

- $X_1, \ldots, X_n$ iid random sample from $N(\mu_X, \sigma_X^2)$
- $Y_1, \ldots, Y_m$ iid random sample from $N(\mu_Y, \sigma_Y^2)$

The random variable 
$$
F = \frac{ S_X^2 / \sigma_X^2  }{ S_Y^2 / \sigma_Y^2 }
$$
has F-distribution with $n-1$ and $m-1$ degrees of freedom.

:::



## Variance ratio distribution {.smaller}
### Idea of Proof

- Similar to the proof (seen in Homework 3) that 
$$
T = \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t_{n-1}
$$

- In our case we need to prove
$$
F = \frac{ S_X^2 / \sigma_X^2  }{ S_Y^2 / \sigma_Y^2 } \sim F_{n-1,m-1}
$$




## Variance ratio distribution {.smaller}
### Idea of Proof


- We already know that
$$
 \frac{S_X^2}{ \sigma_X^2} \sim \frac{\chi_{n-1}^2}{n-1} \,, \qquad 
 \frac{S_Y^2}{ \sigma_Y^2} \sim \frac{\chi_{m-1}^2}{m-1}
$$


- Therefore 
$$
F = \frac{ S_X^2 / \sigma_X^2  }{ S_Y^2 / \sigma_Y^2 } = \frac{U/p}{V/q}  
$$
where we have
$$
U \sim \chi_{p}^2 \,, \qquad 
V \sim \chi_q^2 \,, \qquad 
p = n-1 \,, \qquad
q = m - 1
$$




## Variance ratio distribution {.smaller}
### Idea of Proof

- $U \sim \chi_{p}^2$ and $V \sim \chi_q^2$ are independent. Therefore
\begin{align*}
f_{U,V} (u,v) & = f_U(u) f_V(v) \\
              & = 
\frac{ 1 }{ \Gamma \left( \frac{p}{2} \right) \Gamma \left( \frac{q}{2} \right) 2^{(p+q)/2} }  u^{\frac{p}{2} - 1}
v^{\frac{q}{2} - 1} e^{-(u+v)/2}
\end{align*}


- Consider the change of variables 
$$
x(u,v) := \frac{u/p}{v/q} \,, \quad y(u,v) := u + v
$$




## Variance ratio distribution {.smaller}
### Idea of Proof


- This way we have
$$
F = \frac{U/p}{V/q}  = X
$$


- The pdf of $F$ can be hence computed by computing pdf of $X$

- We can compute $f_X$ via
$$
f_{X}(x) = \int_{0}^\infty f_{X,Y}(x,y) \, dy
$$



## Variance ratio distribution {.smaller}
### Idea of Proof


- The joint pdf $f_{X,Y}$ can be computed by inverting the change of variables
$$
x(u,v) := \frac{u/p}{v/q} \,, \quad y(u,v) := u + v
$$
and using the formula
$$
f_{X,Y}(x,y) = f_{U,V}(u(x,y),v(x,y)) \, |\det J|
$$
where $J$ is the Jacobian of the inverse transformation 
$$
(x,y) \mapsto (u(x,y),v(x,y))
$$



## Variance ratio distribution {.smaller}
### Idea of Proof


- Since $f_{U,V}$ is known, then also $f_{X,Y}$ is known

- Moreover the integral
$$
f_{X}(x) = \int_{0}^\infty f_{X,Y}(x,y) \, dy
$$
can be explicitly computed, yielding the thesis
$$
f_{F}(x) = f_{X}(x) = \frac{ \Gamma \left(\frac{p+q}{2} \right) }{ \Gamma \left( \frac{p}{2} \right) \Gamma \left( \frac{q}{2} \right) } 
\left( \frac{p}{q} \right)^{p/2} \, 
\frac{ x^{ (p/2) - 1 } }{ [ 1 + (p/q) x ]^{(p+q)/2} }
$$









## References
